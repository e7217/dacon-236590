import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import f1_score, make_scorer
import time
import joblib

def load_and_preprocess_data():
    print("=== λ°μ΄ν„° λ΅λ”© λ° μ „μ²λ¦¬ ===")
    train_df = pd.read_csv('./data/open/train.csv')

    X = train_df.drop(columns=['ID', 'target'])
    y = train_df['target']

    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

    print(f"λ°μ΄ν„° ν¬κΈ°: {X_scaled.shape}")
    print(f"ν΄λμ¤ λ¶„ν¬:\n{y.value_counts().sort_index()}")

    return X_scaled, y, scaler

def tune_lightgbm(X, y, n_iter=50):
    print("\n=== LightGBM ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ‹μ‘ ===")

    param_distributions = {
        'n_estimators': [100, 200, 300, 400, 500],
        'max_depth': [4, 6, 8, 10, 12, -1],
        'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.15, 0.2],
        'num_leaves': [31, 50, 70, 100, 150],
        'min_child_samples': [20, 30, 40, 50],
        'subsample': [0.7, 0.8, 0.9, 1.0],
        'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
        'reg_alpha': [0, 0.1, 0.5, 1.0],
        'reg_lambda': [0, 0.1, 0.5, 1.0]
    }

    lgbm_base = lgb.LGBMClassifier(
        device='cpu',
        random_state=42,
        verbose=-1
    )

    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    f1_scorer = make_scorer(f1_score, average='macro')

    random_search = RandomizedSearchCV(
        estimator=lgbm_base,
        param_distributions=param_distributions,
        n_iter=n_iter,
        cv=skf,
        scoring=f1_scorer,
        random_state=42,
        n_jobs=-1,
        verbose=2
    )

    start_time = time.time()
    random_search.fit(X, y)
    elapsed_time = time.time() - start_time

    print(f"\nνλ‹ μ™„λ£ μ‹κ°„: {elapsed_time/60:.2f}λ¶„")
    print(f"μµκ³  F1-macro μ μ: {random_search.best_score_:.6f}")
    print(f"\nμµμ  ν•μ΄νΌνλΌλ―Έν„°:")
    for param, value in random_search.best_params_.items():
        print(f"  {param}: {value}")

    return random_search.best_estimator_, random_search.best_params_, random_search.best_score_

def tune_xgboost(X, y, n_iter=50):
    print("\n=== XGBoost ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ‹μ‘ ===")

    param_distributions = {
        'n_estimators': [100, 200, 300, 400, 500],
        'max_depth': [4, 6, 8, 10, 12],
        'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.15, 0.2],
        'subsample': [0.7, 0.8, 0.9, 1.0],
        'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
        'min_child_weight': [1, 3, 5, 7],
        'gamma': [0, 0.1, 0.2, 0.3],
        'reg_alpha': [0, 0.1, 0.5, 1.0],
        'reg_lambda': [0, 0.1, 0.5, 1.0]
    }

    xgb_base = xgb.XGBClassifier(
        tree_method='hist',
        objective='multi:softmax',
        num_class=len(np.unique(y)),
        random_state=42,
        verbosity=0
    )

    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    f1_scorer = make_scorer(f1_score, average='macro')

    random_search = RandomizedSearchCV(
        estimator=xgb_base,
        param_distributions=param_distributions,
        n_iter=n_iter,
        cv=skf,
        scoring=f1_scorer,
        random_state=42,
        n_jobs=-1,
        verbose=2
    )

    start_time = time.time()
    random_search.fit(X, y)
    elapsed_time = time.time() - start_time

    print(f"\nνλ‹ μ™„λ£ μ‹κ°„: {elapsed_time/60:.2f}λ¶„")
    print(f"μµκ³  F1-macro μ μ: {random_search.best_score_:.6f}")
    print(f"\nμµμ  ν•μ΄νΌνλΌλ―Έν„°:")
    for param, value in random_search.best_params_.items():
        print(f"  {param}: {value}")

    return random_search.best_estimator_, random_search.best_params_, random_search.best_score_

def save_results(lgbm_model, lgbm_params, lgbm_score,
                 xgb_model, xgb_params, xgb_score, scaler):
    print("\n=== λ¨λΈ λ° κ²°κ³Ό μ €μ¥ ===")

    joblib.dump(lgbm_model, './models/lgbm_tuned.pkl')
    joblib.dump(xgb_model, './models/xgb_tuned.pkl')
    joblib.dump(scaler, './models/scaler.pkl')

    results = {
        'lightgbm': {
            'best_score': lgbm_score,
            'best_params': lgbm_params
        },
        'xgboost': {
            'best_score': xgb_score,
            'best_params': xgb_params
        }
    }

    import json
    with open('./models/tuning_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print("μ €μ¥ μ™„λ£:")
    print("  - models/lgbm_tuned.pkl")
    print("  - models/xgb_tuned.pkl")
    print("  - models/scaler.pkl")
    print("  - models/tuning_results.json")

def main():
    print("=" * 60)
    print("ν•μ΄νΌνλΌλ―Έν„° νλ‹ μλ™ν™” μ¤ν¬λ¦½νΈ")
    print("=" * 60)

    X_scaled, y, scaler = load_and_preprocess_data()

    lgbm_model, lgbm_params, lgbm_score = tune_lightgbm(X_scaled, y, n_iter=50)

    xgb_model, xgb_params, xgb_score = tune_xgboost(X_scaled, y, n_iter=50)

    print("\n" + "=" * 60)
    print("=== μµμΆ… κ²°κ³Ό λΉ„κµ ===")
    print("=" * 60)
    print(f"LightGBM μµκ³  μ μ: {lgbm_score:.6f}")
    print(f"XGBoost μµκ³  μ μ:  {xgb_score:.6f}")

    if lgbm_score > xgb_score:
        print(f"\nπ† LightGBMμ΄ {lgbm_score - xgb_score:.6f}λ§νΌ λ” μ°μν•©λ‹λ‹¤!")
    else:
        print(f"\nπ† XGBoostκ°€ {xgb_score - lgbm_score:.6f}λ§νΌ λ” μ°μν•©λ‹λ‹¤!")

    save_results(lgbm_model, lgbm_params, lgbm_score,
                 xgb_model, xgb_params, xgb_score, scaler)

    print("\n" + "=" * 60)
    print("νλ‹ μ™„λ£! μµμ ν™”λ λ¨λΈμ„ μ‚¬μ©ν•μ—¬ μμΈ΅μ„ μ§„ν–‰ν•μ„Έμ”.")
    print("=" * 60)

if __name__ == "__main__":
    main()
# Data Model: Hyperparameter Optimization System

**Feature**: 003-test-csv-optuna
**Date**: 2025-10-01
**Purpose**: Define data structures for optimization workflow and outputs

## Entity Overview

This feature deals primarily with **analysis artifacts** rather than traditional application data. The "entities" are analytical objects created during optimization, training, and prediction phases.

## Core Entities

### 1. OptimizationTrial
**Purpose**: Represents a single hyperparameter configuration attempt during Optuna optimization

**Attributes**:
- `trial_number` (int): Sequential trial identifier (0-indexed)
- `model_name` (str): Model being optimized (e.g., "QDA", "SVC", "RandomForest", "DecisionTree")
- `parameters` (dict): Hyperparameter values for this trial
  - Key: parameter name (str)
  - Value: parameter value (float, int, str, or bool)
- `cv_scores` (list[float]): F1-macro score for each fold (5 elements)
- `mean_score` (float): Average F1-macro across folds
- `std_score` (float): Standard deviation of F1-macro across folds
- `training_time` (float): Total seconds to train and evaluate all folds
- `status` (enum): Trial completion status
  - `COMPLETE`: Trial finished successfully
  - `PRUNED`: Trial terminated early by MedianPruner
  - `FAILED`: Trial raised an exception
- `prune_step` (int, optional): Fold number where pruning occurred (if pruned)
- `timestamp` (datetime): When trial started

**Relationships**:
- Belongs to one OptimizationStudy
- Generated by Optuna during optimization

**Validation Rules**:
- `cv_scores` must have exactly 5 elements (5-fold CV)
- `mean_score` = mean(cv_scores) ± 0.0001 (numerical precision)
- `std_score` = std(cv_scores) ± 0.0001
- `training_time` > 0
- If `status == PRUNED`, `prune_step` must be set (1-4)

**Example**:
```python
{
  "trial_number": 23,
  "model_name": "RandomForest",
  "parameters": {
    "n_estimators": 347,
    "max_depth": 18,
    "min_samples_split": 12,
    "max_features": 0.85
  },
  "cv_scores": [0.8123, 0.8145, 0.8098, 0.8167, 0.8134],
  "mean_score": 0.8133,
  "std_score": 0.0024,
  "training_time": 12.45,
  "status": "COMPLETE",
  "prune_step": null,
  "timestamp": "2025-10-01T14:23:45"
}
```

### 2. OptimizationStudy
**Purpose**: Aggregates all trials for a single model's optimization process

**Attributes**:
- `study_name` (str): Unique identifier (e.g., "qda_optimization_20251001")
- `model_name` (str): Model being optimized
- `n_trials` (int): Total trials requested
- `n_completed` (int): Trials with status = COMPLETE
- `n_pruned` (int): Trials with status = PRUNED
- `n_failed` (int): Trials with status = FAILED
- `best_trial` (OptimizationTrial): Trial with highest mean_score
- `best_parameters` (dict): Hyperparameters from best_trial
- `best_score` (float): mean_score from best_trial
- `total_optimization_time` (float): Total seconds for all trials
- `sampler` (str): Optuna sampler used (e.g., "TPESampler")
- `pruner` (str): Optuna pruner used (e.g., "MedianPruner")
- `start_time` (datetime): When optimization started
- `end_time` (datetime): When optimization completed

**Relationships**:
- Contains many OptimizationTrial objects
- One per model (4 total: QDA, SVC, RF, DT)

**Validation Rules**:
- `n_completed + n_pruned + n_failed == n_trials`
- `best_trial` must be from trials with status = COMPLETE
- `best_score` = best_trial.mean_score
- `total_optimization_time` ≈ sum(all trial.training_time) (within 10% due to overhead)

**Example**:
```python
{
  "study_name": "randomforest_optimization_20251001",
  "model_name": "RandomForest",
  "n_trials": 50,
  "n_completed": 38,
  "n_pruned": 10,
  "n_failed": 2,
  "best_trial": {...},  # OptimizationTrial object
  "best_parameters": {"n_estimators": 347, ...},
  "best_score": 0.8133,
  "total_optimization_time": 485.67,
  "sampler": "TPESampler(seed=42)",
  "pruner": "MedianPruner(n_startup_trials=5)",
  "start_time": "2025-10-01T14:00:00",
  "end_time": "2025-10-01T14:08:06"
}
```

### 3. OptimizedModel
**Purpose**: Represents a trained model with optimized hyperparameters

**Attributes**:
- `model_name` (str): Model identifier
- `model_class` (str): Full class name (e.g., "sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis")
- `hyperparameters` (dict): Final optimized hyperparameters
- `cv_performance` (dict): Cross-validation metrics
  - `f1_macro_mean` (float)
  - `f1_macro_std` (float)
  - `accuracy_mean` (float)
  - `accuracy_std` (float)
  - `precision_macro_mean` (float)
  - `recall_macro_mean` (float)
- `baseline_performance` (dict): Original performance before optimization (from notebook 06)
  - `f1_macro_mean` (float)
  - `f1_macro_std` (float)
- `improvement` (float): Percentage point improvement in F1-macro
- `feature_set` (str): Which features used ("raw", "engineered", "selected")
- `n_features` (int): Number of features
- `training_time` (float): Seconds to train on full dataset
- `model_path` (str): File path to saved model (joblib)
- `trained_on` (datetime): When model was trained on full dataset

**Relationships**:
- Created from one OptimizationStudy.best_parameters
- Used in one or more EnsembleModel objects

**Validation Rules**:
- `improvement` = cv_performance.f1_macro_mean - baseline_performance.f1_macro_mean
- `model_path` must exist as file (after training)
- `hyperparameters` must match OptimizationStudy.best_parameters

**Example**:
```python
{
  "model_name": "RandomForest",
  "model_class": "sklearn.ensemble.RandomForestClassifier",
  "hyperparameters": {
    "n_estimators": 347,
    "max_depth": 18,
    "min_samples_split": 12,
    "max_features": 0.85,
    "random_state": 42,
    "n_jobs": -1
  },
  "cv_performance": {
    "f1_macro_mean": 0.7601,
    "f1_macro_std": 0.0018,
    "accuracy_mean": 0.7623,
    "accuracy_std": 0.0019,
    "precision_macro_mean": 0.7688,
    "recall_macro_mean": 0.7601
  },
  "baseline_performance": {
    "f1_macro_mean": 0.7349,
    "f1_macro_std": 0.0014
  },
  "improvement": 0.0252,  # 2.52 percentage points
  "feature_set": "raw",
  "n_features": 52,
  "training_time": 6.34,
  "model_path": "/home/e7217/projects/dacon-236590/models/rf_optimized.pkl",
  "trained_on": "2025-10-01T15:30:22"
}
```

### 4. FeatureSet
**Purpose**: Represents a collection of engineered features

**Attributes**:
- `feature_set_name` (str): Identifier (e.g., "polynomial_degree2", "statistical_agg")
- `description` (str): Human-readable description
- `n_original_features` (int): Number of input features (52)
- `n_generated_features` (int): Number of new features created
- `n_total_features` (int): Total after generation
- `generation_technique` (str): Method used ("polynomial", "interaction", "statistical")
- `technique_parameters` (dict): Configuration for technique
- `feature_names` (list[str]): Names of all features in order
- `generation_time` (float): Seconds to generate features
- `memory_usage_mb` (float): Peak memory during generation

**Relationships**:
- Used by OptimizedModel (via feature_set attribute)
- Created during feature engineering phase

**Validation Rules**:
- `n_total_features` = n_original_features + n_generated_features
- len(feature_names) == n_total_features
- `generation_time` > 0
- `memory_usage_mb` > 0

**Example**:
```python
{
  "feature_set_name": "polynomial_degree2_qda",
  "description": "Polynomial features degree 2 with interaction_only for QDA",
  "n_original_features": 52,
  "n_generated_features": 1326,
  "n_total_features": 1378,
  "generation_technique": "polynomial",
  "technique_parameters": {
    "degree": 2,
    "interaction_only": True,
    "include_bias": False
  },
  "feature_names": ["f0", "f1", ..., "f0*f1", "f0*f2", ...],
  "generation_time": 2.34,
  "memory_usage_mb": 145.67
}
```

### 5. EnsembleModel
**Purpose**: Represents a combination of multiple OptimizedModel objects

**Attributes**:
- `ensemble_name` (str): Identifier (e.g., "voting_hard_top3", "stacking_qda_meta")
- `ensemble_type` (enum): Type of ensemble
  - `VOTING_HARD`: Simple majority vote
  - `VOTING_SOFT`: Weighted by predicted probabilities
  - `STACKING`: Meta-learner on base model predictions
- `base_models` (list[str]): Model names included (e.g., ["QDA", "RandomForest", "DecisionTree"])
- `weights` (list[float], optional): Weights for soft voting (sum to 1.0)
- `meta_learner` (str, optional): Model name for stacking meta-learner
- `cv_performance` (dict): Ensemble cross-validation metrics (same structure as OptimizedModel)
- `improvement_over_best_base` (float): Percentage point improvement over best individual model
- `training_time` (float): Seconds to train ensemble on full dataset
- `model_path` (str): File path to saved ensemble

**Relationships**:
- Aggregates multiple OptimizedModel objects
- Creates predictions via PredictionOutput

**Validation Rules**:
- len(base_models) >= 2 (must combine at least 2 models)
- If ensemble_type == VOTING_SOFT, len(weights) == len(base_models) and sum(weights) ≈ 1.0
- If ensemble_type == STACKING, meta_learner must be set
- `improvement_over_best_base` = cv_performance.f1_macro_mean - max(base_model cv scores)

**Example**:
```python
{
  "ensemble_name": "voting_soft_top3",
  "ensemble_type": "VOTING_SOFT",
  "base_models": ["QDA", "RandomForest", "DecisionTree"],
  "weights": [0.65, 0.25, 0.10],  # Weighted by CV F1 scores
  "meta_learner": null,
  "cv_performance": {
    "f1_macro_mean": 0.8856,
    "f1_macro_std": 0.0021,
    ...
  },
  "improvement_over_best_base": 0.0074,  # 0.74% over QDA (0.8782)
  "training_time": 7.89,
  "model_path": "/home/e7217/projects/dacon-236590/models/ensemble_voting_soft.pkl"
}
```

### 6. PredictionOutput
**Purpose**: Represents predictions on test.csv for competition submission

**Attributes**:
- `submission_name` (str): Filename (e.g., "submission_qda_optimized.csv")
- `model_or_ensemble_name` (str): Which model/ensemble generated predictions
- `n_predictions` (int): Number of test samples (should be 15,004)
- `prediction_distribution` (dict[int, int]): Count of each class (0-20)
- `prediction_time` (float): Seconds to generate all predictions
- `submission_path` (str): Full path to CSV file
- `created_at` (datetime): When predictions were generated

**Relationships**:
- Generated by one OptimizedModel or EnsembleModel
- Output format matches competition requirements

**Validation Rules**:
- `n_predictions` == 15004 (test.csv size)
- sum(prediction_distribution.values()) == n_predictions
- All keys in prediction_distribution must be in range [0, 20]
- `submission_path` must exist as file

**Example**:
```python
{
  "submission_name": "submission_qda_optimized.csv",
  "model_or_ensemble_name": "QDA",
  "n_predictions": 15004,
  "prediction_distribution": {
    0: 845, 1: 681, 2: 416, ..., 20: 706
  },
  "prediction_time": 0.12,
  "submission_path": "/home/e7217/projects/dacon-236590/outputs/submissions/submission_qda_optimized.csv",
  "created_at": "2025-10-01T16:45:33"
}
```

## Data Flow Diagram

```
[train.csv (21,693 samples, 52 features)]
            ↓
[FeatureSet Generation] (optional)
            ↓
[Raw or Engineered Features]
            ↓
[Optuna Optimization] → [OptimizationStudy]
            ↓                     ↓
     [OptimizationTrial]  [OptimizationTrial]  ... (50 trials per model)
            ↓                     ↓
     [Best Trial Selected]
            ↓
[Train on Full Dataset] → [OptimizedModel]
            ↓
[Create Ensembles] → [EnsembleModel]
            ↓
[test.csv (15,004 samples)] → [Generate Predictions] → [PredictionOutput]
            ↓
[submission_*.csv files for Dacon submission]
```

## Persistence Strategy

### Jupyter Notebook Outputs
All entities are primarily **in-memory** during notebook execution. Key outputs are persisted for reproducibility:

1. **Optuna Studies** → Saved as pickle files
   - Path: `models/optuna_study_{model_name}.pkl`
   - Can resume optimization if interrupted

2. **Trained Models** → Saved via joblib
   - Path: `models/{model_name}_optimized.pkl`
   - Includes fitted model and preprocessing pipeline

3. **Feature Scalers** → Saved via joblib
   - Path: `models/scaler_optimized.pkl`
   - Must use same scaler for test predictions

4. **Results DataFrames** → Saved as CSV
   - Path: `outputs/optimization_results.csv`
   - Summary of all model performances

5. **Submission Files** → CSV format
   - Path: `outputs/submissions/submission_{model_name}.csv`
   - Format: ID, target (competition requirement)

### Example Directory Structure After Execution
```
models/
├── optuna_study_qda.pkl
├── optuna_study_svc.pkl
├── optuna_study_rf.pkl
├── optuna_study_dt.pkl
├── qda_optimized.pkl
├── svc_optimized.pkl
├── rf_optimized.pkl
├── dt_optimized.pkl
├── ensemble_voting_hard.pkl
├── ensemble_voting_soft.pkl
├── ensemble_stacking.pkl
└── scaler_optimized.pkl

outputs/
├── optimization_results.csv
└── submissions/
    ├── submission_qda_optimized.csv
    ├── submission_svc_optimized.csv
    ├── submission_rf_optimized.csv
    ├── submission_dt_optimized.csv
    ├── submission_ensemble_voting_hard.csv
    ├── submission_ensemble_voting_soft.csv
    └── submission_ensemble_stacking.csv
```

## Visualization Artifacts

### Optuna Built-in Visualizations
These are **not entities** but important artifacts generated during optimization:

1. **Optimization History Plot**
   - Shows F1-macro score progression across trials
   - Identifies convergence point

2. **Parameter Importance Plot**
   - Ranks hyperparameters by impact on performance
   - Helps understand what matters most

3. **Parallel Coordinate Plot**
   - Multi-dimensional view of parameter relationships
   - Shows successful parameter combinations

All visualizations are displayed inline in notebooks (not saved as separate files unless explicitly requested).

## State Transitions

### OptimizationTrial Status Flow
```
[RUNNING] → [COMPLETE]  (trial finishes successfully)
[RUNNING] → [PRUNED]    (MedianPruner terminates early)
[RUNNING] → [FAILED]    (exception raised)
```

### Workflow State Progression
```
1. [UNOPTIMIZED] → Baseline models (notebook 06)
2. [OPTIMIZING]  → Optuna trials in progress (notebook 07)
3. [OPTIMIZED]   → Best hyperparameters found
4. [ENGINEERED]  → Features engineered (notebook 08)
5. [ENSEMBLED]   → Ensemble models created (notebook 09)
6. [PREDICTED]   → Test predictions generated
7. [SUBMITTED]   → CSV files ready for competition
```

## Constraints and Assumptions

### Data Constraints
- Training data: Exactly 21,693 samples (1,033 per class × 21 classes)
- Test data: Exactly 15,004 samples
- Features: 52 numeric features (no missing values)
- Target: 21 classes (0-20, perfectly balanced in train)

### Computational Constraints
- GPU Memory: 24GB (RTX 3090) - adequate for all models
- Optimization Budget: ~3 hours for 4 models × 50 trials
- Feature Explosion: Polynomial degree 2 creates ~1,300 features (manageable)

### Reproducibility Constraints
- All random operations use seed=42
- Library versions frozen in pyproject.toml
- Cross-validation split order is deterministic

---
**Status**: Data Model Complete
**Next**: Contract specifications and quickstart guide
# ğŸ­ Dacon ìŠ¤ë§ˆíŠ¸ ì œì¡° ì¥ë¹„ ì´ìƒ ê°ì§€ AI ê²½ì§„ëŒ€íšŒ

> **ëª©í‘œ**: ì¥ë¹„ ì„¼ì„œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¥ë¹„ì˜ ì •ìƒ/ë¹„ì •ìƒ ì‘ë™ ìœ í˜•ì„ ë¶„ë¥˜í•˜ëŠ” AI ëª¨ë¸ ê°œë°œ

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

### ë°°ê²½
- í˜„ì¥ ì¥ë¹„ë“¤ì€ ì˜¨ë„Â·ì••ë ¥Â·ì§„ë™Â·ì „ë¥˜ ë“± ì—¬ëŸ¬ ì„¼ì„œë¡œ ìƒíƒœë¥¼ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- ì‘ì€ ì´ìƒ íŒ¨í„´ì„ ì œë•Œ êµ¬ë¶„í•˜ì§€ ëª»í•˜ë©´ â†’ ë¶ˆí•„ìš”í•œ ì •ì§€, í’ˆì§ˆ ì €í•˜, ì•ˆì „ ë¦¬ìŠ¤í¬ ì¦ê°€
- **ë¸”ë™ë°•ìŠ¤ í™˜ê²½**: ë„ë©”ì¸ ì˜ë¯¸ê°€ ì°¨ë‹¨ëœ ë¹„ì‹ë³„í™” ë°ì´í„°(X_01, X_02 ë“±)ë§Œ ì œê³µ

### ë¬¸ì œ ì •ì˜
- **ë¬¸ì œ ìœ í˜•**: ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ (Multi-class Classification)
- **í´ë˜ìŠ¤ ìˆ˜**: 21ê°œ ì¥ë¹„ ìƒíƒœ
- **íŠ¹ì„± ìˆ˜**: 52ê°œ ì„¼ì„œ ë°ì´í„° (X_01 ~ X_52)
- **í‰ê°€ ì§€í‘œ**: Accuracy

---

## ğŸ“Š 1ë‹¨ê³„: ë°ì´í„° íƒìƒ‰ ë° ë¶„ì„ (EDA)

### 1.1 ë°ì´í„°ì…‹ êµ¬ì¡° íŒŒì•…

```python
# íŒŒì¼ êµ¬ì¡°
- train.csv: 21,693ê°œ í›ˆë ¨ ìƒ˜í”Œ (ID + 52ê°œ íŠ¹ì„± + target)
- test.csv: 15,004ê°œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ (ID + 52ê°œ íŠ¹ì„±)
- sample_submission.csv: ì œì¶œ ì–‘ì‹
```

**í•µì‹¬ ë°œê²¬ì‚¬í•­:**
- âœ… **ì™„ë²½í•œ í´ë˜ìŠ¤ ê· í˜•**: 21ê°œ í´ë˜ìŠ¤ ê°ê° ì •í™•íˆ 1,033ê°œ ìƒ˜í”Œ
- âœ… **ê²°ì¸¡ê°’ ì—†ìŒ**: ê¹¨ë—í•œ ë°ì´í„°ì…‹
- âœ… **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ì „ì²´ ë°ì´í„°ì…‹ ì•½ 17MB

### 1.2 íŠ¹ì„± ë¶„ì„

#### íŠ¹ì„± íŒ¨í„´ ë¶„ë¥˜:
- **ì •ê·œí™”ëœ íŠ¹ì„±** (47ê°œ): 0-1 ì‚¬ì´ ê°’ë“¤ â†’ ëŒ€ë¶€ë¶„ì˜ ì„¼ì„œ ë°ì´í„°
- **ë‹¤ë¥¸ ë²”ìœ„ íŠ¹ì„±** (5ê°œ): X_11, X_19, X_37, X_40 ë“± â†’ íŠ¹ë³„í•œ ì˜ë¯¸ë¥¼ ê°€ì§ˆ ê°€ëŠ¥ì„±

```python
# íŠ¹ì„± í†µê³„ ìš”ì•½
Min values range: [-0.235, 0.000]
Max values range: [0.037, 100.241]
Mean values range: [0.018, 50.956]
```

### 1.3 ì‹¬í™” ë¶„ì„ ê²°ê³¼

#### íŠ¹ì„± ì¤‘ìš”ë„ (Random Forest ê¸°ë°˜):
1. **X_40**: 8.6% - ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„±
2. **X_11**: 5.6%
3. **X_46**: 5.5%
4. **X_36**: 5.3%
5. **X_34**: 3.9%

#### ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ:
- **47ê°œ ê³ ìƒê´€ ìŒ** ë°œê²¬ (ìƒê´€ê³„ìˆ˜ > 0.8)
- ì£¼ìš” ì˜ˆì‹œ:
  - X_06 â†” X_45: 1.000 (ì™„ì „ ìƒê´€)
  - X_04 â†” X_39: 0.991
  - X_05 â†” X_25: 0.998

#### ì°¨ì› ì¶•ì†Œ ê°€ëŠ¥ì„±:
- **PCA ë¶„ì„**: 95% ë¶„ì‚°ì„ 18ê°œ ì£¼ì„±ë¶„ìœ¼ë¡œ ì„¤ëª… ê°€ëŠ¥
- **t-SNE**: í´ë˜ìŠ¤ ê°„ ë¶„ë¦¬ë„ ì–‘í˜¸

---

## ğŸ”§ 2ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬

### 2.1 ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ê³„

```python
class DataPreprocessor:
    def __init__(self, remove_corr_threshold=0.95, variance_threshold=0.01):
        self.scaler = RobustScaler()  # ì•„ì›ƒë¼ì´ì–´ ëŒ€ì‘
        self.variance_selector = VarianceThreshold(threshold)
        # ... ê¸°íƒ€ ì „ì²˜ë¦¬ ë„êµ¬ë“¤
```

### 2.2 ì „ì²˜ë¦¬ ë‹¨ê³„ë³„ íš¨ê³¼

| ë‹¨ê³„ | ì„¤ëª… | íŠ¹ì„± ìˆ˜ ë³€í™” | ëª©ì  |
|------|------|-------------|------|
| 1. Scaling | RobustScaler ì ìš© | 52 â†’ 52 | ì•„ì›ƒë¼ì´ì–´ ì˜í–¥ ìµœì†Œí™” |
| 2. Variance Filtering | ë‚®ì€ ë¶„ì‚° íŠ¹ì„± ì œê±° | 52 â†’ 52 | ì •ë³´ëŸ‰ ë¶€ì¡±í•œ íŠ¹ì„± ì œê±° |
| 3. Correlation Filtering | ê³ ìƒê´€ íŠ¹ì„± ì œê±° | 52 â†’ 40 | ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° |
| 4. Feature Selection (ì˜µì…˜) | í†µê³„ì  íŠ¹ì„± ì„ íƒ | 40 â†’ 30 | ì°¨ì› ì¶•ì†Œ |

### 2.3 3ê°€ì§€ ì „ì²˜ë¦¬ ì „ëµ

1. **Basic** (52â†’40): ê¸°ë³¸ ìƒê´€ê´€ê³„ ì œê±° (threshold=0.95)
2. **Aggressive** (52â†’38): ê°•í™” ìƒê´€ê´€ê³„ ì œê±° (threshold=0.90)
3. **Feature Selected** (52â†’30): í†µê³„ì  íŠ¹ì„± ì„ íƒ ì¶”ê°€

---

## ğŸ¤– 3ë‹¨ê³„: ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ êµ¬ì¶•

### 3.1 ëª¨ë¸ ì„ íƒ ë° ì„±ëŠ¥

| ëª¨ë¸ | í›ˆë ¨ ì •í™•ë„ | ê²€ì¦ ì •í™•ë„ | ê³¼ì í•©ë„ |
|------|-------------|-------------|----------|
| **Random Forest** | 1.0000 | **0.7624** | 0.2376 |
| Extra Trees | 1.0000 | 0.7495 | 0.2505 |
| Logistic Regression | 0.6188 | 0.6059 | 0.0129 |

**ë² ì´ìŠ¤ë¼ì¸ ê²°ê³¼**: Random Forest ëª¨ë¸ë¡œ **76.24% ê²€ì¦ ì •í™•ë„** ë‹¬ì„±

### 3.2 ë² ì´ìŠ¤ë¼ì¸ ë¶„ì„
- âœ… **Random Forestê°€ ìµœê³  ì„±ëŠ¥**: ì•™ìƒë¸” ë°©ë²•ì˜ íš¨ê³¼
- âš ï¸ **ê³¼ì í•© ê²½í–¥**: í›ˆë ¨-ê²€ì¦ ì •í™•ë„ ì°¨ì´ ~24%
- ğŸ’¡ **ê°œì„  ë°©í–¥**: ì •ê·œí™”, ì•™ìƒë¸”, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”

---

## ğŸ¯ 4ë‹¨ê³„: ê³ ê¸‰ ì•™ìƒë¸” ëª¨ë¸

### 4.1 ì•™ìƒë¸” ì „ëµ

```python
# ìµœì¢… ì•™ìƒë¸” êµ¬ì„±
models = {
    'RF1': RandomForestClassifier(n_estimators=150, max_depth=20, ...),
    'RF2': RandomForestClassifier(n_estimators=100, max_depth=15, ...),
    'ET': ExtraTreesClassifier(n_estimators=100, max_depth=18, ...)
}

# ë‹¤ìˆ˜ê²° íˆ¬í‘œ ë°©ì‹
ensemble_prediction = majority_vote(rf1_pred, rf2_pred, et_pred)
```

### 4.2 ìµœì¢… ì„±ëŠ¥

| ëª¨ë¸ | 3-Fold CV ì •í™•ë„ | í‘œì¤€í¸ì°¨ |
|------|------------------|----------|
| RF1 | 0.7468 | Â±0.0105 |
| RF2 | 0.7325 | Â±0.0084 |
| ET | 0.7312 | Â±0.0071 |
| **Ensemble** | **~0.737** | - |

### 4.3 ì˜ˆì¸¡ ë¶„í¬ ë¶„ì„

ìµœì¢… ì˜ˆì¸¡ì—ì„œ ê° í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜:
- ê°€ì¥ ë§ì´ ì˜ˆì¸¡ëœ í´ë˜ìŠ¤: Class 3 (1,593ê°œ)
- ê°€ì¥ ì ê²Œ ì˜ˆì¸¡ëœ í´ë˜ìŠ¤: Class 16 (289ê°œ)
- ì „ë°˜ì ìœ¼ë¡œ ê· í˜•ì ì¸ ë¶„í¬ ìœ ì§€

---

## ğŸ“ˆ í˜„ì¬ê¹Œì§€ ë‹¬ì„± ì„±ê³¼

### âœ… ì™„ë£Œëœ ì‘ì—…

1. **ë°ì´í„° ì´í•´**: ì™„ì „í•œ EDA ë° íŠ¹ì„± ë¶„ì„ ì™„ë£Œ
2. **ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸**: 3ê°€ì§€ ì „ëµìœ¼ë¡œ ì²´ê³„ì  ì „ì²˜ë¦¬
3. **ë² ì´ìŠ¤ë¼ì¸ êµ¬ì¶•**: Random Forestë¡œ 76.24% ë‹¬ì„±
4. **ì•™ìƒë¸” ëª¨ë¸**: 3ê°œ ëª¨ë¸ ì•™ìƒë¸”ë¡œ 73.7% ë‹¬ì„±
5. **ì œì¶œ íŒŒì¼**: 2ê°œ ì œì¶œ íŒŒì¼ ìƒì„± (ë² ì´ìŠ¤ë¼ì¸ + ì•™ìƒë¸”)

### ğŸ“Š ì£¼ìš” ì¸ì‚¬ì´íŠ¸

1. **ë°ì´í„° í’ˆì§ˆ ìš°ìˆ˜**: ê²°ì¸¡ê°’ ì—†ìŒ, í´ë˜ìŠ¤ ê· í˜• ì™„ë²½
2. **íŠ¹ì„± ì¤‘ìš”ë„**: X_40, X_11, X_46ì´ í•µì‹¬ ì„¼ì„œ
3. **ë‹¤ì¤‘ê³µì„ ì„±**: 47ê°œ ê³ ìƒê´€ ìŒìœ¼ë¡œ ì°¨ì›ì¶•ì†Œ í•„ìš”ì„± í™•ì¸
4. **ëª¨ë¸ íŠ¹ì„±**: Tree ê¸°ë°˜ ëª¨ë¸ì´ ì„¼ì„œ ë°ì´í„°ì— íš¨ê³¼ì 

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„: ê³ ë„í™” ì „ëµ

### ğŸ¯ ë‹¨ê¸° ê°œì„  ë°©ì•ˆ (1-2ì£¼)

#### 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
```python
# GridSearch/RandomSearch ì ìš©
param_grids = {
    'RandomForest': {
        'n_estimators': [100, 200, 300],
        'max_depth': [15, 20, 25, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
}

# Optunaë¥¼ ì´ìš©í•œ ë² ì´ì§€ì•ˆ ìµœì í™”
```

#### 2. ê³ ê¸‰ ì•™ìƒë¸” ê¸°ë²•
```python
# Stacking Ensemble
meta_model = LogisticRegression()
stacking_classifier = StackingClassifier(
    estimators=[('rf', rf_model), ('et', et_model), ('gb', gb_model)],
    final_estimator=meta_model
)

# Blending
blend_predictions = 0.4*rf_pred + 0.35*et_pred + 0.25*gb_pred
```

#### 3. êµì°¨ ê²€ì¦ ì „ëµ ê°•í™”
```python
# ì‹œê°„ ê¸°ë°˜ ë¶„í•  (ì‹œê³„ì—´ íŠ¹ì„± ê³ ë ¤)
tscv = TimeSeriesSplit(n_splits=5)

# ì¸µí™” ì¶”ì¶œ ê°•í™”
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
```

### ğŸ”¬ ì¤‘ê¸° ê°œì„  ë°©ì•ˆ (2-4ì£¼)

#### 1. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
```python
# í†µê³„ì  íŠ¹ì„± ìƒì„±
X['mean_sensors'] = X[sensor_cols].mean(axis=1)
X['std_sensors'] = X[sensor_cols].std(axis=1)
X['skew_sensors'] = X[sensor_cols].skew(axis=1)

# ë„ë©”ì¸ ê¸°ë°˜ íŠ¹ì„± (ì„¼ì„œ ê·¸ë£¹í•‘)
temp_sensors = ['X_01', 'X_05', 'X_12']  # ì˜¨ë„ ê´€ë ¨ ì¶”ì •
pressure_sensors = ['X_11', 'X_19', 'X_37']  # ì••ë ¥ ê´€ë ¨ ì¶”ì •
vibration_sensors = ['X_40', 'X_46']  # ì§„ë™ ê´€ë ¨ ì¶”ì •
```

#### 2. ê³ ê¸‰ ML ëª¨ë¸ ì‹¤í—˜
```python
# XGBoost/LightGBM
xgb_model = XGBClassifier(...)
lgb_model = LGBMClassifier(...)

# CatBoost (ë²”ì£¼í˜• íŠ¹ì„± ìë™ ì²˜ë¦¬)
cb_model = CatBoostClassifier(...)

# ì‹ ê²½ë§ ëª¨ë¸
nn_model = MLPClassifier(hidden_layer_sizes=(256, 128, 64))
```

#### 3. ë”¥ëŸ¬ë‹ ì ‘ê·¼ë²•
```python
# TabNet (í…Œì´ë¸” ë°ì´í„° íŠ¹í™” ë”¥ëŸ¬ë‹)
from pytorch_tabnet.tab_model import TabNetClassifier
tabnet_model = TabNetClassifier(...)

# AutoML ë„êµ¬ í™œìš©
from autogluon.tabular import TabularPredictor
predictor = TabularPredictor(label='target').fit(train_data)
```

### ğŸ¨ ì¥ê¸° ê°œì„  ë°©ì•ˆ (1-2ê°œì›”)

#### 1. ê³ ê¸‰ íŠ¹ì„± ì„ íƒ
```python
# Recursive Feature Elimination
rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=30)

# SHAP ê¸°ë°˜ íŠ¹ì„± ì¤‘ìš”ë„
import shap
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)

# Permutation Importance
from sklearn.inspection import permutation_importance
perm_importance = permutation_importance(model, X, y)
```

#### 2. ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬
```python
# Isolation Forestë¡œ ì´ìƒì¹˜ íƒì§€
from sklearn.ensemble import IsolationForest
iso_forest = IsolationForest(contamination=0.1)
outliers = iso_forest.fit_predict(X)

# Local Outlier Factor
from sklearn.neighbors import LocalOutlierFactor
lof = LocalOutlierFactor(n_neighbors=20)
```

#### 3. ë„ë©”ì¸ ì§€ì‹ í™œìš©
- **ì„¼ì„œ ë¬¼ë¦¬í•™**: ì˜¨ë„-ì••ë ¥ ìƒê´€ê´€ê³„, ì§„ë™-ì „ë¥˜ íŒ¨í„´
- **ì‹œê³„ì—´ íŒ¨í„´**: ì‹œê°„ì— ë”°ë¥¸ ì„¼ì„œ ë³€í™”ìœ¨
- **ê³ ì¥ ëª¨ë“œ ë¶„ì„**: ê° í´ë˜ìŠ¤ì˜ ë¬¼ë¦¬ì  ì˜ë¯¸ ì¶”ë¡ 

---

## ğŸ“š í•™ìŠµ ë¦¬ì†ŒìŠ¤ ë° ì°¸ê³ ìë£Œ

### ğŸ“– ì¶”ì²œ í•™ìŠµ ìë£Œ

#### 1. ì œì¡°ì—… AI ê´€ë ¨
- **ë…¼ë¬¸**: "Machine Learning for Predictive Maintenance" (IEEE)
- **ì„œì **: "Hands-On Machine Learning" (Chapter 6: Decision Trees)
- **ê°•ì˜**: Coursera "AI for Manufacturing"

#### 2. ì•™ìƒë¸” ê¸°ë²•
- **ì„œì **: "The Elements of Statistical Learning" (Chapter 15)
- **ì‹¤ìŠµ**: Kaggle Ensemble Guide
- **ì½”ë“œ**: scikit-learn ensemble examples

#### 3. ì„¼ì„œ ë°ì´í„° ë¶„ì„
- **ë…¼ë¬¸**: "Sensor Data Analysis for Equipment Health Monitoring"
- **ë„êµ¬**: pandas, numpy, scipy for signal processing
- **ì‹œê°í™”**: matplotlib, seaborn, plotly

### ğŸ› ï¸ ë„êµ¬ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¥

```bash
# ê³ ê¸‰ ML ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install xgboost lightgbm catboost
pip install optuna  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
pip install shap   # ëª¨ë¸ í•´ì„
pip install imbalanced-learn  # ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬

# ë”¥ëŸ¬ë‹
pip install pytorch-tabnet
pip install autogluon  # AutoML

# ì‹œê°í™” ë° ë¶„ì„
pip install plotly  # ì¸í„°ë™í‹°ë¸Œ ì‹œê°í™”
pip install yellowbrick  # ML ì‹œê°í™”
```

---

## ğŸ“Š ì„±ëŠ¥ ì¶”ì  ë° ì‹¤í—˜ ê´€ë¦¬

### ğŸ¯ ëª©í‘œ ì„±ëŠ¥ ì§€í‘œ

| ë‹¨ê³„ | ëª©í‘œ ì •í™•ë„ | í˜„ì¬ ë‹¬ì„± | ì°¨ì´ |
|------|-------------|----------|------|
| ë² ì´ìŠ¤ë¼ì¸ | 75% | âœ… 76.24% | +1.24% |
| ì•™ìƒë¸” V1 | 78% | ğŸ“ 73.7% | -4.3% |
| í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ | 80% | ğŸ¯ ì˜ˆì • | - |
| ê³ ê¸‰ ì•™ìƒë¸” | 82% | ğŸ¯ ì˜ˆì • | - |
| ë”¥ëŸ¬ë‹ ëª¨ë¸ | 85%+ | ğŸ¯ ì˜ˆì • | - |

### ğŸ“ˆ ì‹¤í—˜ ë¡œê·¸ ê´€ë¦¬

```python
# MLflowë¡œ ì‹¤í—˜ ì¶”ì 
import mlflow

with mlflow.start_run():
    mlflow.log_param("model_type", "RandomForest")
    mlflow.log_param("n_estimators", 150)
    mlflow.log_metric("cv_accuracy", 0.7624)
    mlflow.sklearn.log_model(model, "model")
```

---

## ğŸ† ìµœì¢… ëª©í‘œ ë° ê¸°ëŒ€íš¨ê³¼

### ğŸ¯ ê²½ì§„ëŒ€íšŒ ëª©í‘œ
- **ë‹¨ê¸° ëª©í‘œ**: ìƒìœ„ 20% ì§„ì… (ì •í™•ë„ 80%+)
- **ì¤‘ê¸° ëª©í‘œ**: ìƒìœ„ 10% ì§„ì… (ì •í™•ë„ 85%+)
- **ìµœì¢… ëª©í‘œ**: ìƒìœ„ 5% ì§„ì… (ì •í™•ë„ 90%+)

### ğŸ­ ì‹¤ë¬´ í™œìš© ê°€ëŠ¥ì„±

#### 1. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
```python
# ì‹¤ì‹œê°„ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸
def predict_equipment_status(sensor_data):
    processed_data = preprocessor.transform(sensor_data)
    prediction = ensemble_model.predict(processed_data)
    confidence = ensemble_model.predict_proba(processed_data).max()

    return {
        'status': prediction[0],
        'confidence': confidence,
        'alert_level': get_alert_level(prediction[0])
    }
```

#### 2. ì˜ˆì¸¡ ì •ë¹„ ì‹œìŠ¤í…œ
- **ì¡°ê¸° ê²½ë³´**: ì´ìƒ íŒ¨í„´ ê°ì§€ ì‹œ ì•ŒëŒ
- **ì •ë¹„ ìŠ¤ì¼€ì¤„ë§**: ì˜ˆìƒ ê³ ì¥ ì‹œì  ê¸°ë°˜ ì •ë¹„ ê³„íš
- **ë¶€í’ˆ êµì²´ ìµœì í™”**: ìƒíƒœ ê¸°ë°˜ ë¶€í’ˆ ìˆ˜ëª… ì˜ˆì¸¡

#### 3. í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œ
- **ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§**: ìƒì‚° ì¤‘ í’ˆì§ˆ ì´ìƒ ê°ì§€
- **ê³µì • ìµœì í™”**: ì„¼ì„œ ë°ì´í„° ê¸°ë°˜ ê³µì • íŒŒë¼ë¯¸í„° ì¡°ì •
- **ë¶ˆëŸ‰ë¥  ì˜ˆì¸¡**: ê³¼ê±° íŒ¨í„´ í•™ìŠµìœ¼ë¡œ ë¶ˆëŸ‰ë¥  ì˜ˆì¸¡

---

## ğŸ“ í”„ë¡œì íŠ¸ íŒŒì¼ êµ¬ì¡°

```
dacon-smartmh-02/
â”œâ”€â”€ data/open/
â”‚   â”œâ”€â”€ train.csv              # í›ˆë ¨ ë°ì´í„°
â”‚   â”œâ”€â”€ test.csv               # í…ŒìŠ¤íŠ¸ ë°ì´í„°
â”‚   â””â”€â”€ sample_submission.csv   # ì œì¶œ ì–‘ì‹
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ eda.ipynb              # íƒìƒ‰ì  ë°ì´í„° ë¶„ì„
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ eda.py                 # ê¸°ë³¸ EDA ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ advanced_analysis.py   # ì‹¬í™” ë¶„ì„
â”‚   â”œâ”€â”€ preprocessing.py       # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
â”‚   â”œâ”€â”€ baseline_models.py     # ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸
â”‚   â”œâ”€â”€ quick_baseline.py      # ë¹ ë¥¸ ë² ì´ìŠ¤ë¼ì¸
â”‚   â”œâ”€â”€ simple_submission.py   # ê°„ë‹¨í•œ ì œì¶œ
â”‚   â”œâ”€â”€ advanced_ensemble.py   # ê³ ê¸‰ ì•™ìƒë¸”
â”‚   â””â”€â”€ final_solution.py      # ìµœì¢… ì†”ë£¨ì…˜
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ preprocessor_*.pkl     # ì „ì²˜ë¦¬ê¸° ì €ì¥
â”‚   â””â”€â”€ processed_data_*.pkl   # ì „ì²˜ë¦¬ëœ ë°ì´í„°
â”œâ”€â”€ submissions/
â”‚   â”œâ”€â”€ submission.csv         # ë² ì´ìŠ¤ë¼ì¸ ì œì¶œ
â”‚   â””â”€â”€ final_submission.csv   # ìµœì¢… ì•™ìƒë¸” ì œì¶œ
â”œâ”€â”€ visualizations/
â”‚   â”œâ”€â”€ target_distribution.png
â”‚   â”œâ”€â”€ feature_importance.png
â”‚   â””â”€â”€ correlation_heatmap.png
â”œâ”€â”€ PROJECT_SUMMARY.md         # í”„ë¡œì íŠ¸ ë¬¸ì„œ (ì´ íŒŒì¼)
â””â”€â”€ pyproject.toml            # íŒ¨í‚¤ì§€ ì˜ì¡´ì„±
```

---

## ğŸ¤ ê¸°ì—¬ ë° í˜‘ì—…

### ğŸ’¡ ê°œì„  ì•„ì´ë””ì–´ í™˜ì˜
- ìƒˆë¡œìš´ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì•„ì´ë””ì–´
- ë‹¤ë¥¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì‹¤í—˜
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ê²°ê³¼
- ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸

### ğŸ“ ì—°ë½ì²˜ ë° í˜‘ì—…
- **ì´ìŠˆ ë“±ë¡**: GitHub Issues í™œìš©
- **ì½”ë“œ ê¸°ì—¬**: Pull Request í™˜ì˜
- **ì•„ì´ë””ì–´ ê³µìœ **: Discussion íƒ­ í™œìš©

---

*"ë°ì´í„° ê³¼í•™ì€ ì—¬ì •ì´ì§€ ëª©ì ì§€ê°€ ì•„ë‹ˆë‹¤. ì§€ì†ì ì¸ í•™ìŠµê³¼ ê°œì„ ì„ í†µí•´ ë” ë‚˜ì€ ì†”ë£¨ì…˜ì„ ë§Œë“¤ì–´ ë‚˜ê°€ì!" ğŸš€*

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-09-25
**í˜„ì¬ ìµœê³  ì„±ëŠ¥**: 76.96% (Random Forest 5-Fold CV)
**ë‹¤ìŒ ëª©í‘œ**: 80%+ (í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”)
"""
ì„±ëŠ¥ ê²©ì°¨ ë¶„ì„ê¸°
T004-T010 íƒœìŠ¤í¬ì˜ í•µì‹¬ ì„±ëŠ¥ ì§„ë‹¨ì„ ìœ„í•œ í†µí•© ë¶„ì„ ì‹œìŠ¤í…œ

í˜„ì¬ ìƒí™©:
- CV ì ìˆ˜: ~77% (Random Forest ê¸°ì¤€)
- ì‹¤ì œ ì œì¶œ ì ìˆ˜: 67.6%
- ì„±ëŠ¥ ê²©ì°¨: 9.4%

ë¶„ì„ ëª©í‘œ:
1. CV vs ì‹¤ì œ ì ìˆ˜ ê²©ì°¨ ì›ì¸ ê·œëª…
2. ê³¼ì í•©/ê³¼ì†Œì í•© ì§„ë‹¨
3. ë°ì´í„° ë¦¬í‚¤ì§€ íƒì§€
4. ê²€ì¦ ì „ëµ ìµœì í™” ë°©ì•ˆ ì œì‹œ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import cross_val_score, validation_curve, learning_curve
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

import sys
sys.path.append(str(Path(__file__).parent.parent.parent))
from src.tracking.experiment_tracker import ExperimentTracker
from src.utils.config import get_config, get_paths


class PerformanceGapAnalyzer:
    """
    ì„±ëŠ¥ ê²©ì°¨ ì¢…í•© ë¶„ì„ê¸°
    CV ì ìˆ˜ì™€ ì‹¤ì œ ì„±ëŠ¥ ê°„ì˜ ê²©ì°¨ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„
    """

    def __init__(self, use_tracking: bool = True):
        """
        ì„±ëŠ¥ ê²©ì°¨ ë¶„ì„ê¸° ì´ˆê¸°í™”

        Args:
            use_tracking: ì‹¤í—˜ ì¶”ì  ì‚¬ìš© ì—¬ë¶€
        """
        self.use_tracking = use_tracking
        self.results = {}
        self.figures = {}

        # ê²½ë¡œ ì„¤ì •
        self.paths = get_paths()
        self.plots_dir = self.paths['experiments_dir'] / 'plots' / 'diagnosis'
        self.plots_dir.mkdir(parents=True, exist_ok=True)

        # ì‹¤í—˜ ì¶”ì  ì„¤ì •
        if self.use_tracking:
            self.tracker = ExperimentTracker(
                project_name=get_config('tracking.project_name'),
                experiment_name="performance_gap_analysis"
            )

        print("ğŸ” ì„±ëŠ¥ ê²©ì°¨ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

    def analyze_performance_gap(self, X_train, y_train, X_val=None, y_val=None,
                              model=None, cv_folds=5):
        """
        ì¢…í•©ì ì¸ ì„±ëŠ¥ ê²©ì°¨ ë¶„ì„ ìˆ˜í–‰

        Args:
            X_train: í›ˆë ¨ ë°ì´í„° íŠ¹ì„±
            y_train: í›ˆë ¨ ë°ì´í„° ë ˆì´ë¸”
            X_val: ê²€ì¦ ë°ì´í„° íŠ¹ì„± (ì„ íƒì‚¬í•­)
            y_val: ê²€ì¦ ë°ì´í„° ë ˆì´ë¸” (ì„ íƒì‚¬í•­)
            model: ë¶„ì„í•  ëª¨ë¸ (ê¸°ë³¸ê°’: RandomForest)
            cv_folds: êµì°¨ê²€ì¦ í´ë“œ ìˆ˜

        Returns:
            Dict: ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("\nğŸš€ ì„±ëŠ¥ ê²©ì°¨ ì¢…í•© ë¶„ì„ ì‹œì‘...")
        print("=" * 50)

        # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
        if model is None:
            model = RandomForestClassifier(
                n_estimators=100,
                random_state=get_config('models.random_seed'),
                n_jobs=-1
            )

        # ì‹¤í—˜ ì¶”ì  ì‹œì‘
        if self.use_tracking:
            self.tracker.start_run(
                run_name="performance_gap_comprehensive",
                description="CV vs ì‹¤ì œ ì„±ëŠ¥ ê²©ì°¨ ì¢…í•© ë¶„ì„",
                tags={"analysis_type": "performance_gap", "task": "T004-T010"}
            )

        # 1. ê¸°ë³¸ CV ì„±ëŠ¥ ë¶„ì„
        cv_results = self._analyze_cv_performance(X_train, y_train, model, cv_folds)

        # 2. í™€ë“œì•„ì›ƒ ê²€ì¦ ë¶„ì„ (ê²€ì¦ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
        holdout_results = None
        if X_val is not None and y_val is not None:
            holdout_results = self._analyze_holdout_performance(
                X_train, y_train, X_val, y_val, model
            )

        # 3. í•™ìŠµ ê³¡ì„  ë¶„ì„
        learning_results = self._analyze_learning_curves(X_train, y_train, model)

        # 4. ê²€ì¦ ê³¡ì„  ë¶„ì„
        validation_results = self._analyze_validation_curves(X_train, y_train, model)

        # 5. ë°ì´í„° ëˆ„ìˆ˜ ê²€ì‚¬
        leakage_results = self._check_data_leakage(X_train, y_train, model)

        # 6. íŠ¹ì„± ì•ˆì •ì„± ë¶„ì„
        stability_results = self._analyze_feature_stability(X_train, y_train)

        # ê²°ê³¼ ì¢…í•©
        comprehensive_results = {
            'cv_analysis': cv_results,
            'holdout_analysis': holdout_results,
            'learning_curves': learning_results,
            'validation_curves': validation_results,
            'leakage_check': leakage_results,
            'stability_analysis': stability_results,
            'summary': self._generate_summary(cv_results, holdout_results)
        }

        self.results = comprehensive_results

        # ì‹¤í—˜ ì¶”ì  ë¡œê¹…
        if self.use_tracking:
            self._log_analysis_results(comprehensive_results)

        print("\nâœ… ì„±ëŠ¥ ê²©ì°¨ ë¶„ì„ ì™„ë£Œ!")
        self._print_analysis_summary(comprehensive_results)

        return comprehensive_results

    def _analyze_cv_performance(self, X, y, model, cv_folds):
        """êµì°¨ê²€ì¦ ì„±ëŠ¥ ìƒì„¸ ë¶„ì„"""
        print("\nğŸ“Š 1ë‹¨ê³„: êµì°¨ê²€ì¦ ì„±ëŠ¥ ë¶„ì„")

        from sklearn.model_selection import StratifiedKFold

        skf = StratifiedKFold(
            n_splits=cv_folds,
            shuffle=True,
            random_state=get_config('models.random_seed')
        )

        cv_scores = cross_val_score(model, X, y, cv=skf, scoring='f1_macro', n_jobs=-1)

        # í´ë“œë³„ ì„¸ë¶€ ë¶„ì„
        fold_details = []
        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]
            y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]

            model_fold = model.__class__(**model.get_params())
            model_fold.fit(X_fold_train, y_fold_train)
            y_pred = model_fold.predict(X_fold_val)

            fold_score = f1_score(y_fold_val, y_pred, average='macro')
            fold_details.append({
                'fold': fold_idx + 1,
                'score': fold_score,
                'train_size': len(train_idx),
                'val_size': len(val_idx)
            })

        results = {
            'mean_score': cv_scores.mean(),
            'std_score': cv_scores.std(),
            'min_score': cv_scores.min(),
            'max_score': cv_scores.max(),
            'scores': cv_scores.tolist(),
            'fold_details': fold_details,
            'cv_range': cv_scores.max() - cv_scores.min()
        }

        print(f"  â€¢ í‰ê·  CV ì ìˆ˜: {results['mean_score']:.4f} Â± {results['std_score']:.4f}")
        print(f"  â€¢ ì ìˆ˜ ë²”ìœ„: {results['min_score']:.4f} - {results['max_score']:.4f}")
        print(f"  â€¢ CV ë³€ë™ì„±: {results['cv_range']:.4f}")

        return results

    def _analyze_holdout_performance(self, X_train, y_train, X_val, y_val, model):
        """í™€ë“œì•„ì›ƒ ê²€ì¦ ì„±ëŠ¥ ë¶„ì„"""
        print("\nğŸ“ˆ 2ë‹¨ê³„: í™€ë“œì•„ì›ƒ ê²€ì¦ ë¶„ì„")

        model_holdout = model.__class__(**model.get_params())
        model_holdout.fit(X_train, y_train)
        y_pred = model_holdout.predict(X_val)

        holdout_score = f1_score(y_val, y_pred, average='macro')

        results = {
            'holdout_score': holdout_score,
            'train_size': len(X_train),
            'val_size': len(X_val)
        }

        print(f"  â€¢ í™€ë“œì•„ì›ƒ ì ìˆ˜: {results['holdout_score']:.4f}")

        return results

    def _analyze_learning_curves(self, X, y, model):
        """í•™ìŠµ ê³¡ì„  ë¶„ì„ìœ¼ë¡œ ê³¼ì í•©/ê³¼ì†Œì í•© ì§„ë‹¨"""
        print("\nğŸ“š 3ë‹¨ê³„: í•™ìŠµ ê³¡ì„  ë¶„ì„")

        train_sizes = np.linspace(0.1, 1.0, 10)

        train_sizes_abs, train_scores, val_scores = learning_curve(
            model, X, y,
            train_sizes=train_sizes,
            cv=5,
            scoring='f1_macro',
            n_jobs=-1,
            random_state=get_config('models.random_seed')
        )

        # í•™ìŠµ ê³¡ì„  ì‹œê°í™”
        plt.figure(figsize=(10, 6))

        train_mean = train_scores.mean(axis=1)
        train_std = train_scores.std(axis=1)
        val_mean = val_scores.mean(axis=1)
        val_std = val_scores.std(axis=1)

        plt.plot(train_sizes_abs, train_mean, 'o-', color='blue', label='Training Score')
        plt.fill_between(train_sizes_abs, train_mean - train_std,
                        train_mean + train_std, alpha=0.1, color='blue')

        plt.plot(train_sizes_abs, val_mean, 'o-', color='red', label='Validation Score')
        plt.fill_between(train_sizes_abs, val_mean - val_std,
                        val_mean + val_std, alpha=0.1, color='red')

        plt.xlabel('Training Set Size')
        plt.ylabel('Macro F1 Score')
        plt.title('Learning Curves - Overfitting/Underfitting Analysis')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()

        # ê·¸ë˜í”„ ì €ì¥
        learning_curve_path = self.plots_dir / 'learning_curves.png'
        plt.savefig(learning_curve_path, dpi=300, bbox_inches='tight')
        self.figures['learning_curves'] = plt.gcf()
        plt.close()

        # ê³¼ì í•©/ê³¼ì†Œì í•© ì§„ë‹¨
        final_train_score = train_mean[-1]
        final_val_score = val_mean[-1]
        gap = final_train_score - final_val_score

        if gap > 0.05:
            diagnosis = "ê³¼ì í•© (Overfitting)"
        elif gap < 0.01:
            diagnosis = "ê³¼ì†Œì í•© (Underfitting)"
        else:
            diagnosis = "ì ì ˆí•œ ì í•© (Good Fit)"

        results = {
            'train_sizes': train_sizes_abs.tolist(),
            'train_scores_mean': train_mean.tolist(),
            'train_scores_std': train_std.tolist(),
            'val_scores_mean': val_mean.tolist(),
            'val_scores_std': val_std.tolist(),
            'final_gap': gap,
            'diagnosis': diagnosis,
            'plot_path': str(learning_curve_path)
        }

        print(f"  â€¢ ìµœì¢… í›ˆë ¨ ì ìˆ˜: {final_train_score:.4f}")
        print(f"  â€¢ ìµœì¢… ê²€ì¦ ì ìˆ˜: {final_val_score:.4f}")
        print(f"  â€¢ ì„±ëŠ¥ ê²©ì°¨: {gap:.4f}")
        print(f"  â€¢ ì§„ë‹¨ ê²°ê³¼: {diagnosis}")

        return results

    def _analyze_validation_curves(self, X, y, model):
        """ê²€ì¦ ê³¡ì„ ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë¶„ì„"""
        print("\nğŸ›ï¸ 4ë‹¨ê³„: ê²€ì¦ ê³¡ì„  ë¶„ì„")

        # RandomForestì˜ ì£¼ìš” íŒŒë¼ë¯¸í„° ë¶„ì„
        if hasattr(model, 'n_estimators'):
            param_name = 'n_estimators'
            param_range = [10, 50, 100, 200, 500]
        else:
            param_name = 'C'
            param_range = [0.001, 0.01, 0.1, 1, 10, 100]

        train_scores, val_scores = validation_curve(
            model, X, y,
            param_name=param_name,
            param_range=param_range,
            cv=5,
            scoring='f1_macro',
            n_jobs=-1
        )

        # ê²€ì¦ ê³¡ì„  ì‹œê°í™”
        plt.figure(figsize=(10, 6))

        train_mean = train_scores.mean(axis=1)
        train_std = train_scores.std(axis=1)
        val_mean = val_scores.mean(axis=1)
        val_std = val_scores.std(axis=1)

        plt.semilogx(param_range, train_mean, 'o-', color='blue', label='Training Score')
        plt.fill_between(param_range, train_mean - train_std,
                        train_mean + train_std, alpha=0.1, color='blue')

        plt.semilogx(param_range, val_mean, 'o-', color='red', label='Validation Score')
        plt.fill_between(param_range, val_mean - val_std,
                        val_mean + val_std, alpha=0.1, color='red')

        plt.xlabel(param_name)
        plt.ylabel('Macro F1 Score')
        plt.title(f'Validation Curve - {param_name} Sensitivity')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()

        # ê·¸ë˜í”„ ì €ì¥
        validation_curve_path = self.plots_dir / f'validation_curve_{param_name}.png'
        plt.savefig(validation_curve_path, dpi=300, bbox_inches='tight')
        self.figures[f'validation_curve_{param_name}'] = plt.gcf()
        plt.close()

        # ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°
        best_idx = np.argmax(val_mean)
        best_param = param_range[best_idx]
        best_score = val_mean[best_idx]

        results = {
            'param_name': param_name,
            'param_range': param_range,
            'train_scores_mean': train_mean.tolist(),
            'val_scores_mean': val_mean.tolist(),
            'best_param': best_param,
            'best_score': best_score,
            'param_sensitivity': val_mean.std(),
            'plot_path': str(validation_curve_path)
        }

        print(f"  â€¢ ë¶„ì„ íŒŒë¼ë¯¸í„°: {param_name}")
        print(f"  â€¢ ìµœì ê°’: {best_param}")
        print(f"  â€¢ ìµœì  ì ìˆ˜: {best_score:.4f}")
        print(f"  â€¢ íŒŒë¼ë¯¸í„° ë¯¼ê°ë„: {results['param_sensitivity']:.4f}")

        return results

    def _check_data_leakage(self, X, y, model):
        """ë°ì´í„° ëˆ„ìˆ˜ ê²€ì‚¬"""
        print("\nğŸš° 5ë‹¨ê³„: ë°ì´í„° ëˆ„ìˆ˜ ê²€ì‚¬")

        # ëœë¤ ë¼ë²¨ë¡œ í•™ìŠµí•´ì„œ ì„±ëŠ¥ì´ ë†’ìœ¼ë©´ ëˆ„ìˆ˜ ì˜ì‹¬
        y_random = np.random.permutation(y.values)

        cv_scores_original = cross_val_score(model, X, y, cv=5, scoring='f1_macro')
        cv_scores_random = cross_val_score(model, X, y_random, cv=5, scoring='f1_macro')

        original_mean = cv_scores_original.mean()
        random_mean = cv_scores_random.mean()

        # ëœë¤ ì„±ëŠ¥ì´ ë„ˆë¬´ ë†’ìœ¼ë©´ ë°ì´í„° ëˆ„ìˆ˜ ì˜ì‹¬
        leakage_suspected = random_mean > 0.3  # 21í´ë˜ìŠ¤ ë¶„ë¥˜ì—ì„œ ëœë¤ì€ ~0.048

        results = {
            'original_score': original_mean,
            'random_label_score': random_mean,
            'leakage_suspected': leakage_suspected,
            'baseline_expected': 1.0 / len(np.unique(y)),  # í´ë˜ìŠ¤ ìˆ˜ì˜ ì—­ìˆ˜
            'leakage_ratio': random_mean / (1.0 / len(np.unique(y)))
        }

        print(f"  â€¢ ì›ë³¸ ë¼ë²¨ ì ìˆ˜: {original_mean:.4f}")
        print(f"  â€¢ ëœë¤ ë¼ë²¨ ì ìˆ˜: {random_mean:.4f}")
        print(f"  â€¢ ì˜ˆìƒ ë² ì´ìŠ¤ë¼ì¸: {results['baseline_expected']:.4f}")
        print(f"  â€¢ ëˆ„ìˆ˜ ì˜ì‹¬ ì—¬ë¶€: {'ì˜ì‹¬ë¨' if leakage_suspected else 'ì •ìƒ'}")

        return results

    def _analyze_feature_stability(self, X, y):
        """íŠ¹ì„± ì•ˆì •ì„± ë¶„ì„"""
        print("\nğŸ“Š 6ë‹¨ê³„: íŠ¹ì„± ì•ˆì •ì„± ë¶„ì„")

        # íŠ¹ì„±ë³„ í†µê³„ ì•ˆì •ì„± í™•ì¸
        feature_stats = {}

        for column in X.columns:
            feature_data = X[column]
            stats = {
                'mean': feature_data.mean(),
                'std': feature_data.std(),
                'skew': feature_data.skew(),
                'kurtosis': feature_data.kurtosis(),
                'missing_ratio': feature_data.isnull().sum() / len(feature_data)
            }
            feature_stats[column] = stats

        # ë¶ˆì•ˆì •í•œ íŠ¹ì„± ì‹ë³„ (ë†’ì€ skew, kurtosis)
        unstable_features = []
        for feature, stats in feature_stats.items():
            if abs(stats['skew']) > 3 or abs(stats['kurtosis']) > 10:
                unstable_features.append(feature)

        results = {
            'feature_stats': feature_stats,
            'unstable_features': unstable_features,
            'stability_score': 1 - (len(unstable_features) / len(X.columns))
        }

        print(f"  â€¢ ì´ íŠ¹ì„± ìˆ˜: {len(X.columns)}")
        print(f"  â€¢ ë¶ˆì•ˆì •í•œ íŠ¹ì„± ìˆ˜: {len(unstable_features)}")
        print(f"  â€¢ ì•ˆì •ì„± ì ìˆ˜: {results['stability_score']:.4f}")

        return results

    def _generate_summary(self, cv_results, holdout_results):
        """ë¶„ì„ ê²°ê³¼ ì¢…í•© ìš”ì•½"""
        summary = {
            'cv_mean': cv_results['mean_score'],
            'cv_std': cv_results['std_score'],
            'cv_stability': cv_results['cv_range']
        }

        if holdout_results:
            summary['holdout_score'] = holdout_results['holdout_score']
            summary['cv_holdout_gap'] = abs(cv_results['mean_score'] - holdout_results['holdout_score'])

        # í˜„ì¬ ì•Œë ¤ì§„ ì‹¤ì œ ì œì¶œ ì ìˆ˜ì™€ ë¹„êµ
        current_submission_score = get_config('targets.current_score', 0.67596)
        summary['submission_score'] = current_submission_score
        summary['cv_submission_gap'] = cv_results['mean_score'] - current_submission_score

        return summary

    def _log_analysis_results(self, results):
        """ì‹¤í—˜ ì¶”ì  ì‹œìŠ¤í…œì— ê²°ê³¼ ë¡œê¹…"""
        # ì£¼ìš” ë©”íŠ¸ë¦­ ë¡œê¹…
        metrics = {
            'cv_mean_score': results['cv_analysis']['mean_score'],
            'cv_std_score': results['cv_analysis']['std_score'],
            'cv_range': results['cv_analysis']['cv_range'],
            'learning_gap': results['learning_curves']['final_gap'],
            'leakage_ratio': results['leakage_check']['leakage_ratio'],
            'stability_score': results['stability_analysis']['stability_score']
        }

        if results['holdout_analysis']:
            metrics['holdout_score'] = results['holdout_analysis']['holdout_score']
            metrics['cv_holdout_gap'] = results['summary']['cv_holdout_gap']

        metrics['cv_submission_gap'] = results['summary']['cv_submission_gap']

        self.tracker.log_metrics(metrics)

        # ê·¸ë˜í”„ë“¤ ë¡œê¹…
        for fig_name, fig in self.figures.items():
            self.tracker.log_figure(fig, fig_name)

        # ë¶„ì„ ì™„ë£Œ
        self.tracker.end_run()

    def _print_analysis_summary(self, results):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“‹ ì„±ëŠ¥ ê²©ì°¨ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*60)

        print(f"\nğŸ¯ êµì°¨ê²€ì¦ ì„±ëŠ¥:")
        print(f"  â€¢ í‰ê·  ì ìˆ˜: {results['cv_analysis']['mean_score']:.4f} Â± {results['cv_analysis']['std_score']:.4f}")
        print(f"  â€¢ ì ìˆ˜ ë²”ìœ„: {results['cv_analysis']['cv_range']:.4f}")

        if results['holdout_analysis']:
            print(f"\nğŸ“Š í™€ë“œì•„ì›ƒ ê²€ì¦:")
            print(f"  â€¢ í™€ë“œì•„ì›ƒ ì ìˆ˜: {results['holdout_analysis']['holdout_score']:.4f}")
            print(f"  â€¢ CV-í™€ë“œì•„ì›ƒ ê²©ì°¨: {results['summary']['cv_holdout_gap']:.4f}")

        print(f"\nğŸš¨ í•µì‹¬ ë¬¸ì œì :")
        print(f"  â€¢ CV-ì‹¤ì œì œì¶œ ê²©ì°¨: {results['summary']['cv_submission_gap']:.4f}")
        print(f"  â€¢ í•™ìŠµê³¡ì„  ì§„ë‹¨: {results['learning_curves']['diagnosis']}")
        print(f"  â€¢ ë°ì´í„° ëˆ„ìˆ˜ ì˜ì‹¬: {'ì˜ˆ' if results['leakage_check']['leakage_suspected'] else 'ì•„ë‹ˆì˜¤'}")

        print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        if results['summary']['cv_submission_gap'] > 0.05:
            print("  â€¢ CV ì „ëµ ì¬ê²€í†  í•„ìš” (ì‹œê°„ ê¸°ë°˜ ë¶„í•  ê³ ë ¤)")
        if results['learning_curves']['diagnosis'] == "ê³¼ì í•© (Overfitting)":
            print("  â€¢ ì •ê·œí™” ê°•í™” ë˜ëŠ” ëª¨ë¸ ë³µì¡ë„ ê°ì†Œ")
        if results['leakage_check']['leakage_suspected']:
            print("  â€¢ ë°ì´í„° ëˆ„ìˆ˜ ì¡°ì‚¬ í•„ìš”")

        print("="*60)

    def get_recommendations(self):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ê°œì„  ê¶Œì¥ì‚¬í•­ ë°˜í™˜"""
        if not self.results:
            print("âš ï¸ ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return []

        recommendations = []
        results = self.results

        # CV-ì œì¶œ ì ìˆ˜ ê²©ì°¨ê°€ í° ê²½ìš°
        if results['summary']['cv_submission_gap'] > 0.05:
            recommendations.append({
                'priority': 'HIGH',
                'category': 'validation_strategy',
                'issue': 'CVì™€ ì‹¤ì œ ì ìˆ˜ ê²©ì°¨ê°€ í¼',
                'recommendation': 'TimeSeriesSplit ë˜ëŠ” GroupKFold ì‚¬ìš© ê²€í† ',
                'expected_impact': 'CV ì‹ ë¢°ë„ í–¥ìƒ'
            })

        # ê³¼ì í•© ì§„ë‹¨ì¸ ê²½ìš°
        if results['learning_curves']['diagnosis'] == "ê³¼ì í•© (Overfitting)":
            recommendations.append({
                'priority': 'HIGH',
                'category': 'model_complexity',
                'issue': 'ëª¨ë¸ì´ ê³¼ì í•©ë¨',
                'recommendation': 'ì •ê·œí™” ê°•í™”, early stopping, ë°ì´í„° ì¦ê°•',
                'expected_impact': 'ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ'
            })

        # ë°ì´í„° ëˆ„ìˆ˜ ì˜ì‹¬ì¸ ê²½ìš°
        if results['leakage_check']['leakage_suspected']:
            recommendations.append({
                'priority': 'CRITICAL',
                'category': 'data_quality',
                'issue': 'ë°ì´í„° ëˆ„ìˆ˜ ì˜ì‹¬',
                'recommendation': 'íŠ¹ì„± ìƒì„± ê³¼ì • ì¬ê²€í†  ë° ì‹œê°„ ìˆœì„œ í™•ì¸',
                'expected_impact': 'ì˜¬ë°”ë¥¸ ì„±ëŠ¥ í‰ê°€'
            })

        # CV ë³€ë™ì„±ì´ í° ê²½ìš°
        if results['cv_analysis']['cv_range'] > 0.1:
            recommendations.append({
                'priority': 'MEDIUM',
                'category': 'model_stability',
                'issue': 'CV ì ìˆ˜ ë³€ë™ì„±ì´ í¼',
                'recommendation': 'ì•™ìƒë¸” ë°©ë²• ì‚¬ìš© ë˜ëŠ” ë” ë§ì€ í´ë“œ ì‚¬ìš©',
                'expected_impact': 'ëª¨ë¸ ì•ˆì •ì„± í–¥ìƒ'
            })

        return recommendations


if __name__ == "__main__":
    """ì„±ëŠ¥ ê²©ì°¨ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ì„±ëŠ¥ ê²©ì°¨ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘...")

    # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    from sklearn.datasets import make_classification

    X, y = make_classification(
        n_samples=1000,
        n_features=52,
        n_classes=21,
        n_informative=30,
        random_state=42
    )

    X_df = pd.DataFrame(X, columns=[f'feature_{i:02d}' for i in range(52)])
    y_df = pd.Series(y)

    # ë¶„ì„ê¸° ìƒì„± ë° ì‹¤í–‰
    analyzer = PerformanceGapAnalyzer()
    results = analyzer.analyze_performance_gap(X_df, y_df)

    # ê¶Œì¥ì‚¬í•­ ì¶œë ¥
    recommendations = analyzer.get_recommendations()

    if recommendations:
        print("\nğŸ¯ ê°œì„  ê¶Œì¥ì‚¬í•­:")
        for rec in recommendations:
            print(f"  [{rec['priority']}] {rec['issue']}")
            print(f"      â†’ {rec['recommendation']}")

    print("\nâœ… T003 ì™„ë£Œ: ì„±ëŠ¥ ì§„ë‹¨ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì„±ê³µ!")
    print("ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: T004 (Adversarial Validation êµ¬í˜„)")
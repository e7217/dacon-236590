"""
Adversarial Validation êµ¬í˜„
T004 íƒœìŠ¤í¬: Train/Test ë°ì´í„° ë¶„í¬ ì°¨ì´ ë¶„ì„

ëª©ì :
- CV ì ìˆ˜ì™€ ì‹¤ì œ ì œì¶œ ì ìˆ˜ ê²©ì°¨(9.4%)ì˜ ì›ì¸ ê·œëª…
- Trainê³¼ Test ë°ì´í„° ë¶„í¬ ì°¨ì´ ì •ëŸ‰í™”
- ë°ì´í„° ë„ë©”ì¸ ë³€í™”(Domain Shift) íƒì§€
- ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²€ì¦ ì „ëµ ì œì‹œ

ë°©ë²•ë¡ :
1. Train + Test ë°ì´í„°ë¥¼ í•©ì³ì„œ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¡œ ë³€í™˜
2. Train=0, Test=1ë¡œ ë¼ë²¨ë§í•˜ì—¬ êµ¬ë³„ ê°€ëŠ¥ì„± ì¸¡ì •
3. AUCê°€ 0.5ì— ê°€ê¹Œìš°ë©´ ë¶„í¬ ìœ ì‚¬, 1.0ì— ê°€ê¹Œìš°ë©´ ë¶„í¬ ìƒì´
4. ì¤‘ìš”í•œ íŠ¹ì„±ë“¤ì„ ë¶„ì„í•˜ì—¬ ë¶„í¬ ì°¨ì´ì˜ ì›ì¸ íŒŒì•…
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, roc_curve, classification_report
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.tracking.experiment_tracker import ExperimentTracker
from src.utils.config import get_config, get_paths


class AdversarialValidator:
    """
    Adversarial Validation ë¶„ì„ê¸°
    Train/Test ë¶„í¬ ì°¨ì´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ CV-ì‹¤ì œ ì ìˆ˜ ê²©ì°¨ ì›ì¸ ê·œëª…
    """

    def __init__(self, use_tracking: bool = True):
        """
        Adversarial Validator ì´ˆê¸°í™”

        Args:
            use_tracking: ì‹¤í—˜ ì¶”ì  ì‚¬ìš© ì—¬ë¶€
        """
        self.use_tracking = use_tracking
        self.results = {}
        self.figures = {}

        # ê²½ë¡œ ì„¤ì •
        self.paths = get_paths()
        self.plots_dir = self.paths['experiments_dir'] / 'plots' / 'adversarial'
        self.plots_dir.mkdir(parents=True, exist_ok=True)

        # ì‹¤í—˜ ì¶”ì  ì„¤ì •
        if self.use_tracking:
            self.tracker = ExperimentTracker(
                project_name=get_config('tracking.project_name'),
                experiment_name="adversarial_validation"
            )

        print("ğŸ­ Adversarial Validation ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

    def validate_data_distribution(self, train_data, test_data, target_col=None):
        """
        Train/Test ë°ì´í„° ë¶„í¬ ì°¨ì´ ì¢…í•© ë¶„ì„

        Args:
            train_data: í›ˆë ¨ ë°ì´í„° (DataFrame)
            test_data: í…ŒìŠ¤íŠ¸ ë°ì´í„° (DataFrame)
            target_col: íƒ€ê²Ÿ ì»¬ëŸ¼ëª… (train_dataì—ì„œ ì œì™¸í•  ì»¬ëŸ¼)

        Returns:
            Dict: ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("\nğŸ­ Adversarial Validation ì¢…í•© ë¶„ì„ ì‹œì‘...")
        print("=" * 60)

        # ì‹¤í—˜ ì¶”ì  ì‹œì‘
        if self.use_tracking:
            self.tracker.start_run(
                run_name="adversarial_validation_analysis",
                description="Train/Test ë°ì´í„° ë¶„í¬ ì°¨ì´ ë¶„ì„ - CV ì ìˆ˜ ê²©ì°¨ ì›ì¸ ê·œëª…",
                tags={
                    "analysis_type": "adversarial_validation",
                    "task": "T004",
                    "purpose": "domain_shift_detection"
                }
            )

        # ë°ì´í„° ì „ì²˜ë¦¬ ë° ì¤€ë¹„
        X_train, X_test = self._prepare_data(train_data, test_data, target_col)

        # 1. ê¸°ë³¸ ë¶„í¬ ì°¨ì´ ë¶„ì„
        basic_stats = self._analyze_basic_statistics(X_train, X_test)

        # 2. Adversarial Validation ìˆ˜í–‰
        adversarial_results = self._perform_adversarial_validation(X_train, X_test)

        # 3. íŠ¹ì„±ë³„ ì¤‘ìš”ë„ ë¶„ì„
        feature_importance = self._analyze_feature_importance(
            X_train, X_test, adversarial_results['model']
        )

        # 4. ë¶„í¬ ì‹œê°í™” ìƒì„±
        visualization_results = self._create_distribution_visualizations(
            X_train, X_test, feature_importance
        )

        # 5. ê²€ì¦ ì „ëµ ì œì•ˆ
        validation_strategy = self._propose_validation_strategy(adversarial_results)

        # ê²°ê³¼ ì¢…í•©
        comprehensive_results = {
            'basic_statistics': basic_stats,
            'adversarial_results': adversarial_results,
            'feature_importance': feature_importance,
            'visualizations': visualization_results,
            'validation_strategy': validation_strategy,
            'data_info': {
                'train_shape': X_train.shape,
                'test_shape': X_test.shape,
                'feature_names': X_train.columns.tolist()
            }
        }

        self.results = comprehensive_results

        # ì‹¤í—˜ ì¶”ì  ë¡œê¹…
        if self.use_tracking:
            self._log_adversarial_results(comprehensive_results)

        print("\nâœ… Adversarial Validation ë¶„ì„ ì™„ë£Œ!")
        self._print_analysis_summary(comprehensive_results)

        return comprehensive_results

    def _prepare_data(self, train_data, test_data, target_col):
        """ë°ì´í„° ì „ì²˜ë¦¬ ë° ì¤€ë¹„"""
        print("\nğŸ“‹ 1ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬")

        # íƒ€ê²Ÿ ì»¬ëŸ¼ ì œê±° (ìˆëŠ” ê²½ìš°)
        if target_col and target_col in train_data.columns:
            X_train = train_data.drop(columns=[target_col])
        else:
            X_train = train_data.copy()

        X_test = test_data.copy()

        # ê³µí†µ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
        common_cols = list(set(X_train.columns) & set(X_test.columns))
        X_train = X_train[common_cols]
        X_test = X_test[common_cols]

        # ê²°ì¸¡ê°’ ì²˜ë¦¬ (ê°„ë‹¨í•œ ì¤‘ì•™ê°’ ëŒ€ì²´)
        for col in X_train.columns:
            if X_train[col].dtype in ['int64', 'float64']:
                median_val = X_train[col].median()
                X_train[col].fillna(median_val, inplace=True)
                X_test[col].fillna(median_val, inplace=True)

        print(f"  â€¢ Train ë°ì´í„°: {X_train.shape}")
        print(f"  â€¢ Test ë°ì´í„°: {X_test.shape}")
        print(f"  â€¢ ê³µí†µ íŠ¹ì„±: {len(common_cols)}ê°œ")

        return X_train, X_test

    def _analyze_basic_statistics(self, X_train, X_test):
        """ê¸°ë³¸ í†µê³„ëŸ‰ ë¹„êµ ë¶„ì„"""
        print("\nğŸ“Š 2ë‹¨ê³„: ê¸°ë³¸ í†µê³„ëŸ‰ ë¶„ì„")

        stats_comparison = {}

        for col in X_train.columns:
            train_stats = X_train[col].describe()
            test_stats = X_test[col].describe()

            # í†µê³„ì  ì°¨ì´ ê³„ì‚°
            mean_diff = abs(train_stats['mean'] - test_stats['mean'])
            std_diff = abs(train_stats['std'] - test_stats['std'])

            # ì •ê·œí™”ëœ ì°¨ì´ (í‰ê·  ëŒ€ë¹„)
            normalized_mean_diff = mean_diff / (abs(train_stats['mean']) + 1e-8)
            normalized_std_diff = std_diff / (train_stats['std'] + 1e-8)

            stats_comparison[col] = {
                'train_mean': train_stats['mean'],
                'test_mean': test_stats['mean'],
                'train_std': train_stats['std'],
                'test_std': test_stats['std'],
                'mean_diff': mean_diff,
                'std_diff': std_diff,
                'normalized_mean_diff': normalized_mean_diff,
                'normalized_std_diff': normalized_std_diff
            }

        # ê°€ì¥ ì°¨ì´ê°€ í° íŠ¹ì„±ë“¤ ì‹ë³„
        mean_diffs = {k: v['normalized_mean_diff'] for k, v in stats_comparison.items()}
        top_different_features = sorted(mean_diffs.items(), key=lambda x: x[1], reverse=True)[:10]

        results = {
            'stats_comparison': stats_comparison,
            'top_different_features': top_different_features,
            'overall_mean_diff': np.mean(list(mean_diffs.values())),
            'overall_std_diff': np.std(list(mean_diffs.values()))
        }

        print(f"  â€¢ ì „ì²´ í‰ê·  ì°¨ì´: {results['overall_mean_diff']:.4f}")
        print(f"  â€¢ ì°¨ì´ í‘œì¤€í¸ì°¨: {results['overall_std_diff']:.4f}")
        print(f"  â€¢ ê°€ì¥ ë‹¤ë¥¸ íŠ¹ì„±: {top_different_features[0][0]} ({top_different_features[0][1]:.4f})")

        return results

    def _perform_adversarial_validation(self, X_train, X_test):
        """Adversarial Validation ìˆ˜í–‰"""
        print("\nğŸ­ 3ë‹¨ê³„: Adversarial Validation ìˆ˜í–‰")

        # ë°ì´í„° ê²°í•© ë° ë¼ë²¨ ìƒì„±
        X_combined = pd.concat([X_train, X_test], ignore_index=True)
        y_combined = np.concatenate([
            np.zeros(len(X_train)),  # Train: 0
            np.ones(len(X_test))     # Test: 1
        ])

        # ì—¬ëŸ¬ ëª¨ë¸ë¡œ Adversarial Validation ìˆ˜í–‰
        models = {
            'RandomForest': RandomForestClassifier(
                n_estimators=100,
                random_state=get_config('models.random_seed'),
                n_jobs=-1
            ),
            'LogisticRegression': LogisticRegression(
                random_state=get_config('models.random_seed'),
                max_iter=1000
            )
        }

        results = {}

        for model_name, model in models.items():
            print(f"\n  ğŸ¤– {model_name} ë¶„ì„:")

            # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ (ë¡œì§€ìŠ¤í‹± íšŒê·€ìš©)
            if model_name == 'LogisticRegression':
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X_combined)
                X_model = pd.DataFrame(X_scaled, columns=X_combined.columns)
            else:
                X_model = X_combined

            # êµì°¨ê²€ì¦ìœ¼ë¡œ AUC ê³„ì‚°
            cv_scores = cross_val_score(
                model, X_model, y_combined,
                cv=5, scoring='roc_auc', n_jobs=-1
            )

            # ì „ì²´ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ (íŠ¹ì„± ì¤‘ìš”ë„ìš©)
            model.fit(X_model, y_combined)

            # ROC Curve ê³„ì‚°
            y_pred_proba = model.predict_proba(X_model)[:, 1]
            fpr, tpr, thresholds = roc_curve(y_combined, y_pred_proba)
            auc_score = roc_auc_score(y_combined, y_pred_proba)

            results[model_name] = {
                'cv_auc_mean': cv_scores.mean(),
                'cv_auc_std': cv_scores.std(),
                'full_auc': auc_score,
                'fpr': fpr,
                'tpr': tpr,
                'model': model,
                'predictions': y_pred_proba
            }

            print(f"    â€¢ CV AUC: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
            print(f"    â€¢ Full AUC: {auc_score:.4f}")

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
        best_model_name = max(results.keys(), key=lambda k: results[k]['cv_auc_mean'])
        best_result = results[best_model_name]

        # ROC Curve ì‹œê°í™”
        self._plot_roc_curve(results)

        # Domain shift ì •ë„ í•´ì„
        domain_shift_severity = self._interpret_domain_shift(best_result['cv_auc_mean'])

        final_results = {
            'all_models': results,
            'best_model_name': best_model_name,
            'best_auc': best_result['cv_auc_mean'],
            'domain_shift_severity': domain_shift_severity,
            'model': best_result['model']
        }

        print(f"\n  ğŸ† ìµœê³  ì„±ëŠ¥: {best_model_name} (AUC: {best_result['cv_auc_mean']:.4f})")
        print(f"  ğŸ“Š Domain Shift ì •ë„: {domain_shift_severity}")

        return final_results

    def _plot_roc_curve(self, results):
        """ROC Curve ì‹œê°í™”"""
        plt.figure(figsize=(10, 8))

        colors = ['blue', 'red', 'green', 'orange']

        for i, (model_name, result) in enumerate(results.items()):
            plt.plot(
                result['fpr'], result['tpr'],
                color=colors[i % len(colors)],
                linewidth=2,
                label=f'{model_name} (AUC = {result["full_auc"]:.3f})'
            )

        # ëŒ€ê°ì„  (ëœë¤ ë¶„ë¥˜ê¸°)
        plt.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5, label='Random Classifier')

        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve - Adversarial Validation\n(Train vs Test ë¶„í¬ êµ¬ë³„ ì„±ëŠ¥)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()

        # ê·¸ë˜í”„ ì €ì¥
        roc_path = self.plots_dir / 'adversarial_roc_curve.png'
        plt.savefig(roc_path, dpi=300, bbox_inches='tight')
        self.figures['roc_curve'] = plt.gcf()
        plt.close()

    def _interpret_domain_shift(self, auc_score):
        """Domain Shift ì •ë„ í•´ì„"""
        if auc_score < 0.53:
            return "ë§¤ìš° ë‚®ìŒ (ë¶„í¬ ê±°ì˜ ë™ì¼)"
        elif auc_score < 0.57:
            return "ë‚®ìŒ (ì‘ì€ ë¶„í¬ ì°¨ì´)"
        elif auc_score < 0.65:
            return "ë³´í†µ (ì£¼ì˜ í•„ìš”)"
        elif auc_score < 0.75:
            return "ë†’ìŒ (ì‹¬ê°í•œ ë¶„í¬ ì°¨ì´)"
        else:
            return "ë§¤ìš° ë†’ìŒ (ì‹¬ê°í•œ ë„ë©”ì¸ ë³€í™”)"

    def _analyze_feature_importance(self, X_train, X_test, model):
        """íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„"""
        print("\nğŸ” 4ë‹¨ê³„: íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„")

        # íŠ¹ì„± ì¤‘ìš”ë„ ì¶”ì¶œ
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
        elif hasattr(model, 'coef_'):
            importances = np.abs(model.coef_[0])
        else:
            print("  âš ï¸ í•´ë‹¹ ëª¨ë¸ì—ì„œ íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        # íŠ¹ì„± ì¤‘ìš”ë„ ì •ë ¬
        feature_names = X_train.columns
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False)

        # ìƒìœ„ 20ê°œ íŠ¹ì„± ì‹œê°í™”
        plt.figure(figsize=(12, 8))
        top_features = importance_df.head(20)

        plt.barh(range(len(top_features)), top_features['importance'])
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('Feature Importance')
        plt.title('Top 20 Features Distinguishing Train vs Test\n(High importance = Large distribution difference)')
        plt.gca().invert_yaxis()
        plt.tight_layout()

        # ê·¸ë˜í”„ ì €ì¥
        importance_path = self.plots_dir / 'feature_importance.png'
        plt.savefig(importance_path, dpi=300, bbox_inches='tight')
        self.figures['feature_importance'] = plt.gcf()
        plt.close()

        # ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„±ë“¤ì˜ ë¶„í¬ ì‹œê°í™”
        self._plot_top_feature_distributions(X_train, X_test, top_features['feature'].head(6))

        results = {
            'importance_df': importance_df,
            'top_features': top_features['feature'].head(10).tolist(),
            'top_importances': top_features['importance'].head(10).tolist(),
            'plot_path': str(importance_path)
        }

        print(f"  â€¢ ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„±: {results['top_features'][0]} ({results['top_importances'][0]:.4f})")
        print(f"  â€¢ ìƒìœ„ 3ê°œ íŠ¹ì„±: {', '.join(results['top_features'][:3])}")

        return results

    def _plot_top_feature_distributions(self, X_train, X_test, top_features):
        """ìƒìœ„ íŠ¹ì„±ë“¤ì˜ ë¶„í¬ ì‹œê°í™”"""
        n_features = len(top_features)
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        axes = axes.ravel()

        for i, feature in enumerate(top_features):
            if i >= 6:
                break

            ax = axes[i]

            # íˆìŠ¤í† ê·¸ë¨
            ax.hist(X_train[feature], bins=30, alpha=0.7, label='Train', density=True, color='blue')
            ax.hist(X_test[feature], bins=30, alpha=0.7, label='Test', density=True, color='red')

            ax.set_title(f'{feature}')
            ax.set_xlabel('Value')
            ax.set_ylabel('Density')
            ax.legend()
            ax.grid(True, alpha=0.3)

        plt.tight_layout()

        # ê·¸ë˜í”„ ì €ì¥
        distributions_path = self.plots_dir / 'top_features_distributions.png'
        plt.savefig(distributions_path, dpi=300, bbox_inches='tight')
        self.figures['feature_distributions'] = plt.gcf()
        plt.close()

    def _create_distribution_visualizations(self, X_train, X_test, feature_importance):
        """ë¶„í¬ ì‹œê°í™” ìƒì„±"""
        print("\nğŸ“ˆ 5ë‹¨ê³„: ë¶„í¬ ì‹œê°í™” ìƒì„±")

        # ì „ì²´ ë°ì´í„°ì˜ ì°¨ì› ì¶•ì†Œ ì‹œê°í™” (PCA)
        from sklearn.decomposition import PCA
        from sklearn.preprocessing import StandardScaler

        # ë°ì´í„° ê²°í•©
        X_combined = pd.concat([X_train, X_test], ignore_index=True)
        labels = ['Train'] * len(X_train) + ['Test'] * len(X_test)

        # ìŠ¤ì¼€ì¼ë§ ë° PCA
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X_combined)

        pca = PCA(n_components=2, random_state=get_config('models.random_seed'))
        X_pca = pca.fit_transform(X_scaled)

        # PCA ì‹œê°í™”
        plt.figure(figsize=(12, 8))

        train_mask = np.array(labels) == 'Train'
        test_mask = np.array(labels) == 'Test'

        plt.scatter(X_pca[train_mask, 0], X_pca[train_mask, 1],
                   alpha=0.6, label='Train', color='blue', s=20)
        plt.scatter(X_pca[test_mask, 0], X_pca[test_mask, 1],
                   alpha=0.6, label='Test', color='red', s=20)

        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
        plt.title('PCA Visualization: Train vs Test Distribution')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()

        # ê·¸ë˜í”„ ì €ì¥
        pca_path = self.plots_dir / 'pca_train_test_distribution.png'
        plt.savefig(pca_path, dpi=300, bbox_inches='tight')
        self.figures['pca_distribution'] = plt.gcf()
        plt.close()

        results = {
            'pca_explained_variance': pca.explained_variance_ratio_.tolist(),
            'pca_plot_path': str(pca_path)
        }

        print(f"  â€¢ PCA ì„¤ëª… ë¶„ì‚°: PC1={pca.explained_variance_ratio_[0]:.2%}, PC2={pca.explained_variance_ratio_[1]:.2%}")
        print(f"  â€¢ ì´ ì„¤ëª… ë¶„ì‚°: {sum(pca.explained_variance_ratio_):.2%}")

        return results

    def _propose_validation_strategy(self, adversarial_results):
        """ê²€ì¦ ì „ëµ ì œì•ˆ"""
        print("\nğŸ’¡ 6ë‹¨ê³„: ê²€ì¦ ì „ëµ ì œì•ˆ")

        auc_score = adversarial_results['best_auc']
        strategies = []

        # AUC ì ìˆ˜ì— ë”°ë¥¸ ì „ëµ ì œì•ˆ
        if auc_score >= 0.75:
            strategies.extend([
                "ğŸš¨ ì‹¬ê°í•œ Domain Shift ê°ì§€",
                "â€¢ TimeSeriesSplit ë˜ëŠ” GroupKFold ì‚¬ìš© ê³ ë ¤",
                "â€¢ Test ë°ì´í„°ì™€ ìœ ì‚¬í•œ ë¶„í¬ì˜ validation set êµ¬ì„±",
                "â€¢ Domain Adaptation ê¸°ë²• ì ìš©",
                "â€¢ íŠ¹ì„± ë¶„í¬ë¥¼ testì™€ ìœ ì‚¬í•˜ê²Œ ì¡°ì •"
            ])
        elif auc_score >= 0.65:
            strategies.extend([
                "âš ï¸ ì£¼ì˜ í•„ìš”í•œ ë¶„í¬ ì°¨ì´ ê°ì§€",
                "â€¢ Stratified sampling ê°•í™”",
                "â€¢ Cross-validation í´ë“œ ìˆ˜ ì¦ê°€",
                "â€¢ ì£¼ìš” ì°¨ë³„ íŠ¹ì„±ì— ëŒ€í•œ ì¶”ê°€ ë¶„ì„ í•„ìš”"
            ])
        elif auc_score >= 0.57:
            strategies.extend([
                "ğŸ“Š ì‘ì€ ë¶„í¬ ì°¨ì´ ê°ì§€",
                "â€¢ í˜„ì¬ CV ì „ëµ ìœ ì§€ ê°€ëŠ¥",
                "â€¢ ì •ê¸°ì ì¸ adversarial validation ëª¨ë‹ˆí„°ë§",
                "â€¢ ì•™ìƒë¸” ë°©ë²•ìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ"
            ])
        else:
            strategies.extend([
                "âœ… ë¶„í¬ ì°¨ì´ ê±°ì˜ ì—†ìŒ",
                "â€¢ í˜„ì¬ CV ì „ëµì´ ì‹ ë¢°í•  ë§Œí•¨",
                "â€¢ CV ì ìˆ˜ê°€ ì‹¤ì œ ì„±ëŠ¥ì„ ì˜ ë°˜ì˜í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒ"
            ])

        # ì¶”ê°€ ì¼ë°˜ì  ê¶Œì¥ì‚¬í•­
        strategies.extend([
            "\nğŸ”§ ì¶”ê°€ ê¶Œì¥ì‚¬í•­:",
            "â€¢ ì¤‘ìš”í•œ ì°¨ë³„ íŠ¹ì„±ë“¤ì— ëŒ€í•œ ë„ë©”ì¸ ì „ë¬¸ê°€ ê²€í† ",
            "â€¢ ë°ì´í„° ìˆ˜ì§‘ ê³¼ì •ì—ì„œì˜ systematic bias ì¡°ì‚¬",
            "â€¢ Pseudo-labeling ê¸°ë²• ê³ ë ¤ (ì‹ ì¤‘í•˜ê²Œ ì ìš©)"
        ])

        results = {
            'auc_score': auc_score,
            'severity_level': adversarial_results['domain_shift_severity'],
            'strategies': strategies,
            'recommended_cv': self._recommend_cv_strategy(auc_score)
        }

        print(f"  â€¢ ì‹¬ê°ë„ ìˆ˜ì¤€: {results['severity_level']}")
        print(f"  â€¢ ê¶Œì¥ CV ì „ëµ: {results['recommended_cv']}")

        return results

    def _recommend_cv_strategy(self, auc_score):
        """AUC ì ìˆ˜ ê¸°ë°˜ CV ì „ëµ ì¶”ì²œ"""
        if auc_score >= 0.75:
            return "TimeSeriesSplit or Custom Domain-Aware Split"
        elif auc_score >= 0.65:
            return "StratifiedKFold with increased folds (10+)"
        elif auc_score >= 0.57:
            return "StratifiedKFold (current strategy acceptable)"
        else:
            return "Standard StratifiedKFold (highly reliable)"

    def _log_adversarial_results(self, results):
        """ì‹¤í—˜ ì¶”ì ì— ê²°ê³¼ ë¡œê¹…"""
        # ì£¼ìš” ë©”íŠ¸ë¦­ ë¡œê¹…
        metrics = {
            'adversarial_auc': results['adversarial_results']['best_auc'],
            'domain_shift_severity_score': self._get_severity_score(
                results['adversarial_results']['domain_shift_severity']
            ),
            'top_feature_importance': results['feature_importance']['top_importances'][0] if results['feature_importance']['top_importances'] else 0,
            'overall_distribution_difference': results['basic_statistics']['overall_mean_diff'],
            'pca_explained_variance_total': sum(results['visualizations']['pca_explained_variance'])
        }

        # íŒŒë¼ë¯¸í„° ë¡œê¹…
        params = {
            'best_model': results['adversarial_results']['best_model_name'],
            'train_samples': results['data_info']['train_shape'][0],
            'test_samples': results['data_info']['test_shape'][0],
            'n_features': results['data_info']['train_shape'][1],
            'recommended_cv': results['validation_strategy']['recommended_cv']
        }

        self.tracker.log_metrics(metrics)
        self.tracker.log_params(params)

        # ì‹œê°í™” ë¡œê¹…
        for fig_name, fig in self.figures.items():
            self.tracker.log_figure(fig, fig_name)

        self.tracker.end_run()

    def _get_severity_score(self, severity_text):
        """ì‹¬ê°ë„ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜"""
        severity_map = {
            "ë§¤ìš° ë‚®ìŒ (ë¶„í¬ ê±°ì˜ ë™ì¼)": 1,
            "ë‚®ìŒ (ì‘ì€ ë¶„í¬ ì°¨ì´)": 2,
            "ë³´í†µ (ì£¼ì˜ í•„ìš”)": 3,
            "ë†’ìŒ (ì‹¬ê°í•œ ë¶„í¬ ì°¨ì´)": 4,
            "ë§¤ìš° ë†’ìŒ (ì‹¬ê°í•œ ë„ë©”ì¸ ë³€í™”)": 5
        }
        return severity_map.get(severity_text, 3)

    def _print_analysis_summary(self, results):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*70)
        print("ğŸ­ Adversarial Validation ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*70)

        adv_results = results['adversarial_results']
        print(f"\nğŸ¯ í•µì‹¬ ê²°ê³¼:")
        print(f"  â€¢ Adversarial AUC: {adv_results['best_auc']:.4f}")
        print(f"  â€¢ Domain Shift ì •ë„: {adv_results['domain_shift_severity']}")
        print(f"  â€¢ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {adv_results['best_model_name']}")

        feat_imp = results['feature_importance']
        if feat_imp['top_features']:
            print(f"\nğŸ” ì£¼ìš” ì°¨ë³„ íŠ¹ì„±:")
            for i, (feature, importance) in enumerate(zip(feat_imp['top_features'][:3], feat_imp['top_importances'][:3])):
                print(f"  {i+1}. {feature}: {importance:.4f}")

        val_strategy = results['validation_strategy']
        print(f"\nğŸ’¡ ê¶Œì¥ ê²€ì¦ ì „ëµ:")
        print(f"  â€¢ {val_strategy['recommended_cv']}")

        print(f"\nğŸ“Š ë°ì´í„° ì •ë³´:")
        data_info = results['data_info']
        print(f"  â€¢ Train: {data_info['train_shape']}")
        print(f"  â€¢ Test: {data_info['test_shape']}")

        print("\nğŸ¨ ìƒì„±ëœ ì‹œê°í™”:")
        print(f"  â€¢ ROC Curve: {self.plots_dir}/adversarial_roc_curve.png")
        print(f"  â€¢ íŠ¹ì„± ì¤‘ìš”ë„: {self.plots_dir}/feature_importance.png")
        print(f"  â€¢ íŠ¹ì„± ë¶„í¬ ë¹„êµ: {self.plots_dir}/top_features_distributions.png")
        print(f"  â€¢ PCA ì‹œê°í™”: {self.plots_dir}/pca_train_test_distribution.png")

        print("="*70)

    def get_recommendations(self):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ êµ¬ì²´ì  ê¶Œì¥ì‚¬í•­"""
        if not self.results:
            print("âš ï¸ ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return []

        recommendations = []
        auc_score = self.results['adversarial_results']['best_auc']

        # AUC ì ìˆ˜ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if auc_score >= 0.75:
            recommendations.extend([
                {
                    'priority': 'CRITICAL',
                    'category': 'validation_strategy',
                    'issue': f'ì‹¬ê°í•œ Domain Shift ê°ì§€ (AUC: {auc_score:.3f})',
                    'recommendation': 'TimeSeriesSplit ë˜ëŠ” domain-aware validation ì „ëµ ì ìš©',
                    'expected_impact': 'CV ì‹ ë¢°ë„ ëŒ€í­ í–¥ìƒ'
                },
                {
                    'priority': 'HIGH',
                    'category': 'feature_engineering',
                    'issue': 'Train/Test ë¶„í¬ ì°¨ì´ê°€ ë§¤ìš° í¼',
                    'recommendation': 'Domain adaptation ê¸°ë²• ë˜ëŠ” ë¶„í¬ ì •ë ¬ ê¸°ë²• ì ìš©',
                    'expected_impact': 'ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ'
                }
            ])
        elif auc_score >= 0.65:
            recommendations.append({
                'priority': 'HIGH',
                'category': 'validation_strategy',
                'issue': f'ì£¼ì˜ í•„ìš”í•œ ë¶„í¬ ì°¨ì´ (AUC: {auc_score:.3f})',
                'recommendation': 'StratifiedKFold í´ë“œ ìˆ˜ ì¦ê°€, ë” ì‹ ì¤‘í•œ ê²€ì¦',
                'expected_impact': 'CV ì•ˆì •ì„± í–¥ìƒ'
            })

        # ì¤‘ìš” íŠ¹ì„± ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if self.results['feature_importance']['top_features']:
            top_feature = self.results['feature_importance']['top_features'][0]
            recommendations.append({
                'priority': 'MEDIUM',
                'category': 'data_analysis',
                'issue': f'ì£¼ìš” ì°¨ë³„ íŠ¹ì„± ë°œê²¬: {top_feature}',
                'recommendation': f'{top_feature} íŠ¹ì„±ì˜ ë¶„í¬ ì°¨ì´ ì›ì¸ ì¡°ì‚¬',
                'expected_impact': 'ë°ì´í„° í’ˆì§ˆ í–¥ìƒ'
            })

        return recommendations


if __name__ == "__main__":
    """Adversarial Validation í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Adversarial Validation í…ŒìŠ¤íŠ¸ ì‹œì‘...")

    # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ (ì‹¤ì œì™€ ìœ ì‚¬í•˜ê²Œ ë¶„í¬ ì°¨ì´ ìƒì„±)
    np.random.seed(42)

    # Train ë°ì´í„° ìƒì„±
    n_train, n_test = 800, 200
    n_features = 20

    X_train = pd.DataFrame(
        np.random.randn(n_train, n_features),
        columns=[f'feature_{i:02d}' for i in range(n_features)]
    )

    # Test ë°ì´í„° ìƒì„± (ì¼ë¶€ íŠ¹ì„±ì— ë¶„í¬ ë³€í™” ì¶”ê°€)
    X_test = pd.DataFrame(
        np.random.randn(n_test, n_features),
        columns=[f'feature_{i:02d}' for i in range(n_features)]
    )

    # ì¸ìœ„ì ì¸ ë¶„í¬ ì°¨ì´ ìƒì„± (ì²˜ìŒ 3ê°œ íŠ¹ì„±)
    X_test.iloc[:, :3] += 0.5  # í‰ê·  ì´ë™
    X_test.iloc[:, :3] *= 1.2  # ë¶„ì‚° ì¦ê°€

    # ë¶„ì„ ì‹¤í–‰
    validator = AdversarialValidator()
    results = validator.validate_data_distribution(X_train, X_test)

    # ê¶Œì¥ì‚¬í•­ ì¶œë ¥
    recommendations = validator.get_recommendations()
    if recommendations:
        print("\nğŸ¯ ê°œì„  ê¶Œì¥ì‚¬í•­:")
        for rec in recommendations:
            print(f"  [{rec['priority']}] {rec['issue']}")
            print(f"      â†’ {rec['recommendation']}")

    print("\nâœ… T004 ì™„ë£Œ: Adversarial Validation êµ¬í˜„ ì„±ê³µ!")
    print("ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: T005 (êµì°¨ê²€ì¦ ì „ëµ ë¹„êµ ë¶„ì„)")
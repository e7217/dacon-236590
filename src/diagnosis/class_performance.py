"""
í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ ëª¨ë“ˆ (Class Performance Analysis)

ì´ ëª¨ë“ˆì€ 21ê°œ í´ë˜ìŠ¤ì˜ ê°œë³„ ì„±ëŠ¥ì„ ë¶„ì„í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.
- í´ë˜ìŠ¤ë³„ F1-score, Precision, Recall ë¶„ì„
- í˜¼ë™í–‰ë ¬(Confusion Matrix) ì‹œê°í™”
- í´ë˜ìŠ¤ ë¶ˆê· í˜• ì˜í–¥ ë¶„ì„
- ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­ ì œê³µ

ì‘ì„±ì: Claude
ë‚ ì§œ: 2024-01-01
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    classification_report, confusion_matrix, f1_score, precision_score, recall_score
)
from sklearn.model_selection import cross_val_predict
from sklearn.ensemble import RandomForestClassifier
from typing import Dict, List, Tuple, Optional, Any
import warnings
warnings.filterwarnings('ignore')

from ..tracking.experiment_tracker import ExperimentTracker
from ..utils.config import Config

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['DejaVu Sans', 'Malgun Gothic', 'AppleGothic']
plt.rcParams['axes.unicode_minus'] = False

class ClassPerformanceAnalyzer:
    """
    21ê°œ í´ë˜ìŠ¤ì˜ ê°œë³„ ì„±ëŠ¥ì„ ë¶„ì„í•˜ëŠ” í´ë˜ìŠ¤

    ì£¼ìš” ê¸°ëŠ¥:
    1. í´ë˜ìŠ¤ë³„ ìƒì„¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
    2. í´ë˜ìŠ¤ ë¶ˆê· í˜• ì˜í–¥ ë¶„ì„
    3. í˜¼ë™í–‰ë ¬ ì‹œê°í™”
    4. ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±
    """

    def __init__(self, config: Config = None, experiment_tracker: ExperimentTracker = None):
        self.config = config or Config()
        self.tracker = experiment_tracker
        self.class_performance = {}
        self.confusion_matrices = {}
        self.recommendations = []

        # 21ê°œ í´ë˜ìŠ¤ ì •ë³´
        self.class_names = [f'Class_{i}' for i in range(21)]

    def analyze_class_performance(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: pd.DataFrame = None,
        y_val: pd.Series = None,
        models: Dict[str, Any] = None,
        cv_folds: int = 5
    ) -> Dict[str, Any]:
        """
        í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ì„ ì¢…í•© ë¶„ì„í•©ë‹ˆë‹¤.

        Args:
            X_train: í›ˆë ¨ ë°ì´í„° íŠ¹ì„±
            y_train: í›ˆë ¨ ë°ì´í„° ë ˆì´ë¸”
            X_val: ê²€ì¦ ë°ì´í„° íŠ¹ì„±
            y_val: ê²€ì¦ ë°ì´í„° ë ˆì´ë¸”
            models: ë¶„ì„í•  ëª¨ë¸ ë”•ì…”ë„ˆë¦¬
            cv_folds: êµì°¨ê²€ì¦ í´ë“œ ìˆ˜

        Returns:
            Dict: í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼
        """
        print("ğŸ¯ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

        if self.tracker:
            self.tracker.start_run(
                run_name="class_performance_analysis",
                description="21ê°œ í´ë˜ìŠ¤ì˜ ê°œë³„ ì„±ëŠ¥ ë¶„ì„",
                tags={"analysis_type": "class_performance", "task": "T007"}
            )

        # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
        if models is None:
            models = {
                'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
                'RandomForest_Balanced': RandomForestClassifier(
                    n_estimators=100, class_weight='balanced', random_state=42
                )
            }

        # í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„
        class_distribution = self._analyze_class_distribution(y_train)

        # ê° ëª¨ë¸ë³„ í´ë˜ìŠ¤ ì„±ëŠ¥ ë¶„ì„
        model_results = {}
        for model_name, model in models.items():
            print(f"\nğŸ“Š {model_name} ëª¨ë¸ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")

            # êµì°¨ê²€ì¦ìœ¼ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
            y_pred_cv = cross_val_predict(model, X_train, y_train, cv=cv_folds, method='predict')

            # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ê³„ì‚°
            class_metrics = self._calculate_class_metrics(y_train, y_pred_cv)

            # í˜¼ë™í–‰ë ¬ ìƒì„±
            cm = confusion_matrix(y_train, y_pred_cv)

            model_results[model_name] = {
                'class_metrics': class_metrics,
                'confusion_matrix': cm,
                'overall_f1': f1_score(y_train, y_pred_cv, average='macro')
            }

        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¹„êµ ë¶„ì„
        comparison_analysis = self._compare_class_performance(model_results)

        # ì‹œê°í™” ìƒì„±
        self._create_visualizations(class_distribution, model_results, comparison_analysis)

        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        self._generate_recommendations(class_distribution, comparison_analysis)

        # ê²°ê³¼ í†µí•©
        analysis_result = {
            'class_distribution': class_distribution,
            'model_results': model_results,
            'comparison_analysis': comparison_analysis,
            'recommendations': self.recommendations,
            'summary': self._generate_summary(model_results, comparison_analysis)
        }

        # ì‹¤í—˜ ì¶”ì ì— ê²°ê³¼ ê¸°ë¡
        if self.tracker:
            self._log_results_to_tracker(analysis_result)

        print("âœ… í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        return analysis_result

    def _analyze_class_distribution(self, y: pd.Series) -> Dict[str, Any]:
        """í´ë˜ìŠ¤ ë¶„í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        print("ğŸ“ˆ í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„ ì¤‘...")

        class_counts = y.value_counts().sort_index()
        total_samples = len(y)

        distribution_stats = {
            'counts': class_counts.to_dict(),
            'percentages': (class_counts / total_samples * 100).to_dict(),
            'total_samples': total_samples,
            'num_classes': len(class_counts),
            'min_samples': class_counts.min(),
            'max_samples': class_counts.max(),
            'imbalance_ratio': class_counts.max() / class_counts.min(),
            'std_deviation': class_counts.std()
        }

        # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì‹¬ê°ë„ í‰ê°€
        imbalance_severity = self._assess_imbalance_severity(distribution_stats)
        distribution_stats['imbalance_severity'] = imbalance_severity

        return distribution_stats

    def _assess_imbalance_severity(self, stats: Dict) -> str:
        """í´ë˜ìŠ¤ ë¶ˆê· í˜• ì‹¬ê°ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤."""
        ratio = stats['imbalance_ratio']

        if ratio <= 2:
            return "ë‚®ìŒ (Balanced)"
        elif ratio <= 5:
            return "ë³´í†µ (Moderate)"
        elif ratio <= 10:
            return "ë†’ìŒ (High)"
        else:
            return "ë§¤ìš° ë†’ìŒ (Severe)"

    def _calculate_class_metrics(self, y_true: pd.Series, y_pred: np.ndarray) -> Dict[str, Dict[str, float]]:
        """í´ë˜ìŠ¤ë³„ ìƒì„¸ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""

        # ì „ì²´ ë¶„ë¥˜ ë³´ê³ ì„œ
        report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)

        class_metrics = {}
        for class_idx in range(21):
            class_key = str(class_idx)
            if class_key in report:
                class_metrics[class_idx] = {
                    'f1_score': report[class_key]['f1-score'],
                    'precision': report[class_key]['precision'],
                    'recall': report[class_key]['recall'],
                    'support': int(report[class_key]['support'])
                }
            else:
                # í•´ë‹¹ í´ë˜ìŠ¤ê°€ ì˜ˆì¸¡ë˜ì§€ ì•Šì€ ê²½ìš°
                class_metrics[class_idx] = {
                    'f1_score': 0.0,
                    'precision': 0.0,
                    'recall': 0.0,
                    'support': 0
                }

        return class_metrics

    def _compare_class_performance(self, model_results: Dict) -> Dict[str, Any]:
        """ëª¨ë¸ ê°„ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤."""
        print("âš–ï¸ ëª¨ë¸ ê°„ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¹„êµ ì¤‘...")

        comparison = {
            'best_model_per_class': {},
            'worst_performing_classes': [],
            'best_performing_classes': [],
            'performance_variance': {},
            'class_difficulty_ranking': {}
        }

        # ê° í´ë˜ìŠ¤ë³„ë¡œ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        for class_idx in range(21):
            best_f1 = 0
            best_model = None
            f1_scores = []

            for model_name, results in model_results.items():
                class_f1 = results['class_metrics'].get(class_idx, {}).get('f1_score', 0)
                f1_scores.append(class_f1)

                if class_f1 > best_f1:
                    best_f1 = class_f1
                    best_model = model_name

            comparison['best_model_per_class'][class_idx] = {
                'model': best_model,
                'f1_score': best_f1
            }

            # ì„±ëŠ¥ ë¶„ì‚° ê³„ì‚°
            comparison['performance_variance'][class_idx] = np.std(f1_scores)

        # ìµœê³ /ìµœì € ì„±ëŠ¥ í´ë˜ìŠ¤ ì‹ë³„
        class_best_f1s = [(idx, data['f1_score']) for idx, data in comparison['best_model_per_class'].items()]
        class_best_f1s.sort(key=lambda x: x[1])

        comparison['worst_performing_classes'] = [
            {'class': idx, 'f1_score': f1} for idx, f1 in class_best_f1s[:5]
        ]
        comparison['best_performing_classes'] = [
            {'class': idx, 'f1_score': f1} for idx, f1 in class_best_f1s[-5:]
        ]

        # í´ë˜ìŠ¤ ë‚œì´ë„ ìˆœìœ„
        comparison['class_difficulty_ranking'] = {
            idx: {'rank': rank + 1, 'f1_score': f1}
            for rank, (idx, f1) in enumerate(class_best_f1s)
        }

        return comparison

    def _create_visualizations(
        self,
        class_distribution: Dict,
        model_results: Dict,
        comparison_analysis: Dict
    ):
        """í´ë˜ìŠ¤ ì„±ëŠ¥ ê´€ë ¨ ì‹œê°í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        print("ğŸ“Š í´ë˜ìŠ¤ ì„±ëŠ¥ ì‹œê°í™” ìƒì„± ì¤‘...")

        # ì‹œê°í™” ì €ì¥ ê²½ë¡œ
        plots_dir = self.config.get_paths()['plots'] / 'class_performance'
        plots_dir.mkdir(exist_ok=True)

        # 1. í´ë˜ìŠ¤ ë¶„í¬ ì‹œê°í™”
        self._plot_class_distribution(class_distribution, plots_dir)

        # 2. í´ë˜ìŠ¤ë³„ F1-score ë¹„êµ
        self._plot_class_f1_comparison(model_results, plots_dir)

        # 3. í˜¼ë™í–‰ë ¬ íˆíŠ¸ë§µ
        self._plot_confusion_matrices(model_results, plots_dir)

        # 4. í´ë˜ìŠ¤ ë‚œì´ë„ ë¶„ì„
        self._plot_class_difficulty_analysis(comparison_analysis, class_distribution, plots_dir)

        # 5. ì„±ëŠ¥-ìƒ˜í”Œìˆ˜ ìƒê´€ê´€ê³„
        self._plot_performance_vs_samples(comparison_analysis, class_distribution, plots_dir)

    def _plot_class_distribution(self, class_distribution: Dict, plots_dir):
        """í´ë˜ìŠ¤ ë¶„í¬ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."""

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜
        classes = list(range(21))
        counts = [class_distribution['counts'].get(i, 0) for i in classes]

        ax1.bar(classes, counts, color='skyblue', alpha=0.7)
        ax1.set_title('í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ ë¶„í¬', fontsize=14, fontweight='bold')
        ax1.set_xlabel('í´ë˜ìŠ¤ ë²ˆí˜¸')
        ax1.set_ylabel('ìƒ˜í”Œ ìˆ˜')
        ax1.grid(True, alpha=0.3)

        # ë¶ˆê· í˜• ì •ë„ ì‹œê°í™” (ë¹„ìœ¨)
        percentages = [class_distribution['percentages'].get(i, 0) for i in classes]
        colors = plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(classes)))

        ax2.bar(classes, percentages, color=colors)
        ax2.set_title('í´ë˜ìŠ¤ë³„ ë¹„ìœ¨ ë¶„í¬', fontsize=14, fontweight='bold')
        ax2.set_xlabel('í´ë˜ìŠ¤ ë²ˆí˜¸')
        ax2.set_ylabel('ë¹„ìœ¨ (%)')
        ax2.grid(True, alpha=0.3)

        # í†µê³„ ì •ë³´ ì¶”ê°€
        stats_text = f"""ë¶ˆê· í˜• ì‹¬ê°ë„: {class_distribution['imbalance_severity']}
        ë¶ˆê· í˜• ë¹„ìœ¨: {class_distribution['imbalance_ratio']:.2f}
        ìµœì†Œ ìƒ˜í”Œ: {class_distribution['min_samples']}
        ìµœëŒ€ ìƒ˜í”Œ: {class_distribution['max_samples']}"""

        fig.suptitle('í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„', fontsize=16, fontweight='bold')
        fig.text(0.02, 0.02, stats_text, fontsize=10,
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.7))

        plt.tight_layout()
        plt.savefig(plots_dir / 'class_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()

        if self.tracker:
            self.tracker.log_artifact(str(plots_dir / 'class_distribution.png'), "class_distribution")

    def _plot_class_f1_comparison(self, model_results: Dict, plots_dir):
        """í´ë˜ìŠ¤ë³„ F1-scoreë¥¼ ë¹„êµ ì‹œê°í™”í•©ë‹ˆë‹¤."""

        fig, ax = plt.subplots(figsize=(16, 8))

        classes = list(range(21))
        x = np.arange(len(classes))
        width = 0.35

        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']

        for i, (model_name, results) in enumerate(model_results.items()):
            f1_scores = [results['class_metrics'].get(cls, {}).get('f1_score', 0) for cls in classes]
            offset = (i - len(model_results)/2 + 0.5) * width / len(model_results)

            bars = ax.bar(x + offset, f1_scores, width/len(model_results),
                         label=model_name, color=colors[i % len(colors)], alpha=0.8)

            # ê°’ í‘œì‹œ (0.5 ì´í•˜ë§Œ)
            for j, bar in enumerate(bars):
                height = bar.get_height()
                if height < 0.5:
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           f'{height:.2f}', ha='center', va='bottom', fontsize=8)

        ax.set_title('í´ë˜ìŠ¤ë³„ F1-Score ë¹„êµ', fontsize=16, fontweight='bold')
        ax.set_xlabel('í´ë˜ìŠ¤ ë²ˆí˜¸', fontsize=12)
        ax.set_ylabel('F1-Score', fontsize=12)
        ax.set_xticks(x)
        ax.set_xticklabels(classes)
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_ylim(0, 1.1)

        # ëª©í‘œ ì„±ëŠ¥ ë¼ì¸ ì¶”ê°€
        ax.axhline(y=0.9, color='red', linestyle='--', alpha=0.7, label='ëª©í‘œ ì„±ëŠ¥ (0.9)')
        ax.axhline(y=0.67596, color='orange', linestyle='--', alpha=0.7, label='í˜„ì¬ ì „ì²´ ì„±ëŠ¥')

        plt.xticks(rotation=0)
        plt.tight_layout()
        plt.savefig(plots_dir / 'class_f1_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()

        if self.tracker:
            self.tracker.log_artifact(str(plots_dir / 'class_f1_comparison.png'), "class_f1_comparison")

    def _plot_confusion_matrices(self, model_results: Dict, plots_dir):
        """í˜¼ë™í–‰ë ¬ë“¤ì„ ì‹œê°í™”í•©ë‹ˆë‹¤."""

        n_models = len(model_results)
        fig, axes = plt.subplots(1, n_models, figsize=(8*n_models, 6))
        if n_models == 1:
            axes = [axes]

        for i, (model_name, results) in enumerate(model_results.items()):
            cm = results['confusion_matrix']

            # ì •ê·œí™”ëœ í˜¼ë™í–‰ë ¬
            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

            im = axes[i].imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)
            axes[i].set_title(f'{model_name}\nConfusion Matrix', fontweight='bold')

            # ì»¬ëŸ¬ë°” ì¶”ê°€
            plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)

            # ì¶• ë ˆì´ë¸”
            axes[i].set_xlabel('ì˜ˆì¸¡ í´ë˜ìŠ¤')
            axes[i].set_ylabel('ì‹¤ì œ í´ë˜ìŠ¤')

            # í‹± ì„¤ì •
            tick_marks = np.arange(21)
            axes[i].set_xticks(tick_marks[::2])  # 2ê°œì”© ê±´ë„ˆë›°ê¸°
            axes[i].set_yticks(tick_marks[::2])
            axes[i].set_xticklabels([str(i) for i in tick_marks[::2]])
            axes[i].set_yticklabels([str(i) for i in tick_marks[::2]])

        plt.suptitle('ëª¨ë¸ë³„ í˜¼ë™í–‰ë ¬ ë¹„êµ', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(plots_dir / 'confusion_matrices.png', dpi=300, bbox_inches='tight')
        plt.close()

        if self.tracker:
            self.tracker.log_artifact(str(plots_dir / 'confusion_matrices.png'), "confusion_matrices")

    def _plot_class_difficulty_analysis(self, comparison_analysis: Dict, class_distribution: Dict, plots_dir):
        """í´ë˜ìŠ¤ ë‚œì´ë„ ë¶„ì„ì„ ì‹œê°í™”í•©ë‹ˆë‹¤."""

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

        # 1. í´ë˜ìŠ¤ ë‚œì´ë„ ìˆœìœ„
        difficulty_data = comparison_analysis['class_difficulty_ranking']
        classes = list(difficulty_data.keys())
        f1_scores = [difficulty_data[cls]['f1_score'] for cls in classes]
        ranks = [difficulty_data[cls]['rank'] for cls in classes]

        # ë‚œì´ë„ë³„ ìƒ‰ìƒ (ë‚®ì€ F1 = ì–´ë ¤ìš´ í´ë˜ìŠ¤ = ë¹¨ê°„ìƒ‰)
        colors = plt.cm.RdYlGn([score for score in f1_scores])

        bars1 = ax1.bar(classes, f1_scores, color=colors, alpha=0.8)
        ax1.set_title('í´ë˜ìŠ¤ë³„ ë‚œì´ë„ ë¶„ì„\n(ë‚®ì€ F1-Score = ì–´ë ¤ìš´ í´ë˜ìŠ¤)', fontweight='bold')
        ax1.set_xlabel('í´ë˜ìŠ¤ ë²ˆí˜¸')
        ax1.set_ylabel('ìµœê³  F1-Score')
        ax1.grid(True, alpha=0.3)

        # ì–´ë ¤ìš´ í´ë˜ìŠ¤ ê°•ì¡°
        worst_classes = [item['class'] for item in comparison_analysis['worst_performing_classes']]
        for i, (bar, cls) in enumerate(zip(bars1, classes)):
            if cls in worst_classes:
                ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,
                        f'ì–´ë ¤ì›€\n{bar.get_height():.2f}',
                        ha='center', va='bottom', fontweight='bold', color='red')

        # 2. ì„±ëŠ¥ vs ìƒ˜í”Œ ìˆ˜ ìƒê´€ê´€ê³„
        sample_counts = [class_distribution['counts'].get(cls, 0) for cls in classes]

        scatter = ax2.scatter(sample_counts, f1_scores, c=f1_scores, cmap='RdYlGn',
                            s=100, alpha=0.7, edgecolors='black', linewidth=0.5)

        # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        correlation = np.corrcoef(sample_counts, f1_scores)[0, 1]

        ax2.set_title(f'ì„±ëŠ¥ vs ìƒ˜í”Œ ìˆ˜ ìƒê´€ê´€ê³„\nìƒê´€ê³„ìˆ˜: {correlation:.3f}', fontweight='bold')
        ax2.set_xlabel('ìƒ˜í”Œ ìˆ˜')
        ax2.set_ylabel('ìµœê³  F1-Score')
        ax2.grid(True, alpha=0.3)

        # ì»¬ëŸ¬ë°”
        plt.colorbar(scatter, ax=ax2, label='F1-Score')

        # ë¬¸ì œ í´ë˜ìŠ¤ ë ˆì´ë¸”ë§
        for cls in worst_classes[:3]:  # ìƒìœ„ 3ê°œë§Œ
            idx = classes.index(cls)
            ax2.annotate(f'Class {cls}',
                        (sample_counts[idx], f1_scores[idx]),
                        xytext=(10, 10), textcoords='offset points',
                        bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7),
                        arrowprops=dict(arrowstyle='->', color='red'))

        plt.tight_layout()
        plt.savefig(plots_dir / 'class_difficulty_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()

        if self.tracker:
            self.tracker.log_artifact(str(plots_dir / 'class_difficulty_analysis.png'), "class_difficulty_analysis")

    def _plot_performance_vs_samples(self, comparison_analysis: Dict, class_distribution: Dict, plots_dir):
        """ì„±ëŠ¥ê³¼ ìƒ˜í”Œ ìˆ˜ì˜ ê´€ê³„ë¥¼ ìƒì„¸ ë¶„ì„í•©ë‹ˆë‹¤."""

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

        classes = list(range(21))
        f1_scores = [comparison_analysis['best_model_per_class'][cls]['f1_score'] for cls in classes]
        sample_counts = [class_distribution['counts'].get(cls, 0) for cls in classes]

        # 1. ì‚°ì ë„ with íšŒê·€ì„ 
        ax1.scatter(sample_counts, f1_scores, c=f1_scores, cmap='RdYlGn', s=100, alpha=0.7)

        # íšŒê·€ì„  ì¶”ê°€
        z = np.polyfit(sample_counts, f1_scores, 1)
        p = np.poly1d(z)
        ax1.plot(sample_counts, p(sample_counts), "r--", alpha=0.8, linewidth=2)

        correlation = np.corrcoef(sample_counts, f1_scores)[0, 1]
        ax1.set_title(f'ì„±ëŠ¥ vs ìƒ˜í”Œ ìˆ˜ (ìƒê´€ê³„ìˆ˜: {correlation:.3f})', fontweight='bold')
        ax1.set_xlabel('ìƒ˜í”Œ ìˆ˜')
        ax1.set_ylabel('ìµœê³  F1-Score')
        ax1.grid(True, alpha=0.3)

        # 2. ìƒ˜í”Œ ìˆ˜ êµ¬ê°„ë³„ ì„±ëŠ¥ ë¶„í¬
        # ìƒ˜í”Œ ìˆ˜ë¥¼ 4ë¶„ìœ„ë¡œ ë‚˜ëˆ„ê¸°
        sample_quartiles = np.percentile(sample_counts, [25, 50, 75])

        q1_classes = [cls for cls in classes if sample_counts[cls] <= sample_quartiles[0]]
        q2_classes = [cls for cls in classes if sample_quartiles[0] < sample_counts[cls] <= sample_quartiles[1]]
        q3_classes = [cls for cls in classes if sample_quartiles[1] < sample_counts[cls] <= sample_quartiles[2]]
        q4_classes = [cls for cls in classes if sample_counts[cls] > sample_quartiles[2]]

        quartile_f1s = [
            [f1_scores[cls] for cls in q1_classes],
            [f1_scores[cls] for cls in q2_classes],
            [f1_scores[cls] for cls in q3_classes],
            [f1_scores[cls] for cls in q4_classes]
        ]

        box_plot = ax2.boxplot(quartile_f1s, labels=['Q1 (ì ìŒ)', 'Q2', 'Q3', 'Q4 (ë§ìŒ)'], patch_artist=True)
        colors = ['#ff9999', '#ffcc99', '#99ccff', '#99ff99']
        for patch, color in zip(box_plot['boxes'], colors):
            patch.set_facecolor(color)

        ax2.set_title('ìƒ˜í”Œ ìˆ˜ ë¶„ìœ„ë³„ ì„±ëŠ¥ ë¶„í¬', fontweight='bold')
        ax2.set_xlabel('ìƒ˜í”Œ ìˆ˜ ë¶„ìœ„')
        ax2.set_ylabel('F1-Score')
        ax2.grid(True, alpha=0.3)

        # 3. í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ìˆœìœ„ vs ìƒ˜í”Œ ìˆœìœ„
        sample_ranks = np.argsort(np.argsort(sample_counts)) + 1  # 1ë¶€í„° ì‹œì‘
        f1_ranks = np.argsort(np.argsort(f1_scores)) + 1

        ax3.scatter(sample_ranks, f1_ranks, c=f1_scores, cmap='RdYlGn', s=100, alpha=0.7)
        ax3.plot([1, 21], [1, 21], 'k--', alpha=0.5, label='ì™„ë²½í•œ ìƒê´€ê´€ê³„')

        rank_correlation = np.corrcoef(sample_ranks, f1_ranks)[0, 1]
        ax3.set_title(f'ìƒ˜í”Œ ìˆœìœ„ vs ì„±ëŠ¥ ìˆœìœ„\nìˆœìœ„ ìƒê´€ê³„ìˆ˜: {rank_correlation:.3f}', fontweight='bold')
        ax3.set_xlabel('ìƒ˜í”Œ ìˆ˜ ìˆœìœ„ (1=ê°€ì¥ ì ìŒ)')
        ax3.set_ylabel('ì„±ëŠ¥ ìˆœìœ„ (1=ê°€ì¥ ë‚®ìŒ)')
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # 4. ë¬¸ì œ í´ë˜ìŠ¤ ì‹ë³„ (ë‚®ì€ ì„±ëŠ¥ + ì ì€ ìƒ˜í”Œ)
        problem_threshold_f1 = np.percentile(f1_scores, 25)  # í•˜ìœ„ 25%
        problem_threshold_samples = np.percentile(sample_counts, 25)  # í•˜ìœ„ 25%

        problem_classes = []
        normal_classes = []

        for cls in classes:
            if (f1_scores[cls] <= problem_threshold_f1 and
                sample_counts[cls] <= problem_threshold_samples):
                problem_classes.append(cls)
            else:
                normal_classes.append(cls)

        # ë¬¸ì œ í´ë˜ìŠ¤ì™€ ì¼ë°˜ í´ë˜ìŠ¤ ë¶„ë¦¬í•´ì„œ í‘œì‹œ
        problem_samples = [sample_counts[cls] for cls in problem_classes]
        problem_f1s = [f1_scores[cls] for cls in problem_classes]
        normal_samples = [sample_counts[cls] for cls in normal_classes]
        normal_f1s = [f1_scores[cls] for cls in normal_classes]

        ax4.scatter(normal_samples, normal_f1s, c='blue', label='ì¼ë°˜ í´ë˜ìŠ¤', s=100, alpha=0.7)
        ax4.scatter(problem_samples, problem_f1s, c='red', label='ë¬¸ì œ í´ë˜ìŠ¤', s=100, alpha=0.7)

        # ë¬¸ì œ ì˜ì—­ í‘œì‹œ
        ax4.axvline(x=problem_threshold_samples, color='red', linestyle='--', alpha=0.5)
        ax4.axhline(y=problem_threshold_f1, color='red', linestyle='--', alpha=0.5)
        ax4.fill_betweenx([0, problem_threshold_f1], 0, problem_threshold_samples,
                         alpha=0.2, color='red', label='ë¬¸ì œ ì˜ì—­')

        ax4.set_title(f'ë¬¸ì œ í´ë˜ìŠ¤ ì‹ë³„\në¬¸ì œ í´ë˜ìŠ¤: {len(problem_classes)}ê°œ', fontweight='bold')
        ax4.set_xlabel('ìƒ˜í”Œ ìˆ˜')
        ax4.set_ylabel('F1-Score')
        ax4.legend()
        ax4.grid(True, alpha=0.3)

        # ë¬¸ì œ í´ë˜ìŠ¤ ë ˆì´ë¸”ë§
        for cls in problem_classes:
            ax4.annotate(f'{cls}',
                        (sample_counts[cls], f1_scores[cls]),
                        xytext=(5, 5), textcoords='offset points',
                        fontsize=8, color='red', fontweight='bold')

        plt.tight_layout()
        plt.savefig(plots_dir / 'performance_vs_samples_detailed.png', dpi=300, bbox_inches='tight')
        plt.close()

        if self.tracker:
            self.tracker.log_artifact(str(plots_dir / 'performance_vs_samples_detailed.png'), "performance_vs_samples")

        return problem_classes

    def _generate_recommendations(self, class_distribution: Dict, comparison_analysis: Dict):
        """í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        print("ğŸ’¡ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘...")

        recommendations = []

        # 1. í´ë˜ìŠ¤ ë¶ˆê· í˜• ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        imbalance_severity = class_distribution['imbalance_severity']
        if "ë†’ìŒ" in imbalance_severity or "ë§¤ìš°" in imbalance_severity:
            recommendations.extend([
                {
                    "category": "í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°",
                    "priority": "ë†’ìŒ",
                    "issue": f"í´ë˜ìŠ¤ ë¶ˆê· í˜• ì‹¬ê°ë„: {imbalance_severity}",
                    "solution": "SMOTE, ADASYN ë“± ì˜¤ë²„ìƒ˜í”Œë§ ê¸°ë²• ì ìš©",
                    "expected_impact": "ì†Œìˆ˜ í´ë˜ìŠ¤ ì¬í˜„ìœ¨ 20-30% í–¥ìƒ",
                    "implementation": "imblearn.over_sampling ëª¨ë“ˆ í™œìš©"
                },
                {
                    "category": "ëª¨ë¸ ê°€ì¤‘ì¹˜ ì¡°ì •",
                    "priority": "ë†’ìŒ",
                    "issue": "ë¶ˆê· í˜• ë°ì´í„°ë¡œ ì¸í•œ í¸í–¥ëœ í•™ìŠµ",
                    "solution": "class_weight='balanced' ë˜ëŠ” focal loss ì ìš©",
                    "expected_impact": "Macro F1-score 5-10% í–¥ìƒ",
                    "implementation": "sklearn.utils.class_weight.compute_class_weight í™œìš©"
                }
            ])

        # 2. ìµœì € ì„±ëŠ¥ í´ë˜ìŠ¤ ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        worst_classes = comparison_analysis['worst_performing_classes']
        worst_class_ids = [item['class'] for item in worst_classes[:3]]
        worst_avg_f1 = np.mean([item['f1_score'] for item in worst_classes[:3]])

        if worst_avg_f1 < 0.3:
            recommendations.append({
                "category": "ìµœì € ì„±ëŠ¥ í´ë˜ìŠ¤ ì§‘ì¤‘ ê°œì„ ",
                "priority": "ë§¤ìš° ë†’ìŒ",
                "issue": f"í´ë˜ìŠ¤ {worst_class_ids} í‰ê·  F1-score: {worst_avg_f1:.3f}",
                "solution": "1) íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì§‘ì¤‘, 2) í´ë˜ìŠ¤ë³„ ì „ë¬¸ ëª¨ë¸ ê°œë°œ, 3) ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì¡°ì •",
                "expected_impact": "í•´ë‹¹ í´ë˜ìŠ¤ë“¤ F1-score 50% ì´ìƒ í–¥ìƒ",
                "implementation": "í´ë˜ìŠ¤ë³„ íŠ¹í™”ëœ íŠ¹ì„± ìƒì„± ë° ëª¨ë¸ íŠœë‹"
            })

        # 3. ì„±ëŠ¥ ë¶„ì‚° ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        high_variance_classes = []
        for class_idx, variance in comparison_analysis['performance_variance'].items():
            if variance > 0.1:  # ë†’ì€ ë¶„ì‚° ê¸°ì¤€
                high_variance_classes.append((class_idx, variance))

        if len(high_variance_classes) > 5:
            recommendations.append({
                "category": "ëª¨ë¸ ì•ˆì •ì„± ê°œì„ ",
                "priority": "ë³´í†µ",
                "issue": f"{len(high_variance_classes)}ê°œ í´ë˜ìŠ¤ì—ì„œ ëª¨ë¸ ê°„ ì„±ëŠ¥ ì°¨ì´ í¼",
                "solution": "ì•™ìƒë¸” ë°©ë²• ê°œì„  ë° êµì°¨ê²€ì¦ ì „ëµ ì¬ê²€í† ",
                "expected_impact": "ì „ì²´ ëª¨ë¸ ì•ˆì •ì„± 15-20% í–¥ìƒ",
                "implementation": "Stacking, Voting ì•™ìƒë¸” ë° Stratified CV ì ìš©"
            })

        # 4. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ê¶Œì¥ì‚¬í•­
        recommendations.append({
            "category": "í´ë˜ìŠ¤ë³„ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§",
            "priority": "ë³´í†µ",
            "issue": "ë²”ìš© íŠ¹ì„±ìœ¼ë¡œ ì¸í•œ í´ë˜ìŠ¤ë³„ ì°¨ë³„í™” ë¶€ì¡±",
            "solution": "1) í´ë˜ìŠ¤ë³„ ì¤‘ìš” íŠ¹ì„± ì‹ë³„, 2) íŠ¹ì„± ìƒí˜¸ì‘ìš© ìƒì„±, 3) ë„ë©”ì¸ ì§€ì‹ í™œìš©",
            "expected_impact": "ì „ì²´ Macro F1-score 10-15% í–¥ìƒ",
            "implementation": "SHAP, Permutation Importanceë¥¼ í™œìš©í•œ í´ë˜ìŠ¤ë³„ íŠ¹ì„± ë¶„ì„"
        })

        # 5. í‰ê°€ ë©”íŠ¸ë¦­ ë‹¤ì–‘í™”
        recommendations.append({
            "category": "í‰ê°€ ë©”íŠ¸ë¦­ ë³´ì™„",
            "priority": "ë‚®ìŒ",
            "issue": "Macro F1-scoreë§Œìœ¼ë¡œëŠ” í´ë˜ìŠ¤ë³„ ë¬¸ì œ íŒŒì•… í•œê³„",
            "solution": "í´ë˜ìŠ¤ë³„ Precision, Recall, F1-score ê°œë³„ ëª¨ë‹ˆí„°ë§",
            "expected_impact": "ëª¨ë¸ ê°œì„  ë°©í–¥ì„± ëª…í™•í™”",
            "implementation": "classification_report ë° í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­ ì¶”ì "
        })

        self.recommendations = recommendations

    def _generate_summary(self, model_results: Dict, comparison_analysis: Dict) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""

        # ì „ì²´ ì„±ëŠ¥ í†µê³„
        overall_f1s = [results['overall_f1'] for results in model_results.values()]
        best_overall_f1 = max(overall_f1s)
        best_model = max(model_results.items(), key=lambda x: x[1]['overall_f1'])[0]

        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ í†µê³„
        all_class_f1s = []
        for results in model_results.values():
            for class_metrics in results['class_metrics'].values():
                all_class_f1s.append(class_metrics['f1_score'])

        # ìµœê³ /ìµœì € ì„±ëŠ¥ í´ë˜ìŠ¤
        worst_classes = comparison_analysis['worst_performing_classes'][:3]
        best_classes = comparison_analysis['best_performing_classes'][-3:]

        summary = {
            "ì „ì²´_ì„±ëŠ¥": {
                "ìµœê³ _Macro_F1": round(best_overall_f1, 4),
                "ìµœê³ _ì„±ëŠ¥_ëª¨ë¸": best_model,
                "ëª©í‘œ_ë‹¬ì„±ë¥ ": round((best_overall_f1 / 0.9) * 100, 1),
                "í˜„ì¬_ëŒ€ë¹„_ê°œì„ ë¥ ": round(((best_overall_f1 / 0.67596) - 1) * 100, 1)
            },
            "í´ë˜ìŠ¤ë³„_ì„±ëŠ¥_ë¶„í¬": {
                "í‰ê· _F1": round(np.mean(all_class_f1s), 4),
                "ì¤‘ì•™ê°’_F1": round(np.median(all_class_f1s), 4),
                "í‘œì¤€í¸ì°¨": round(np.std(all_class_f1s), 4),
                "ìµœê³ _í´ë˜ìŠ¤_F1": round(max(all_class_f1s), 4),
                "ìµœì €_í´ë˜ìŠ¤_F1": round(min(all_class_f1s), 4)
            },
            "ë¬¸ì œ_í´ë˜ìŠ¤": {
                "ìµœì €_ì„±ëŠ¥_3ê°œ": [
                    f"Class {item['class']} (F1: {item['f1_score']:.3f})"
                    for item in worst_classes
                ],
                "zero_f1_í´ë˜ìŠ¤_ìˆ˜": sum(1 for f1 in all_class_f1s if f1 == 0),
                "ëª©í‘œ_ë¯¸ë‹¬_í´ë˜ìŠ¤_ìˆ˜": sum(1 for f1 in all_class_f1s if f1 < 0.9)
            },
            "ìš°ìˆ˜_í´ë˜ìŠ¤": {
                "ìµœê³ _ì„±ëŠ¥_3ê°œ": [
                    f"Class {item['class']} (F1: {item['f1_score']:.3f})"
                    for item in best_classes
                ],
                "ëª©í‘œ_ë‹¬ì„±_í´ë˜ìŠ¤_ìˆ˜": sum(1 for f1 in all_class_f1s if f1 >= 0.9)
            },
            "ê°œì„ _ê¶Œì¥ì‚¬í•­_ìˆ˜": len(self.recommendations),
            "ì¦‰ì‹œ_ì¡°ì¹˜_í•„ìš”": len([r for r in self.recommendations if r['priority'] in ['ë§¤ìš° ë†’ìŒ', 'ë†’ìŒ']])
        }

        return summary

    def _log_results_to_tracker(self, analysis_result: Dict[str, Any]):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ì‹¤í—˜ ì¶”ì  ì‹œìŠ¤í…œì— ê¸°ë¡í•©ë‹ˆë‹¤."""
        if not self.tracker:
            return

        # ë©”íŠ¸ë¦­ ê¸°ë¡
        summary = analysis_result['summary']

        metrics = {
            "class_performance/best_macro_f1": summary['ì „ì²´_ì„±ëŠ¥']['ìµœê³ _Macro_F1'],
            "class_performance/target_achievement_rate": summary['ì „ì²´_ì„±ëŠ¥']['ëª©í‘œ_ë‹¬ì„±ë¥ '],
            "class_performance/improvement_rate": summary['ì „ì²´_ì„±ëŠ¥']['í˜„ì¬_ëŒ€ë¹„_ê°œì„ ë¥ '],
            "class_performance/avg_class_f1": summary['í´ë˜ìŠ¤ë³„_ì„±ëŠ¥_ë¶„í¬']['í‰ê· _F1'],
            "class_performance/class_f1_std": summary['í´ë˜ìŠ¤ë³„_ì„±ëŠ¥_ë¶„í¬']['í‘œì¤€í¸ì°¨'],
            "class_performance/zero_f1_classes": summary['ë¬¸ì œ_í´ë˜ìŠ¤']['zero_f1_í´ë˜ìŠ¤_ìˆ˜'],
            "class_performance/below_target_classes": summary['ë¬¸ì œ_í´ë˜ìŠ¤']['ëª©í‘œ_ë¯¸ë‹¬_í´ë˜ìŠ¤_ìˆ˜'],
            "class_performance/target_achieved_classes": summary['ìš°ìˆ˜_í´ë˜ìŠ¤']['ëª©í‘œ_ë‹¬ì„±_í´ë˜ìŠ¤_ìˆ˜'],
            "class_performance/high_priority_recommendations": summary['ì¦‰ì‹œ_ì¡°ì¹˜_í•„ìš”']
        }

        self.tracker.log_metrics(metrics)

        # íŒŒë¼ë¯¸í„° ê¸°ë¡
        params = {
            "analysis_type": "class_performance",
            "num_classes": 21,
            "best_model": summary['ì „ì²´_ì„±ëŠ¥']['ìµœê³ _ì„±ëŠ¥_ëª¨ë¸'],
            "imbalance_severity": analysis_result['class_distribution']['imbalance_severity']
        }

        self.tracker.log_params(params)

        print(f"ğŸ“ˆ ì‹¤í—˜ ê²°ê³¼ê°€ ì¶”ì  ì‹œìŠ¤í…œì— ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def get_recommendations(self) -> List[Dict[str, Any]]:
        """ìƒì„±ëœ ê¶Œì¥ì‚¬í•­ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.recommendations

    def print_summary(self, analysis_result: Dict[str, Any]):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""

        summary = analysis_result['summary']

        print("\n" + "="*60)
        print("ğŸ¯ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*60)

        # ì „ì²´ ì„±ëŠ¥
        print(f"\nğŸ“Š ì „ì²´ ì„±ëŠ¥:")
        print(f"  â€¢ ìµœê³  Macro F1-score: {summary['ì „ì²´_ì„±ëŠ¥']['ìµœê³ _Macro_F1']:.4f}")
        print(f"  â€¢ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {summary['ì „ì²´_ì„±ëŠ¥']['ìµœê³ _ì„±ëŠ¥_ëª¨ë¸']}")
        print(f"  â€¢ ëª©í‘œ ë‹¬ì„±ë¥ : {summary['ì „ì²´_ì„±ëŠ¥']['ëª©í‘œ_ë‹¬ì„±ë¥ ']:.1f}%")
        print(f"  â€¢ í˜„ì¬ ëŒ€ë¹„ ê°œì„ ë¥ : {summary['ì „ì²´_ì„±ëŠ¥']['í˜„ì¬_ëŒ€ë¹„_ê°œì„ ë¥ ']:.1f}%")

        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„í¬
        print(f"\nğŸ“ˆ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„í¬:")
        print(f"  â€¢ í‰ê·  F1-score: {summary['í´ë˜ìŠ¤ë³„_ì„±ëŠ¥_ë¶„í¬']['í‰ê· _F1']:.4f}")
        print(f"  â€¢ ì„±ëŠ¥ í¸ì°¨ (í‘œì¤€í¸ì°¨): {summary['í´ë˜ìŠ¤ë³„_ì„±ëŠ¥_ë¶„í¬']['í‘œì¤€í¸ì°¨']:.4f}")
        print(f"  â€¢ ìµœê³  í´ë˜ìŠ¤ F1: {summary['í´ë˜ìŠ¤ë³„_ì„±ëŠ¥_ë¶„í¬']['ìµœê³ _í´ë˜ìŠ¤_F1']:.4f}")
        print(f"  â€¢ ìµœì € í´ë˜ìŠ¤ F1: {summary['í´ë˜ìŠ¤ë³„_ì„±ëŠ¥_ë¶„í¬']['ìµœì €_í´ë˜ìŠ¤_F1']:.4f}")

        # ë¬¸ì œ í´ë˜ìŠ¤
        print(f"\nğŸš¨ ë¬¸ì œ í´ë˜ìŠ¤:")
        print(f"  â€¢ Zero F1 í´ë˜ìŠ¤: {summary['ë¬¸ì œ_í´ë˜ìŠ¤']['zero_f1_í´ë˜ìŠ¤_ìˆ˜']}ê°œ")
        print(f"  â€¢ ëª©í‘œ ë¯¸ë‹¬ í´ë˜ìŠ¤: {summary['ë¬¸ì œ_í´ë˜ìŠ¤']['ëª©í‘œ_ë¯¸ë‹¬_í´ë˜ìŠ¤_ìˆ˜']}/21ê°œ")
        print(f"  â€¢ ìµœì € ì„±ëŠ¥ í´ë˜ìŠ¤:")
        for class_info in summary['ë¬¸ì œ_í´ë˜ìŠ¤']['ìµœì €_ì„±ëŠ¥_3ê°œ']:
            print(f"    - {class_info}")

        # ìš°ìˆ˜ í´ë˜ìŠ¤
        print(f"\nğŸ† ìš°ìˆ˜ í´ë˜ìŠ¤:")
        print(f"  â€¢ ëª©í‘œ ë‹¬ì„± í´ë˜ìŠ¤: {summary['ìš°ìˆ˜_í´ë˜ìŠ¤']['ëª©í‘œ_ë‹¬ì„±_í´ë˜ìŠ¤_ìˆ˜']}/21ê°œ")
        print(f"  â€¢ ìµœê³  ì„±ëŠ¥ í´ë˜ìŠ¤:")
        for class_info in summary['ìš°ìˆ˜_í´ë˜ìŠ¤']['ìµœê³ _ì„±ëŠ¥_3ê°œ']:
            print(f"    - {class_info}")

        # ê°œì„  ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­:")
        print(f"  â€¢ ì´ ê¶Œì¥ì‚¬í•­: {summary['ê°œì„ _ê¶Œì¥ì‚¬í•­_ìˆ˜']}ê°œ")
        print(f"  â€¢ ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”: {summary['ì¦‰ì‹œ_ì¡°ì¹˜_í•„ìš”']}ê°œ")

        high_priority_recs = [r for r in self.recommendations if r['priority'] in ['ë§¤ìš° ë†’ìŒ', 'ë†’ìŒ']]
        for i, rec in enumerate(high_priority_recs[:3], 1):
            print(f"    {i}. [{rec['priority']}] {rec['category']}: {rec['solution']}")

        print("\n" + "="*60)
        print("âœ… í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ“ ìƒì„¸ ì‹œê°í™”ëŠ” experiments/plots/class_performance/ ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
        print("="*60)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    from ..utils.config import Config
    from ..tracking.experiment_tracker import ExperimentTracker
    import pandas as pd

    # ì„¤ì • ë¡œë“œ
    config = Config()

    # ì‹¤í—˜ ì¶”ì ê¸° ì´ˆê¸°í™”
    tracker = ExperimentTracker(
        project_name="dacon-smartmh-02",
        experiment_name="class_performance_analysis"
    )

    # í´ë˜ìŠ¤ ì„±ëŠ¥ ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = ClassPerformanceAnalyzer(config=config, experiment_tracker=tracker)

    try:
        # ë°ì´í„° ë¡œë“œ (ì‹¤ì œ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
        data_path = config.get_paths()['data']
        train_data = pd.read_csv(data_path / 'train.csv')

        # íŠ¹ì„±ê³¼ ë ˆì´ë¸” ë¶„ë¦¬
        X_train = train_data.drop(['target'], axis=1, errors='ignore')
        y_train = train_data['target']

        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰
        results = analyzer.analyze_class_performance(X_train, y_train)

        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        analyzer.print_summary(results)

        return results

    except Exception as e:
        print(f"âŒ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None


if __name__ == "__main__":
    main()
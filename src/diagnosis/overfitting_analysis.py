"""
ê³¼ì í•© ì§„ë‹¨ ì‹œìŠ¤í…œ êµ¬í˜„
T006 íƒœìŠ¤í¬: ê³¼ì í•©/ê³¼ì†Œì í•© ì •ë°€ ì§„ë‹¨ ë° í•´ê²°ë°©ì•ˆ ì œì‹œ

ëª©ì :
- í˜„ì¬ 77% CV vs 67.6% ì‹¤ì œ ì ìˆ˜ ê²©ì°¨ì˜ ê³¼ì í•© ì›ì¸ ì •ë°€ ë¶„ì„
- ë‹¤ì–‘í•œ ê°ë„ì—ì„œ ê³¼ì í•© ì •ë„ ì •ëŸ‰í™”
- ëª¨ë¸ë³„, íŠ¹ì„±ë³„, ë°ì´í„° í¬ê¸°ë³„ ê³¼ì í•© íŒ¨í„´ ë¶„ì„
- êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê³¼ì í•© í•´ê²° ë°©ì•ˆ ì œì‹œ

ë¶„ì„ ë°©ë²•ë¡ :
1. Learning Curves (ë°ì´í„° í¬ê¸°ë³„ ì„±ëŠ¥)
2. Validation Curves (í•˜ì´í¼íŒŒë¼ë¯¸í„°ë³„ ì„±ëŠ¥)
3. Feature Learning Curves (íŠ¹ì„± ìˆ˜ë³„ ì„±ëŠ¥)
4. Regularization Analysis (ì •ê·œí™” íš¨ê³¼)
5. Early Stopping Analysis (ìµœì  ì¤‘ë‹¨ì )
6. Ensemble Diversity Analysis (ì•™ìƒë¸” ë‹¤ì–‘ì„±)
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import (
    learning_curve, validation_curve, cross_val_score,
    StratifiedKFold
)
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.svm import SVC
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, roc_auc_score
import warnings
warnings.filterwarnings('ignore')

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.tracking.experiment_tracker import ExperimentTracker
from src.utils.config import get_config, get_paths


class OverfittingAnalyzer:
    """
    ê³¼ì í•© ì§„ë‹¨ ë° í•´ê²° ì‹œìŠ¤í…œ
    ë‹¤ê°ë„ ê³¼ì í•© ë¶„ì„ìœ¼ë¡œ CV-ì‹¤ì œ ì ìˆ˜ ê²©ì°¨ ì›ì¸ ê·œëª… ë° í•´ê²°ë°©ì•ˆ ë„ì¶œ
    """

    def __init__(self, use_tracking: bool = True):
        """
        ê³¼ì í•© ë¶„ì„ê¸° ì´ˆê¸°í™”

        Args:
            use_tracking: ì‹¤í—˜ ì¶”ì  ì‚¬ìš© ì—¬ë¶€
        """
        self.use_tracking = use_tracking
        self.results = {}
        self.figures = {}

        # ê²½ë¡œ ì„¤ì •
        self.paths = get_paths()
        self.plots_dir = self.paths['experiments_dir'] / 'plots' / 'overfitting'
        self.plots_dir.mkdir(parents=True, exist_ok=True)

        # ì‹¤í—˜ ì¶”ì  ì„¤ì •
        if self.use_tracking:
            self.tracker = ExperimentTracker(
                project_name=get_config('tracking.project_name'),
                experiment_name="overfitting_analysis"
            )

        print("ğŸ” ê³¼ì í•© ì§„ë‹¨ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    def comprehensive_overfitting_analysis(self, X, y, models=None, target_score_gap=0.096):
        """
        ì¢…í•©ì ì¸ ê³¼ì í•© ë¶„ì„ ìˆ˜í–‰

        Args:
            X: íŠ¹ì„± ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            models: ë¶„ì„í•  ëª¨ë¸ë“¤
            target_score_gap: ëª©í‘œë¡œ í•˜ëŠ” CV-ì‹¤ì œ ì ìˆ˜ ê²©ì°¨ (9.6%)

        Returns:
            Dict: ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("\nğŸ” ì¢…í•©ì ì¸ ê³¼ì í•© ë¶„ì„ ì‹œì‘...")
        print("=" * 60)

        # ì‹¤í—˜ ì¶”ì  ì‹œì‘
        if self.use_tracking:
            self.tracker.start_run(
                run_name="comprehensive_overfitting_analysis",
                description="ê³¼ì í•©/ê³¼ì†Œì í•© ì •ë°€ ì§„ë‹¨ ë° í•´ê²°ë°©ì•ˆ ë„ì¶œ",
                tags={
                    "analysis_type": "overfitting_diagnosis",
                    "task": "T006",
                    "purpose": "cv_score_gap_resolution"
                }
            )

        # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
        if models is None:
            models = self._define_analysis_models()

        # 1. í•™ìŠµ ê³¡ì„  ë¶„ì„ (ë°ì´í„° í¬ê¸°ë³„)
        learning_analysis = self._analyze_learning_curves(X, y, models)

        # 2. ê²€ì¦ ê³¡ì„  ë¶„ì„ (í•˜ì´í¼íŒŒë¼ë¯¸í„°ë³„)
        validation_analysis = self._analyze_validation_curves(X, y, models)

        # 3. íŠ¹ì„± í•™ìŠµ ê³¡ì„  ë¶„ì„ (íŠ¹ì„± ìˆ˜ë³„)
        feature_analysis = self._analyze_feature_learning_curves(X, y, models)

        # 4. ì •ê·œí™” íš¨ê³¼ ë¶„ì„
        regularization_analysis = self._analyze_regularization_effects(X, y)

        # 5. Early Stopping ë¶„ì„
        early_stopping_analysis = self._analyze_early_stopping(X, y)

        # 6. ì•™ìƒë¸” ë‹¤ì–‘ì„± ë¶„ì„
        ensemble_analysis = self._analyze_ensemble_diversity(X, y)

        # 7. ê³¼ì í•© ì‹¬ê°ë„ í‰ê°€
        severity_assessment = self._assess_overfitting_severity(
            learning_analysis, validation_analysis, target_score_gap
        )

        # 8. í•´ê²°ë°©ì•ˆ ìƒì„±
        solutions = self._generate_overfitting_solutions(
            learning_analysis, validation_analysis, feature_analysis,
            regularization_analysis, severity_assessment
        )

        # ê²°ê³¼ ì¢…í•©
        comprehensive_results = {
            'learning_curves': learning_analysis,
            'validation_curves': validation_analysis,
            'feature_curves': feature_analysis,
            'regularization': regularization_analysis,
            'early_stopping': early_stopping_analysis,
            'ensemble_diversity': ensemble_analysis,
            'severity_assessment': severity_assessment,
            'solutions': solutions,
            'data_info': {
                'n_samples': len(X),
                'n_features': len(X.columns),
                'n_classes': len(np.unique(y)),
                'target_score_gap': target_score_gap
            }
        }

        self.results = comprehensive_results

        # ì‹¤í—˜ ì¶”ì  ë¡œê¹…
        if self.use_tracking:
            self._log_overfitting_results(comprehensive_results)

        print("\nâœ… ì¢…í•©ì ì¸ ê³¼ì í•© ë¶„ì„ ì™„ë£Œ!")
        self._print_analysis_summary(comprehensive_results)

        return comprehensive_results

    def _define_analysis_models(self):
        """ë¶„ì„ìš© ëª¨ë¸ë“¤ ì •ì˜"""
        random_state = get_config('models.random_seed')

        models = {
            'RandomForest_Default': RandomForestClassifier(
                n_estimators=100,
                random_state=random_state,
                n_jobs=-1
            ),
            'RandomForest_Regularized': RandomForestClassifier(
                n_estimators=50,
                max_depth=5,
                min_samples_split=20,
                min_samples_leaf=10,
                random_state=random_state,
                n_jobs=-1
            ),
            'LogisticRegression_L2': LogisticRegression(
                C=1.0,
                random_state=random_state,
                max_iter=1000
            ),
            'LogisticRegression_L1': LogisticRegression(
                penalty='l1',
                C=1.0,
                solver='liblinear',
                random_state=random_state,
                max_iter=1000
            )
        }

        return models

    def _analyze_learning_curves(self, X, y, models):
        """í•™ìŠµ ê³¡ì„  ë¶„ì„"""
        print("\nğŸ“š 1ë‹¨ê³„: í•™ìŠµ ê³¡ì„  ë¶„ì„")

        results = {}
        train_sizes = np.linspace(0.1, 1.0, 10)

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        axes = axes.ravel()

        for idx, (model_name, model) in enumerate(models.items()):
            if idx >= 4:
                break

            print(f"  ğŸ“Š {model_name} í•™ìŠµ ê³¡ì„  ë¶„ì„...")

            try:
                train_sizes_abs, train_scores, val_scores = learning_curve(
                    model, X, y,
                    train_sizes=train_sizes,
                    cv=5,
                    scoring='f1_macro',
                    n_jobs=-1,
                    random_state=get_config('models.random_seed')
                )

                # í†µê³„ ê³„ì‚°
                train_mean = train_scores.mean(axis=1)
                train_std = train_scores.std(axis=1)
                val_mean = val_scores.mean(axis=1)
                val_std = val_scores.std(axis=1)

                # ê³¼ì í•© ì§€í‘œ ê³„ì‚°
                final_gap = train_mean[-1] - val_mean[-1]
                convergence_point = self._find_convergence_point(train_mean, val_mean)
                optimal_size = self._find_optimal_training_size(train_sizes_abs, val_mean)

                results[model_name] = {
                    'train_sizes': train_sizes_abs.tolist(),
                    'train_scores_mean': train_mean.tolist(),
                    'train_scores_std': train_std.tolist(),
                    'val_scores_mean': val_mean.tolist(),
                    'val_scores_std': val_std.tolist(),
                    'final_gap': final_gap,
                    'convergence_point': convergence_point,
                    'optimal_training_size': optimal_size,
                    'overfitting_severity': self._classify_overfitting_severity(final_gap)
                }

                # ì‹œê°í™”
                ax = axes[idx]
                ax.plot(train_sizes_abs, train_mean, 'o-', color='blue',
                       label=f'Training Score')
                ax.fill_between(train_sizes_abs, train_mean - train_std,
                              train_mean + train_std, alpha=0.1, color='blue')

                ax.plot(train_sizes_abs, val_mean, 'o-', color='red',
                       label=f'Validation Score')
                ax.fill_between(train_sizes_abs, val_mean - val_std,
                              val_mean + val_std, alpha=0.1, color='red')

                ax.set_title(f'{model_name}\nGap: {final_gap:.3f} ({results[model_name]["overfitting_severity"]})')
                ax.set_xlabel('Training Set Size')
                ax.set_ylabel('F1-Score (Macro)')
                ax.legend()
                ax.grid(True, alpha=0.3)

                print(f"    â€¢ ìµœì¢… ê²©ì°¨: {final_gap:.4f} ({results[model_name]['overfitting_severity']})")
                print(f"    â€¢ ìµœì  ë°ì´í„° í¬ê¸°: {optimal_size}")

            except Exception as e:
                print(f"    âŒ {model_name} ì˜¤ë¥˜: {str(e)}")
                results[model_name] = None

        plt.tight_layout()

        # ê·¸ë˜í”„ ì €ì¥
        learning_path = self.plots_dir / 'learning_curves_analysis.png'
        plt.savefig(learning_path, dpi=300, bbox_inches='tight')
        self.figures['learning_curves'] = plt.gcf()
        plt.close()

        return results

    def _analyze_validation_curves(self, X, y, models):
        """ê²€ì¦ ê³¡ì„  ë¶„ì„ (í•˜ì´í¼íŒŒë¼ë¯¸í„°ë³„)"""
        print("\nğŸ›ï¸ 2ë‹¨ê³„: ê²€ì¦ ê³¡ì„  ë¶„ì„")

        results = {}

        # RandomForest n_estimators ë¶„ì„
        if 'RandomForest_Default' in models:
            rf_model = models['RandomForest_Default']
            param_range = [10, 50, 100, 200, 500, 1000]

            print("  ğŸŒ² RandomForest n_estimators ë¶„ì„...")

            train_scores, val_scores = validation_curve(
                rf_model, X, y,
                param_name='n_estimators',
                param_range=param_range,
                cv=5, scoring='f1_macro', n_jobs=-1
            )

            results['RandomForest_n_estimators'] = self._process_validation_curve(
                param_range, train_scores, val_scores, 'n_estimators'
            )

        # LogisticRegression C ë¶„ì„
        if 'LogisticRegression_L2' in models:
            lr_model = models['LogisticRegression_L2']
            param_range = [0.001, 0.01, 0.1, 1, 10, 100]

            print("  ğŸ“Š LogisticRegression C ë¶„ì„...")

            train_scores, val_scores = validation_curve(
                lr_model, X, y,
                param_name='C',
                param_range=param_range,
                cv=5, scoring='f1_macro', n_jobs=-1
            )

            results['LogisticRegression_C'] = self._process_validation_curve(
                param_range, train_scores, val_scores, 'C'
            )

        # ê²€ì¦ ê³¡ì„  ì‹œê°í™”
        self._plot_validation_curves(results)

        return results

    def _process_validation_curve(self, param_range, train_scores, val_scores, param_name):
        """ê²€ì¦ ê³¡ì„  ê²°ê³¼ ì²˜ë¦¬"""
        train_mean = train_scores.mean(axis=1)
        train_std = train_scores.std(axis=1)
        val_mean = val_scores.mean(axis=1)
        val_std = val_scores.std(axis=1)

        # ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°
        best_idx = np.argmax(val_mean)
        best_param = param_range[best_idx]
        best_score = val_mean[best_idx]

        # ê³¼ì í•© êµ¬ê°„ ì°¾ê¸°
        gaps = train_mean - val_mean
        overfitting_threshold = 0.05
        overfitting_params = [param_range[i] for i, gap in enumerate(gaps) if gap > overfitting_threshold]

        return {
            'param_name': param_name,
            'param_range': param_range,
            'train_scores_mean': train_mean.tolist(),
            'train_scores_std': train_std.tolist(),
            'val_scores_mean': val_mean.tolist(),
            'val_scores_std': val_std.tolist(),
            'best_param': best_param,
            'best_score': best_score,
            'overfitting_params': overfitting_params,
            'gaps': gaps.tolist()
        }

    def _plot_validation_curves(self, results):
        """ê²€ì¦ ê³¡ì„  ì‹œê°í™”"""
        n_plots = len(results)
        if n_plots == 0:
            return

        fig, axes = plt.subplots(1, n_plots, figsize=(6*n_plots, 6))
        if n_plots == 1:
            axes = [axes]

        for idx, (curve_name, curve_data) in enumerate(results.items()):
            ax = axes[idx]

            param_range = curve_data['param_range']
            train_mean = np.array(curve_data['train_scores_mean'])
            train_std = np.array(curve_data['train_scores_std'])
            val_mean = np.array(curve_data['val_scores_mean'])
            val_std = np.array(curve_data['val_scores_std'])

            ax.semilogx(param_range, train_mean, 'o-', color='blue', label='Training Score')
            ax.fill_between(param_range, train_mean - train_std,
                          train_mean + train_std, alpha=0.1, color='blue')

            ax.semilogx(param_range, val_mean, 'o-', color='red', label='Validation Score')
            ax.fill_between(param_range, val_mean - val_std,
                          val_mean + val_std, alpha=0.1, color='red')

            # ìµœì  íŒŒë¼ë¯¸í„° í‘œì‹œ
            best_param = curve_data['best_param']
            ax.axvline(x=best_param, color='green', linestyle='--', alpha=0.7,
                      label=f'Best: {best_param}')

            ax.set_title(f'Validation Curve: {curve_name}')
            ax.set_xlabel(curve_data['param_name'])
            ax.set_ylabel('F1-Score (Macro)')
            ax.legend()
            ax.grid(True, alpha=0.3)

        plt.tight_layout()

        # ì €ì¥
        validation_path = self.plots_dir / 'validation_curves_analysis.png'
        plt.savefig(validation_path, dpi=300, bbox_inches='tight')
        self.figures['validation_curves'] = plt.gcf()
        plt.close()

    def _analyze_feature_learning_curves(self, X, y, models):
        """íŠ¹ì„± í•™ìŠµ ê³¡ì„  ë¶„ì„"""
        print("\nğŸ” 3ë‹¨ê³„: íŠ¹ì„± í•™ìŠµ ê³¡ì„  ë¶„ì„")

        results = {}

        # íŠ¹ì„± ìˆ˜ë³„ ì„±ëŠ¥ ë¶„ì„
        feature_counts = [5, 10, 15, 20, min(30, len(X.columns)), len(X.columns)]
        feature_counts = [f for f in feature_counts if f <= len(X.columns)]

        # ì£¼ìš” ëª¨ë¸ í•˜ë‚˜ë§Œ ì‚¬ìš© (ì‹œê°„ ì ˆì•½)
        main_model = RandomForestClassifier(
            n_estimators=100,
            random_state=get_config('models.random_seed'),
            n_jobs=-1
        )

        print("  ğŸ“Š íŠ¹ì„± ìˆ˜ë³„ ì„±ëŠ¥ ë¶„ì„...")

        feature_scores = []
        feature_train_scores = []

        for n_features in feature_counts:
            # íŠ¹ì„± ì„ íƒ
            selector = SelectKBest(score_func=f_classif, k=n_features)
            X_selected = selector.fit_transform(X, y)

            # CV ì ìˆ˜ ê³„ì‚°
            cv_scores = cross_val_score(main_model, X_selected, y, cv=5, scoring='f1_macro')

            # í›ˆë ¨ ì ìˆ˜ ê³„ì‚° (ê³¼ì í•© ì •ë„ í™•ì¸ìš©)
            main_model.fit(X_selected, y)
            train_pred = main_model.predict(X_selected)
            train_score = f1_score(y, train_pred, average='macro')

            feature_scores.append(cv_scores.mean())
            feature_train_scores.append(train_score)

            print(f"    â€¢ {n_features}ê°œ íŠ¹ì„±: CV={cv_scores.mean():.4f}, Train={train_score:.4f}")

        # ê²°ê³¼ ì €ì¥
        results = {
            'feature_counts': feature_counts,
            'val_scores': feature_scores,
            'train_scores': feature_train_scores,
            'gaps': [train - val for train, val in zip(feature_train_scores, feature_scores)],
            'optimal_features': feature_counts[np.argmax(feature_scores)]
        }

        # ì‹œê°í™”
        plt.figure(figsize=(10, 6))
        plt.plot(feature_counts, feature_train_scores, 'o-', color='blue', label='Training Score')
        plt.plot(feature_counts, feature_scores, 'o-', color='red', label='Validation Score')
        plt.axvline(x=results['optimal_features'], color='green', linestyle='--',
                   label=f'Optimal: {results["optimal_features"]} features')

        plt.xlabel('Number of Features')
        plt.ylabel('F1-Score (Macro)')
        plt.title('Feature Learning Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()

        # ì €ì¥
        feature_path = self.plots_dir / 'feature_learning_curve.png'
        plt.savefig(feature_path, dpi=300, bbox_inches='tight')
        self.figures['feature_learning'] = plt.gcf()
        plt.close()

        print(f"  ğŸ¯ ìµœì  íŠ¹ì„± ìˆ˜: {results['optimal_features']}ê°œ")

        return results

    def _analyze_regularization_effects(self, X, y):
        """ì •ê·œí™” íš¨ê³¼ ë¶„ì„"""
        print("\nğŸ›¡ï¸ 4ë‹¨ê³„: ì •ê·œí™” íš¨ê³¼ ë¶„ì„")

        results = {}

        # L1, L2 ì •ê·œí™” ê°•ë„ë³„ ë¶„ì„
        C_values = [0.001, 0.01, 0.1, 1, 10, 100]

        for penalty in ['l1', 'l2']:
            print(f"  ğŸ“Š {penalty.upper()} ì •ê·œí™” ë¶„ì„...")

            penalty_results = {
                'C_values': C_values,
                'val_scores': [],
                'train_scores': [],
                'gaps': []
            }

            for C in C_values:
                try:
                    if penalty == 'l1':
                        model = LogisticRegression(
                            penalty='l1', C=C, solver='liblinear',
                            random_state=get_config('models.random_seed'),
                            max_iter=1000
                        )
                    else:
                        model = LogisticRegression(
                            penalty='l2', C=C,
                            random_state=get_config('models.random_seed'),
                            max_iter=1000
                        )

                    # êµì°¨ê²€ì¦ ì ìˆ˜
                    cv_scores = cross_val_score(model, X, y, cv=5, scoring='f1_macro')
                    val_score = cv_scores.mean()

                    # í›ˆë ¨ ì ìˆ˜
                    model.fit(X, y)
                    train_pred = model.predict(X)
                    train_score = f1_score(y, train_pred, average='macro')

                    penalty_results['val_scores'].append(val_score)
                    penalty_results['train_scores'].append(train_score)
                    penalty_results['gaps'].append(train_score - val_score)

                except:
                    penalty_results['val_scores'].append(0)
                    penalty_results['train_scores'].append(0)
                    penalty_results['gaps'].append(0)

            # ìµœì  C ì°¾ê¸°
            best_idx = np.argmax(penalty_results['val_scores'])
            penalty_results['best_C'] = C_values[best_idx]
            penalty_results['best_score'] = penalty_results['val_scores'][best_idx]
            penalty_results['best_gap'] = penalty_results['gaps'][best_idx]

            results[penalty] = penalty_results

            print(f"    â€¢ ìµœì  C: {penalty_results['best_C']}")
            print(f"    â€¢ ìµœê³  ì ìˆ˜: {penalty_results['best_score']:.4f}")
            print(f"    â€¢ ê²©ì°¨: {penalty_results['best_gap']:.4f}")

        # ì‹œê°í™”
        self._plot_regularization_effects(results)

        return results

    def _plot_regularization_effects(self, results):
        """ì •ê·œí™” íš¨ê³¼ ì‹œê°í™”"""
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))

        for idx, (penalty, penalty_data) in enumerate(results.items()):
            ax = axes[idx]

            C_values = penalty_data['C_values']
            train_scores = penalty_data['train_scores']
            val_scores = penalty_data['val_scores']

            ax.semilogx(C_values, train_scores, 'o-', color='blue', label='Training Score')
            ax.semilogx(C_values, val_scores, 'o-', color='red', label='Validation Score')

            # ìµœì  C í‘œì‹œ
            best_C = penalty_data['best_C']
            ax.axvline(x=best_C, color='green', linestyle='--',
                      label=f'Best C: {best_C}')

            ax.set_title(f'{penalty.upper()} Regularization Effect')
            ax.set_xlabel('C (Inverse Regularization Strength)')
            ax.set_ylabel('F1-Score (Macro)')
            ax.legend()
            ax.grid(True, alpha=0.3)

        plt.tight_layout()

        # ì €ì¥
        reg_path = self.plots_dir / 'regularization_effects.png'
        plt.savefig(reg_path, dpi=300, bbox_inches='tight')
        self.figures['regularization'] = plt.gcf()
        plt.close()

    def _analyze_early_stopping(self, X, y):
        """Early Stopping ë¶„ì„"""
        print("\nâ° 5ë‹¨ê³„: Early Stopping ë¶„ì„")

        # RandomForest n_estimatorsë³„ ì„±ëŠ¥ ë¶„ì„ (ì¡°ê¸° ì¢…ë£Œ ì‹œë®¬ë ˆì´ì…˜)
        n_estimators_range = range(10, 201, 10)
        val_scores = []
        train_scores = []

        print("  ğŸ“Š RandomForest íŠ¸ë¦¬ ìˆ˜ë³„ ì„±ëŠ¥ ë¶„ì„...")

        for n_est in n_estimators_range:
            model = RandomForestClassifier(
                n_estimators=n_est,
                random_state=get_config('models.random_seed'),
                n_jobs=-1
            )

            # êµì°¨ê²€ì¦ ì ìˆ˜
            cv_scores = cross_val_score(model, X, y, cv=5, scoring='f1_macro')
            val_score = cv_scores.mean()

            # í›ˆë ¨ ì ìˆ˜
            model.fit(X, y)
            train_pred = model.predict(X)
            train_score = f1_score(y, train_pred, average='macro')

            val_scores.append(val_score)
            train_scores.append(train_score)

        # ìµœì  ì¤‘ë‹¨ì  ì°¾ê¸°
        gaps = np.array(train_scores) - np.array(val_scores)
        val_scores_array = np.array(val_scores)

        # ê²€ì¦ ì ìˆ˜ê°€ ê°œì„ ë˜ì§€ ì•ŠëŠ” ì§€ì  ì°¾ê¸°
        patience = 3
        best_score = 0
        best_iter = 0
        no_improve_count = 0
        early_stop_point = len(n_estimators_range)

        for i, score in enumerate(val_scores):
            if score > best_score:
                best_score = score
                best_iter = i
                no_improve_count = 0
            else:
                no_improve_count += 1
                if no_improve_count >= patience and early_stop_point == len(n_estimators_range):
                    early_stop_point = i

        optimal_n_estimators = list(n_estimators_range)[best_iter]
        early_stop_n_estimators = list(n_estimators_range)[min(early_stop_point, len(n_estimators_range)-1)]

        results = {
            'n_estimators_range': list(n_estimators_range),
            'val_scores': val_scores,
            'train_scores': train_scores,
            'gaps': gaps.tolist(),
            'optimal_n_estimators': optimal_n_estimators,
            'optimal_score': best_score,
            'early_stop_point': early_stop_n_estimators,
            'early_stop_score': val_scores[early_stop_point] if early_stop_point < len(val_scores) else val_scores[-1]
        }

        # ì‹œê°í™”
        plt.figure(figsize=(12, 6))
        plt.plot(n_estimators_range, train_scores, 'o-', color='blue', label='Training Score')
        plt.plot(n_estimators_range, val_scores, 'o-', color='red', label='Validation Score')
        plt.axvline(x=optimal_n_estimators, color='green', linestyle='--',
                   label=f'Optimal: {optimal_n_estimators}')
        plt.axvline(x=early_stop_n_estimators, color='orange', linestyle='--',
                   label=f'Early Stop: {early_stop_n_estimators}')

        plt.xlabel('Number of Estimators')
        plt.ylabel('F1-Score (Macro)')
        plt.title('Early Stopping Analysis')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()

        # ì €ì¥
        early_stop_path = self.plots_dir / 'early_stopping_analysis.png'
        plt.savefig(early_stop_path, dpi=300, bbox_inches='tight')
        self.figures['early_stopping'] = plt.gcf()
        plt.close()

        print(f"  ğŸ¯ ìµœì  n_estimators: {optimal_n_estimators}")
        print(f"  â° Early Stop ì§€ì : {early_stop_n_estimators}")

        return results

    def _analyze_ensemble_diversity(self, X, y):
        """ì•™ìƒë¸” ë‹¤ì–‘ì„± ë¶„ì„"""
        print("\nğŸ­ 6ë‹¨ê³„: ì•™ìƒë¸” ë‹¤ì–‘ì„± ë¶„ì„")

        # ë‹¤ì–‘í•œ ëœë¤ ì‹œë“œë¡œ ëª¨ë¸ í•™ìŠµ
        n_models = 10
        models = []
        predictions = []

        print("  ğŸ² ë‹¤ì–‘í•œ ì‹œë“œë¡œ ëª¨ë¸ í•™ìŠµ...")

        for seed in range(n_models):
            model = RandomForestClassifier(
                n_estimators=100,
                random_state=seed,
                n_jobs=-1
            )
            model.fit(X, y)
            pred = model.predict(X)
            models.append(model)
            predictions.append(pred)

        # ë‹¤ì–‘ì„± ì¸¡ì •
        diversity_metrics = self._calculate_ensemble_diversity(predictions, y)

        # ì•™ìƒë¸” ì„±ëŠ¥ (ë‹¤ìˆ˜ê²° íˆ¬í‘œ)
        ensemble_pred = np.array(predictions).T
        final_pred = []

        for pred_row in ensemble_pred:
            # ë‹¤ìˆ˜ê²° íˆ¬í‘œ
            unique, counts = np.unique(pred_row, return_counts=True)
            majority_vote = unique[np.argmax(counts)]
            final_pred.append(majority_vote)

        ensemble_score = f1_score(y, final_pred, average='macro')

        # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥
        individual_scores = [f1_score(y, pred, average='macro') for pred in predictions]

        results = {
            'n_models': n_models,
            'individual_scores': individual_scores,
            'individual_mean': np.mean(individual_scores),
            'individual_std': np.std(individual_scores),
            'ensemble_score': ensemble_score,
            'ensemble_improvement': ensemble_score - np.mean(individual_scores),
            'diversity_metrics': diversity_metrics
        }

        print(f"  ğŸ“Š ê°œë³„ ëª¨ë¸ í‰ê· : {results['individual_mean']:.4f} Â± {results['individual_std']:.4f}")
        print(f"  ğŸ­ ì•™ìƒë¸” ì„±ëŠ¥: {results['ensemble_score']:.4f}")
        print(f"  ğŸ“ˆ ê°œì„  íš¨ê³¼: +{results['ensemble_improvement']:.4f}")

        return results

    def _calculate_ensemble_diversity(self, predictions, y_true):
        """ì•™ìƒë¸” ë‹¤ì–‘ì„± ë©”íŠ¸ë¦­ ê³„ì‚°"""
        predictions = np.array(predictions)
        n_models = len(predictions)

        # Disagreement measure (ëª¨ë¸ ê°„ ë¶ˆì¼ì¹˜ ì •ë„)
        disagreements = []
        for i in range(n_models):
            for j in range(i+1, n_models):
                disagreement = np.mean(predictions[i] != predictions[j])
                disagreements.append(disagreement)

        avg_disagreement = np.mean(disagreements)

        # Q-statistic (ë‘ ëª¨ë¸ ê°„ ìƒê´€ê´€ê³„)
        q_statistics = []
        for i in range(n_models):
            for j in range(i+1, n_models):
                # ì¼ì¹˜/ë¶ˆì¼ì¹˜ í‘œ ìƒì„±
                both_correct = np.sum((predictions[i] == y_true) & (predictions[j] == y_true))
                both_wrong = np.sum((predictions[i] != y_true) & (predictions[j] != y_true))
                i_correct_j_wrong = np.sum((predictions[i] == y_true) & (predictions[j] != y_true))
                i_wrong_j_correct = np.sum((predictions[i] != y_true) & (predictions[j] == y_true))

                # Q-statistic ê³„ì‚°
                if (both_correct * both_wrong + i_correct_j_wrong * i_wrong_j_correct) != 0:
                    q_stat = (both_correct * both_wrong - i_correct_j_wrong * i_wrong_j_correct) / \
                            (both_correct * both_wrong + i_correct_j_wrong * i_wrong_j_correct)
                    q_statistics.append(q_stat)

        avg_q_statistic = np.mean(q_statistics) if q_statistics else 0

        return {
            'average_disagreement': avg_disagreement,
            'average_q_statistic': avg_q_statistic,
            'diversity_score': avg_disagreement  # ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ë†’ìŒ
        }

    def _find_convergence_point(self, train_scores, val_scores):
        """í•™ìŠµ/ê²€ì¦ ì ìˆ˜ ìˆ˜ë ´ì  ì°¾ê¸°"""
        gaps = train_scores - val_scores
        # ê²©ì°¨ê°€ ì•ˆì •í™”ë˜ëŠ” ì§€ì  ì°¾ê¸°
        convergence_threshold = 0.01

        for i in range(1, len(gaps)):
            if abs(gaps[i] - gaps[i-1]) < convergence_threshold:
                return i

        return len(gaps) - 1

    def _find_optimal_training_size(self, train_sizes, val_scores):
        """ìµœì  í›ˆë ¨ ë°ì´í„° í¬ê¸° ì°¾ê¸°"""
        best_idx = np.argmax(val_scores)
        return train_sizes[best_idx]

    def _classify_overfitting_severity(self, gap):
        """ê³¼ì í•© ì‹¬ê°ë„ ë¶„ë¥˜"""
        if gap < 0.02:
            return "ë§¤ìš° ë‚®ìŒ"
        elif gap < 0.05:
            return "ë‚®ìŒ"
        elif gap < 0.1:
            return "ë³´í†µ"
        elif gap < 0.2:
            return "ë†’ìŒ"
        else:
            return "ë§¤ìš° ë†’ìŒ"

    def _assess_overfitting_severity(self, learning_analysis, validation_analysis, target_gap):
        """ê³¼ì í•© ì‹¬ê°ë„ ì¢…í•© í‰ê°€"""
        print("\nğŸ“Š 7ë‹¨ê³„: ê³¼ì í•© ì‹¬ê°ë„ ì¢…í•© í‰ê°€")

        severity_scores = []
        model_assessments = {}

        for model_name, model_result in learning_analysis.items():
            if model_result is None:
                continue

            gap = model_result['final_gap']
            severity = model_result['overfitting_severity']

            # ì ìˆ˜í™” (0-5 ìŠ¤ì¼€ì¼)
            severity_map = {
                "ë§¤ìš° ë‚®ìŒ": 1,
                "ë‚®ìŒ": 2,
                "ë³´í†µ": 3,
                "ë†’ìŒ": 4,
                "ë§¤ìš° ë†’ìŒ": 5
            }

            score = severity_map.get(severity, 3)
            severity_scores.append(score)

            model_assessments[model_name] = {
                'gap': gap,
                'severity': severity,
                'severity_score': score,
                'exceeds_target': gap > target_gap
            }

        # ì „ì²´ í‰ê°€
        overall_severity_score = np.mean(severity_scores) if severity_scores else 3
        overall_severity = self._score_to_severity(overall_severity_score)

        # ëª©í‘œ ê²©ì°¨ì™€ ë¹„êµ
        models_exceeding_target = sum(1 for assessment in model_assessments.values()
                                    if assessment['exceeds_target'])

        results = {
            'overall_severity_score': overall_severity_score,
            'overall_severity': overall_severity,
            'target_gap': target_gap,
            'models_exceeding_target': models_exceeding_target,
            'model_assessments': model_assessments,
            'requires_action': overall_severity_score > 3 or models_exceeding_target > 0
        }

        print(f"  ğŸ“Š ì „ì²´ ì‹¬ê°ë„: {overall_severity} (ì ìˆ˜: {overall_severity_score:.2f})")
        print(f"  ğŸ¯ ëª©í‘œ ê²©ì°¨ ì´ˆê³¼ ëª¨ë¸: {models_exceeding_target}ê°œ")
        print(f"  âš ï¸ ì¡°ì¹˜ í•„ìš”: {'ì˜ˆ' if results['requires_action'] else 'ì•„ë‹ˆì˜¤'}")

        return results

    def _score_to_severity(self, score):
        """ì ìˆ˜ë¥¼ ì‹¬ê°ë„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if score <= 1.5:
            return "ë§¤ìš° ë‚®ìŒ"
        elif score <= 2.5:
            return "ë‚®ìŒ"
        elif score <= 3.5:
            return "ë³´í†µ"
        elif score <= 4.5:
            return "ë†’ìŒ"
        else:
            return "ë§¤ìš° ë†’ìŒ"

    def _generate_overfitting_solutions(self, learning_analysis, validation_analysis,
                                       feature_analysis, regularization_analysis,
                                       severity_assessment):
        """ê³¼ì í•© í•´ê²°ë°©ì•ˆ ìƒì„±"""
        print("\nğŸ’¡ 8ë‹¨ê³„: ê³¼ì í•© í•´ê²°ë°©ì•ˆ ìƒì„±")

        solutions = []

        # ì‹¬ê°ë„ì— ë”°ë¥¸ ê¸°ë³¸ ì „ëµ
        severity_score = severity_assessment['overall_severity_score']

        if severity_score > 4:  # ë§¤ìš° ë†’ìŒ/ë†’ìŒ
            solutions.extend([
                {
                    'priority': 'HIGH',
                    'category': 'model_complexity',
                    'solution': 'ëª¨ë¸ ë³µì¡ë„ ëŒ€í­ ê°ì†Œ',
                    'details': [
                        'RandomForest: max_depth=3-5, min_samples_leaf=20-50',
                        'LogisticRegression: ê°•í•œ L1 ì •ê·œí™” (C=0.01-0.1)',
                        'íŠ¹ì„± ìˆ˜ ì¤„ì´ê¸° (SelectKBest, RFE)'
                    ],
                    'expected_impact': 'ê³¼ì í•© ê²©ì°¨ 50% ì´ìƒ ê°ì†Œ'
                },
                {
                    'priority': 'HIGH',
                    'category': 'data_augmentation',
                    'solution': 'ë°ì´í„° ì¦ê°• ë° ì •ê·œí™”',
                    'details': [
                        'êµì°¨ê²€ì¦ í´ë“œ ìˆ˜ ì¦ê°€ (10-15 folds)',
                        'Dropout, Early Stopping ì ìš©',
                        'ì•™ìƒë¸” ë‹¤ì–‘ì„± ì¦ê°€'
                    ],
                    'expected_impact': 'ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ'
                }
            ])
        elif severity_score > 3:  # ë³´í†µ
            solutions.extend([
                {
                    'priority': 'MEDIUM',
                    'category': 'regularization',
                    'solution': 'ì ì ˆí•œ ì •ê·œí™” ì ìš©',
                    'details': [],
                    'expected_impact': 'ê³¼ì í•© ê²©ì°¨ 20-30% ê°ì†Œ'
                }
            ])

        # íŠ¹ì„± ë¶„ì„ ê¸°ë°˜ ì†”ë£¨ì…˜
        if feature_analysis:
            optimal_features = feature_analysis['optimal_features']
            current_features = len(feature_analysis['feature_counts']) - 1
            if optimal_features < feature_analysis['feature_counts'][-1]:
                solutions.append({
                    'priority': 'MEDIUM',
                    'category': 'feature_selection',
                    'solution': f'íŠ¹ì„± ìˆ˜ë¥¼ {optimal_features}ê°œë¡œ ì¤„ì´ê¸°',
                    'details': [
                        'SelectKBest, RFE, ë˜ëŠ” L1 ì •ê·œí™” ì‚¬ìš©',
                        'íŠ¹ì„± ì¤‘ìš”ë„ ê¸°ë°˜ ì„ íƒ',
                        'ìƒê´€ê´€ê³„ ë†’ì€ íŠ¹ì„± ì œê±°'
                    ],
                    'expected_impact': 'ì°¨ì›ì˜ ì €ì£¼ ë°©ì§€, ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ'
                })

        # ì •ê·œí™” ë¶„ì„ ê¸°ë°˜ ì†”ë£¨ì…˜
        if regularization_analysis:
            for penalty, penalty_data in regularization_analysis.items():
                if penalty_data['best_gap'] < 0.05:  # ì¢‹ì€ ì •ê·œí™” íš¨ê³¼
                    solutions.append({
                        'priority': 'MEDIUM',
                        'category': 'regularization',
                        'solution': f'{penalty.upper()} ì •ê·œí™” ì ìš© (C={penalty_data["best_C"]})',
                        'details': [
                            f'ê²€ì¦ëœ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©',
                            f'ì˜ˆìƒ ê²©ì°¨: {penalty_data["best_gap"]:.3f}'
                        ],
                        'expected_impact': 'ê³¼ì í•© ì œì–´ ë° ì„±ëŠ¥ ì•ˆì •í™”'
                    })

        # ì•™ìƒë¸” ê¸°ë°˜ ì†”ë£¨ì…˜
        solutions.append({
            'priority': 'LOW',
            'category': 'ensemble',
            'solution': 'ì•™ìƒë¸” ë‹¤ì–‘ì„± ì¦ê°€',
            'details': [
                'ë‹¤ì–‘í•œ ì‹œë“œë¡œ ì—¬ëŸ¬ ëª¨ë¸ í•™ìŠµ',
                'ì„œë¡œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ ì¡°í•©',
                'Bagging, Boosting ê¸°ë²• í™œìš©'
            ],
            'expected_impact': 'ê°œë³„ ëª¨ë¸ ê³¼ì í•© ìƒì‡„'
        })

        # ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
        priority_order = {'HIGH': 1, 'MEDIUM': 2, 'LOW': 3}
        solutions.sort(key=lambda x: priority_order.get(x['priority'], 3))

        print(f"  ğŸ’¡ ìƒì„±ëœ í•´ê²°ë°©ì•ˆ: {len(solutions)}ê°œ")
        for i, solution in enumerate(solutions[:3], 1):
            print(f"    {i}. [{solution['priority']}] {solution['solution']}")

        return solutions

    def _log_overfitting_results(self, results):
        """ì‹¤í—˜ ì¶”ì ì— ê²°ê³¼ ë¡œê¹…"""
        # ì£¼ìš” ë©”íŠ¸ë¦­ ë¡œê¹…
        severity = results['severity_assessment']
        metrics = {
            'overall_severity_score': severity['overall_severity_score'],
            'models_exceeding_target': severity['models_exceeding_target'],
            'requires_action': 1 if severity['requires_action'] else 0
        }

        # í•™ìŠµ ê³¡ì„  ê²°ê³¼
        for model_name, model_result in results['learning_curves'].items():
            if model_result:
                metrics[f'{model_name}_final_gap'] = model_result['final_gap']
                metrics[f'{model_name}_optimal_size'] = model_result['optimal_training_size']

        # íŠ¹ì„± í•™ìŠµ ê³¡ì„  ê²°ê³¼
        if results['feature_curves']:
            metrics['optimal_features'] = results['feature_curves']['optimal_features']

        # ì•™ìƒë¸” ë‹¤ì–‘ì„± ê²°ê³¼
        if results['ensemble_diversity']:
            ensemble = results['ensemble_diversity']
            metrics['ensemble_improvement'] = ensemble['ensemble_improvement']
            metrics['ensemble_diversity'] = ensemble['diversity_metrics']['diversity_score']

        self.tracker.log_metrics(metrics)

        # íŒŒë¼ë¯¸í„° ë¡œê¹…
        params = {
            'target_score_gap': results['data_info']['target_score_gap'],
            'n_samples': results['data_info']['n_samples'],
            'n_features': results['data_info']['n_features'],
            'n_classes': results['data_info']['n_classes'],
            'n_solutions': len(results['solutions'])
        }
        self.tracker.log_params(params)

        # ì‹œê°í™” ë¡œê¹…
        for fig_name, fig in self.figures.items():
            self.tracker.log_figure(fig, fig_name)

        self.tracker.end_run()

    def _print_analysis_summary(self, results):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*70)
        print("ğŸ” ê³¼ì í•© ì§„ë‹¨ ì‹œìŠ¤í…œ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*70)

        severity = results['severity_assessment']
        print(f"\nğŸ¯ ì „ì²´ ê³¼ì í•© ì‹¬ê°ë„: {severity['overall_severity']} ({severity['overall_severity_score']:.2f}/5.0)")
        print(f"ğŸ“Š ëª©í‘œ ê²©ì°¨({severity['target_gap']:.3f}) ì´ˆê³¼ ëª¨ë¸: {severity['models_exceeding_target']}ê°œ")
        print(f"âš ï¸ ì¡°ì¹˜ í•„ìš” ì—¬ë¶€: {'ì˜ˆ' if severity['requires_action'] else 'ì•„ë‹ˆì˜¤'}")

        # ëª¨ë¸ë³„ ê²©ì°¨
        print(f"\nğŸ¤– ëª¨ë¸ë³„ ê³¼ì í•© ê²©ì°¨:")
        for model_name, assessment in severity['model_assessments'].items():
            status = "âš ï¸ ì´ˆê³¼" if assessment['exceeds_target'] else "âœ… ì–‘í˜¸"
            print(f"  â€¢ {model_name}: {assessment['gap']:.3f} ({assessment['severity']}) {status}")

        # ì£¼ìš” ë°œê²¬ì‚¬í•­
        if results['feature_curves']:
            print(f"\nğŸ” íŠ¹ì„± ë¶„ì„:")
            print(f"  â€¢ ìµœì  íŠ¹ì„± ìˆ˜: {results['feature_curves']['optimal_features']}ê°œ")

        if results['ensemble_diversity']:
            ensemble = results['ensemble_diversity']
            print(f"\nğŸ­ ì•™ìƒë¸” ë¶„ì„:")
            print(f"  â€¢ ê°œë³„ ëª¨ë¸: {ensemble['individual_mean']:.3f} Â± {ensemble['individual_std']:.3f}")
            print(f"  â€¢ ì•™ìƒë¸” ì„±ëŠ¥: {ensemble['ensemble_score']:.3f}")
            print(f"  â€¢ ê°œì„  íš¨ê³¼: +{ensemble['ensemble_improvement']:.3f}")

        # í•µì‹¬ í•´ê²°ë°©ì•ˆ
        print(f"\nğŸ’¡ í•µì‹¬ í•´ê²°ë°©ì•ˆ:")
        high_priority_solutions = [sol for sol in results['solutions'] if sol['priority'] == 'HIGH']
        if high_priority_solutions:
            for i, solution in enumerate(high_priority_solutions[:3], 1):
                print(f"  {i}. {solution['solution']}")
                print(f"     â†’ {solution['expected_impact']}")
        else:
            medium_solutions = [sol for sol in results['solutions'] if sol['priority'] == 'MEDIUM'][:2]
            for i, solution in enumerate(medium_solutions, 1):
                print(f"  {i}. {solution['solution']}")

        print("="*70)

    def get_actionable_recommendations(self):
        """ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­ ë°˜í™˜"""
        if not self.results:
            print("âš ï¸ ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return []

        return self.results['solutions']


if __name__ == "__main__":
    """ê³¼ì í•© ì§„ë‹¨ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ê³¼ì í•© ì§„ë‹¨ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")

    # ë”ë¯¸ ë°ì´í„° ìƒì„± (ê³¼ì í•©ì´ ë°œìƒí•˜ë„ë¡)
    from sklearn.datasets import make_classification

    X, y = make_classification(
        n_samples=500,      # ì ì€ ìƒ˜í”Œ
        n_features=50,      # ë§ì€ íŠ¹ì„± (ê³¼ì í•© ìœ ë°œ)
        n_classes=5,
        n_informative=20,
        n_redundant=10,
        n_clusters_per_class=1,
        random_state=42
    )

    X_df = pd.DataFrame(X, columns=[f'feature_{i:02d}' for i in range(50)])
    y_series = pd.Series(y)

    # ë¶„ì„ ì‹¤í–‰
    analyzer = OverfittingAnalyzer()
    results = analyzer.comprehensive_overfitting_analysis(
        X_df, y_series,
        target_score_gap=0.096  # 9.6% ëª©í‘œ ê²©ì°¨
    )

    # ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­ ì¶œë ¥
    recommendations = analyzer.get_actionable_recommendations()
    if recommendations:
        print("\nğŸ¯ ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(recommendations[:5], 1):
            print(f"  {i}. [{rec['priority']}] {rec['solution']}")
            if rec['details']:
                for detail in rec['details'][:2]:
                    print(f"      - {detail}")

    print("\nâœ… T006 ì™„ë£Œ: ê³¼ì í•© ì§„ë‹¨ ì‹œìŠ¤í…œ êµ¬í˜„ ì„±ê³µ!")
    print("ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: T007 (í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„)")
"""
êµì°¨ê²€ì¦ ì „ëµ ë¹„êµ ë¶„ì„
T005 íƒœìŠ¤í¬: ë‹¤ì–‘í•œ CV ì „ëµì˜ ì„±ëŠ¥ê³¼ ì‹ ë¢°ë„ ë¹„êµ

ëª©ì :
- í˜„ì¬ CV ì „ëµì˜ ë¬¸ì œì  ì§„ë‹¨
- ë‹¤ì–‘í•œ CV ë°©ë²•ë¡ ì˜ ì„±ëŠ¥ ë¹„êµ
- Adversarial Validation ê²°ê³¼ë¥¼ ë°˜ì˜í•œ ìµœì  ì „ëµ ë„ì¶œ
- CV ì ìˆ˜ì™€ ì‹¤ì œ ì„±ëŠ¥ ê°„ ê²©ì°¨ ìµœì†Œí™”

ë¹„êµí•  CV ì „ëµ:
1. StratifiedKFold (í˜„ì¬ ì‚¬ìš©)
2. TimeSeriesSplit (ì‹œê°„ ìˆœì„œ ê³ ë ¤)
3. GroupKFold (ê·¸ë£¹ ê¸°ë°˜)
4. RepeatedStratifiedKFold (ë°˜ë³µ ê²€ì¦)
5. Custom Domain-Aware Split
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import (
    StratifiedKFold, TimeSeriesSplit, GroupKFold,
    RepeatedStratifiedKFold, cross_val_score, cross_validate
)
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, accuracy_score, classification_report
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.tracking.experiment_tracker import ExperimentTracker
from src.utils.config import get_config, get_paths


class CrossValidationAnalyzer:
    """
    êµì°¨ê²€ì¦ ì „ëµ ë¹„êµ ë¶„ì„ê¸°
    ë‹¤ì–‘í•œ CV ë°©ë²•ë¡ ì„ ì²´ê³„ì ìœ¼ë¡œ ë¹„êµí•˜ê³  ìµœì  ì „ëµ ë„ì¶œ
    """

    def __init__(self, use_tracking: bool = True):
        """
        CV ë¶„ì„ê¸° ì´ˆê¸°í™”

        Args:
            use_tracking: ì‹¤í—˜ ì¶”ì  ì‚¬ìš© ì—¬ë¶€
        """
        self.use_tracking = use_tracking
        self.results = {}
        self.figures = {}

        # ê²½ë¡œ ì„¤ì •
        self.paths = get_paths()
        self.plots_dir = self.paths['experiments_dir'] / 'plots' / 'cv_analysis'
        self.plots_dir.mkdir(parents=True, exist_ok=True)

        # ì‹¤í—˜ ì¶”ì  ì„¤ì •
        if self.use_tracking:
            self.tracker = ExperimentTracker(
                project_name=get_config('tracking.project_name'),
                experiment_name="cv_strategy_analysis"
            )

        print("ğŸ“Š êµì°¨ê²€ì¦ ì „ëµ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

    def compare_cv_strategies(self, X, y, models=None, adversarial_auc=None):
        """
        ë‹¤ì–‘í•œ êµì°¨ê²€ì¦ ì „ëµ ì¢…í•© ë¹„êµ

        Args:
            X: íŠ¹ì„± ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            models: ë¹„êµí•  ëª¨ë¸ë“¤ (ê¸°ë³¸ê°’: RandomForest, LogisticRegression)
            adversarial_auc: Adversarial Validation AUC ì ìˆ˜

        Returns:
            Dict: ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("\nğŸ“Š êµì°¨ê²€ì¦ ì „ëµ ì¢…í•© ë¹„êµ ì‹œì‘...")
        print("=" * 60)

        # ì‹¤í—˜ ì¶”ì  ì‹œì‘
        if self.use_tracking:
            self.tracker.start_run(
                run_name="cv_strategy_comparison",
                description="ë‹¤ì–‘í•œ CV ì „ëµ ì„±ëŠ¥ ë° ì‹ ë¢°ë„ ë¹„êµ ë¶„ì„",
                tags={
                    "analysis_type": "cv_comparison",
                    "task": "T005",
                    "purpose": "optimal_cv_selection"
                }
            )

        # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
        if models is None:
            models = {
                'RandomForest': RandomForestClassifier(
                    n_estimators=100,
                    random_state=get_config('models.random_seed'),
                    n_jobs=-1
                ),
                'LogisticRegression': LogisticRegression(
                    random_state=get_config('models.random_seed'),
                    max_iter=1000
                )
            }

        # 1. CV ì „ëµ ì •ì˜
        cv_strategies = self._define_cv_strategies(X, y, adversarial_auc)

        # 2. ê° ì „ëµë³„ ì„±ëŠ¥ í‰ê°€
        strategy_results = self._evaluate_cv_strategies(X, y, cv_strategies, models)

        # 3. ì•ˆì •ì„± ë¶„ì„
        stability_analysis = self._analyze_cv_stability(X, y, cv_strategies, models)

        # 4. í†µê³„ì  ìœ ì˜ì„± ê²€ì¦
        statistical_analysis = self._perform_statistical_tests(strategy_results, models)

        # 5. ì‹œê°í™” ìƒì„±
        visualization_results = self._create_cv_visualizations(
            strategy_results, stability_analysis
        )

        # 6. ìµœì  ì „ëµ ì¶”ì²œ
        recommendations = self._recommend_optimal_strategy(
            strategy_results, stability_analysis, statistical_analysis, adversarial_auc
        )

        # ê²°ê³¼ ì¢…í•©
        comprehensive_results = {
            'cv_strategies': cv_strategies,
            'strategy_results': strategy_results,
            'stability_analysis': stability_analysis,
            'statistical_analysis': statistical_analysis,
            'visualizations': visualization_results,
            'recommendations': recommendations,
            'data_info': {
                'n_samples': len(X),
                'n_features': len(X.columns),
                'n_classes': len(np.unique(y)),
                'class_distribution': np.bincount(y).tolist()
            }
        }

        self.results = comprehensive_results

        # ì‹¤í—˜ ì¶”ì  ë¡œê¹…
        if self.use_tracking:
            self._log_cv_results(comprehensive_results)

        print("\nâœ… êµì°¨ê²€ì¦ ì „ëµ ë¹„êµ ë¶„ì„ ì™„ë£Œ!")
        self._print_analysis_summary(comprehensive_results)

        return comprehensive_results

    def _define_cv_strategies(self, X, y, adversarial_auc):
        """CV ì „ëµ ì •ì˜"""
        print("\nğŸ¯ 1ë‹¨ê³„: CV ì „ëµ ì •ì˜")

        random_state = get_config('models.random_seed')
        strategies = {}

        # 1. StratifiedKFold (ê¸°ë³¸)
        strategies['StratifiedKFold_5'] = {
            'cv': StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),
            'description': 'ê¸°ë³¸ 5-fold ê³„ì¸µí™” êµì°¨ê²€ì¦',
            'type': 'stratified'
        }

        # 2. StratifiedKFold (ë” ë§ì€ í´ë“œ)
        strategies['StratifiedKFold_10'] = {
            'cv': StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state),
            'description': '10-fold ê³„ì¸µí™” êµì°¨ê²€ì¦ (ë” ì•ˆì •ì )',
            'type': 'stratified'
        }

        # 3. RepeatedStratifiedKFold
        strategies['RepeatedStratifiedKFold'] = {
            'cv': RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=random_state),
            'description': 'ë°˜ë³µ ê³„ì¸µí™” êµì°¨ê²€ì¦ (5-fold x 3íšŒ)',
            'type': 'repeated'
        }

        # 4. TimeSeriesSplit (ì‹œê°„ ìˆœì„œ ê³ ë ¤)
        if len(X) >= 50:  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ í™•ì¸
            n_splits = min(5, len(X) // 10)  # ì ì ˆí•œ split ìˆ˜ ê³„ì‚°
            strategies['TimeSeriesSplit'] = {
                'cv': TimeSeriesSplit(n_splits=n_splits),
                'description': f'ì‹œê°„ ìˆœì„œ ê¸°ë°˜ {n_splits}-fold êµì°¨ê²€ì¦',
                'type': 'temporal'
            }

        # 5. Custom Domain-Aware Split (Adversarial ê²°ê³¼ ë°˜ì˜)
        if adversarial_auc and adversarial_auc > 0.6:
            strategies['DomainAwareSplit'] = {
                'cv': self._create_domain_aware_split(X, y, adversarial_auc),
                'description': 'Domain Shiftë¥¼ ê³ ë ¤í•œ ë§ì¶¤í˜• ë¶„í• ',
                'type': 'domain_aware'
            }

        # 6. GroupKFold (ê°€ëŠ¥í•œ ê²½ìš°)
        if len(X) >= 100:  # ê·¸ë£¹ì„ ë§Œë“¤ê¸°ì— ì¶©ë¶„í•œ ìƒ˜í”Œ
            groups = self._create_artificial_groups(X, y)
            strategies['GroupKFold'] = {
                'cv': GroupKFold(n_splits=5),
                'description': 'ê·¸ë£¹ ê¸°ë°˜ 5-fold êµì°¨ê²€ì¦',
                'type': 'group',
                'groups': groups
            }

        print(f"  â€¢ ì •ì˜ëœ CV ì „ëµ: {len(strategies)}ê°œ")
        for name, info in strategies.items():
            print(f"    - {name}: {info['description']}")

        return strategies

    def _create_domain_aware_split(self, X, y, adversarial_auc):
        """Domain-aware ë¶„í•  ì „ëµ ìƒì„±"""
        # Adversarial AUCê°€ ë†’ì„ìˆ˜ë¡ ë” ë³´ìˆ˜ì ì¸ ë¶„í•  ì‚¬ìš©
        if adversarial_auc > 0.75:
            # ë§¤ìš° ë³´ìˆ˜ì : ì‹œê°„ ìˆœì„œ ê¸°ë°˜
            return TimeSeriesSplit(n_splits=3)
        else:
            # ë³´í†µ ìˆ˜ì¤€: ë” ë§ì€ í´ë“œë¡œ ì•ˆì •ì„± í–¥ìƒ
            return StratifiedKFold(n_splits=8, shuffle=True,
                                 random_state=get_config('models.random_seed'))

    def _create_artificial_groups(self, X, y):
        """ì¸ìœ„ì  ê·¸ë£¹ ìƒì„± (ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ë„ë©”ì¸ ì§€ì‹ í™œìš©)"""
        # ê°„ë‹¨í•œ í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ê·¸ë£¹ ìƒì„±
        from sklearn.cluster import KMeans

        n_groups = min(10, len(X) // 20)  # ì ì ˆí•œ ê·¸ë£¹ ìˆ˜
        if n_groups < 3:
            n_groups = 3

        kmeans = KMeans(n_clusters=n_groups, random_state=get_config('models.random_seed'))
        groups = kmeans.fit_predict(X)

        return groups

    def _evaluate_cv_strategies(self, X, y, cv_strategies, models):
        """ê° CV ì „ëµë³„ ì„±ëŠ¥ í‰ê°€"""
        print("\nâš¡ 2ë‹¨ê³„: CV ì „ëµë³„ ì„±ëŠ¥ í‰ê°€")

        results = {}

        for strategy_name, strategy_info in cv_strategies.items():
            print(f"\n  ğŸ“‹ {strategy_name} í‰ê°€ ì¤‘...")

            strategy_results = {'models': {}}
            cv = strategy_info['cv']

            for model_name, model in models.items():
                try:
                    # GroupKFoldì˜ ê²½ìš° groups íŒŒë¼ë¯¸í„° í•„ìš”
                    if strategy_info.get('type') == 'group':
                        groups = strategy_info['groups']
                        cv_results = cross_validate(
                            model, X, y, cv=cv, groups=groups,
                            scoring=['f1_macro', 'accuracy'],
                            return_train_score=True, n_jobs=-1
                        )
                    else:
                        cv_results = cross_validate(
                            model, X, y, cv=cv,
                            scoring=['f1_macro', 'accuracy'],
                            return_train_score=True, n_jobs=-1
                        )

                    # ê²°ê³¼ ì •ë¦¬
                    model_results = {
                        'f1_macro_test': cv_results['test_f1_macro'],
                        'f1_macro_train': cv_results['train_f1_macro'],
                        'accuracy_test': cv_results['test_accuracy'],
                        'accuracy_train': cv_results['train_accuracy'],
                        'f1_macro_mean': cv_results['test_f1_macro'].mean(),
                        'f1_macro_std': cv_results['test_f1_macro'].std(),
                        'accuracy_mean': cv_results['test_accuracy'].mean(),
                        'accuracy_std': cv_results['test_accuracy'].std(),
                        'overfitting_gap': (cv_results['train_f1_macro'].mean() -
                                          cv_results['test_f1_macro'].mean())
                    }

                    strategy_results['models'][model_name] = model_results

                    print(f"    â€¢ {model_name}: F1={model_results['f1_macro_mean']:.4f}Â±{model_results['f1_macro_std']:.4f}")

                except Exception as e:
                    print(f"    âŒ {model_name} ì˜¤ë¥˜: {str(e)}")
                    strategy_results['models'][model_name] = None

            strategy_results['description'] = strategy_info['description']
            strategy_results['type'] = strategy_info['type']
            results[strategy_name] = strategy_results

        return results

    def _analyze_cv_stability(self, X, y, cv_strategies, models):
        """CV ì•ˆì •ì„± ë¶„ì„ (ì—¬ëŸ¬ ì‹œë“œë¡œ ë°˜ë³µ ì‹¤í–‰)"""
        print("\nğŸ”„ 3ë‹¨ê³„: CV ì•ˆì •ì„± ë¶„ì„")

        stability_results = {}
        test_seeds = [42, 123, 456, 789, 999]  # 5ê°œ ì‹œë“œë¡œ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸

        # ì£¼ìš” ì „ëµë“¤ë§Œ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ (ì‹œê°„ ì ˆì•½)
        key_strategies = ['StratifiedKFold_5', 'StratifiedKFold_10', 'RepeatedStratifiedKFold']
        key_strategies = [s for s in key_strategies if s in cv_strategies]

        for strategy_name in key_strategies:
            print(f"  ğŸ”„ {strategy_name} ì•ˆì •ì„± í…ŒìŠ¤íŠ¸...")

            strategy_stability = {'models': {}}

            for model_name, base_model in models.items():
                seed_results = []

                for seed in test_seeds:
                    # ëª¨ë¸ê³¼ CV ëª¨ë‘ ì‹œë“œ ë³€ê²½
                    model = base_model.__class__(**base_model.get_params())
                    if hasattr(model, 'random_state'):
                        model.set_params(random_state=seed)

                    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)

                    try:
                        scores = cross_val_score(model, X, y, cv=cv, scoring='f1_macro', n_jobs=-1)
                        seed_results.append(scores.mean())
                    except:
                        pass

                if seed_results:
                    strategy_stability['models'][model_name] = {
                        'seed_scores': seed_results,
                        'mean_score': np.mean(seed_results),
                        'std_score': np.std(seed_results),
                        'min_score': np.min(seed_results),
                        'max_score': np.max(seed_results),
                        'score_range': np.max(seed_results) - np.min(seed_results),
                        'stability_coefficient': np.std(seed_results) / (np.mean(seed_results) + 1e-8)
                    }

                    print(f"    â€¢ {model_name}: ë²”ìœ„={strategy_stability['models'][model_name]['score_range']:.4f}")

            stability_results[strategy_name] = strategy_stability

        return stability_results

    def _perform_statistical_tests(self, strategy_results, models):
        """í†µê³„ì  ìœ ì˜ì„± ê²€ì¦"""
        print("\nğŸ“ˆ 4ë‹¨ê³„: í†µê³„ì  ìœ ì˜ì„± ê²€ì¦")

        statistical_results = {}

        # ê° ëª¨ë¸ë³„ë¡œ ì „ëµ ê°„ ì„±ëŠ¥ ë¹„êµ
        for model_name in models.keys():
            print(f"  ğŸ“Š {model_name} ì „ëµ ê°„ ë¹„êµ:")

            model_comparisons = {}
            strategy_scores = {}

            # ê° ì „ëµì˜ CV ì ìˆ˜ ìˆ˜ì§‘
            for strategy_name, strategy_result in strategy_results.items():
                if (strategy_result['models'][model_name] and
                    strategy_result['models'][model_name] is not None):

                    f1_scores = strategy_result['models'][model_name]['f1_macro_test']
                    strategy_scores[strategy_name] = f1_scores

            # ì „ëµ ê°„ t-test ìˆ˜í–‰
            strategy_names = list(strategy_scores.keys())

            if len(strategy_names) >= 2:
                # ë² ìŠ¤íŠ¸ ì „ëµê³¼ ë‹¤ë¥¸ ì „ëµë“¤ ë¹„êµ
                best_strategy = max(strategy_names,
                                  key=lambda x: strategy_results[x]['models'][model_name]['f1_macro_mean'])

                best_scores = strategy_scores[best_strategy]

                for strategy_name in strategy_names:
                    if strategy_name != best_strategy:
                        other_scores = strategy_scores[strategy_name]

                        # t-test ìˆ˜í–‰ (ê¸¸ì´ê°€ ê°™ì„ ë•Œë§Œ)
                        if (len(best_scores) > 1 and len(other_scores) > 1 and
                            len(best_scores) == len(other_scores)):
                            statistic, p_value = stats.ttest_rel(best_scores, other_scores)
                        else:
                            # ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ ë…ë¦½ t-test ì‚¬ìš©
                            statistic, p_value = stats.ttest_ind(best_scores, other_scores)

                            model_comparisons[f'{best_strategy}_vs_{strategy_name}'] = {
                                'statistic': statistic,
                                'p_value': p_value,
                                'significant': p_value < 0.05,
                                'better_strategy': best_strategy if statistic > 0 else strategy_name
                            }

                            significance = "ìœ ì˜í•¨" if p_value < 0.05 else "ë¹„ìœ ì˜í•¨"
                            print(f"    â€¢ {best_strategy} vs {strategy_name}: p={p_value:.4f} ({significance})")

            statistical_results[model_name] = {
                'best_strategy': best_strategy if 'best_strategy' in locals() else None,
                'comparisons': model_comparisons
            }

        return statistical_results

    def _create_cv_visualizations(self, strategy_results, stability_analysis):
        """CV ë¹„êµ ì‹œê°í™” ìƒì„±"""
        print("\nğŸ“Š 5ë‹¨ê³„: CV ë¹„êµ ì‹œê°í™” ìƒì„±")

        # 1. ì „ëµë³„ ì„±ëŠ¥ ë¹„êµ ë°•ìŠ¤í”Œë¡¯
        self._plot_strategy_performance_comparison(strategy_results)

        # 2. ì•ˆì •ì„± ë¶„ì„ ì‹œê°í™”
        if stability_analysis:
            self._plot_stability_analysis(stability_analysis)

        # 3. ê³¼ì í•© ë¶„ì„ ì‹œê°í™”
        self._plot_overfitting_analysis(strategy_results)

        return {
            'performance_comparison': str(self.plots_dir / 'strategy_performance_comparison.png'),
            'stability_analysis': str(self.plots_dir / 'stability_analysis.png'),
            'overfitting_analysis': str(self.plots_dir / 'overfitting_analysis.png')
        }

    def _plot_strategy_performance_comparison(self, strategy_results):
        """ì „ëµë³„ ì„±ëŠ¥ ë¹„êµ ë°•ìŠ¤í”Œë¡¯"""
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))

        models = ['RandomForest', 'LogisticRegression']

        for idx, model_name in enumerate(models):
            ax = axes[idx]

            box_data = []
            box_labels = []

            for strategy_name, strategy_result in strategy_results.items():
                if (strategy_result['models'].get(model_name) and
                    strategy_result['models'][model_name] is not None):

                    f1_scores = strategy_result['models'][model_name]['f1_macro_test']
                    box_data.append(f1_scores)
                    box_labels.append(strategy_name.replace('StratifiedKFold_', 'SK'))

            if box_data:
                bp = ax.boxplot(box_data, labels=box_labels, patch_artist=True)

                # ìƒ‰ìƒ ì„¤ì •
                colors = plt.cm.Set3(np.linspace(0, 1, len(bp['boxes'])))
                for patch, color in zip(bp['boxes'], colors):
                    patch.set_facecolor(color)

                ax.set_title(f'{model_name} - CV Strategy Performance')
                ax.set_ylabel('F1-Score (Macro)')
                ax.tick_params(axis='x', rotation=45)
                ax.grid(True, alpha=0.3)

        plt.tight_layout()

        # ì €ì¥
        perf_path = self.plots_dir / 'strategy_performance_comparison.png'
        plt.savefig(perf_path, dpi=300, bbox_inches='tight')
        self.figures['performance_comparison'] = plt.gcf()
        plt.close()

    def _plot_stability_analysis(self, stability_analysis):
        """ì•ˆì •ì„± ë¶„ì„ ì‹œê°í™”"""
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))

        models = ['RandomForest', 'LogisticRegression']

        for idx, model_name in enumerate(models):
            ax = axes[idx]

            strategies = []
            stability_scores = []
            score_ranges = []

            for strategy_name, strategy_result in stability_analysis.items():
                if strategy_result['models'].get(model_name):
                    model_stability = strategy_result['models'][model_name]
                    strategies.append(strategy_name.replace('StratifiedKFold_', 'SK'))
                    stability_scores.append(model_stability['stability_coefficient'])
                    score_ranges.append(model_stability['score_range'])

            if strategies:
                x_pos = np.arange(len(strategies))

                # ì•ˆì •ì„± ê³„ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì•ˆì •ì )
                bars1 = ax.bar(x_pos - 0.2, stability_scores, 0.4,
                             label='Stability Coefficient', alpha=0.7, color='blue')

                # ì ìˆ˜ ë²”ìœ„ (ë‚®ì„ìˆ˜ë¡ ì•ˆì •ì )
                ax2 = ax.twinx()
                bars2 = ax2.bar(x_pos + 0.2, score_ranges, 0.4,
                              label='Score Range', alpha=0.7, color='red')

                ax.set_xlabel('CV Strategy')
                ax.set_ylabel('Stability Coefficient', color='blue')
                ax2.set_ylabel('Score Range', color='red')
                ax.set_title(f'{model_name} - CV Strategy Stability')
                ax.set_xticks(x_pos)
                ax.set_xticklabels(strategies, rotation=45)

                # ë²”ë¡€
                ax.legend(loc='upper left')
                ax2.legend(loc='upper right')

                ax.grid(True, alpha=0.3)

        plt.tight_layout()

        # ì €ì¥
        stability_path = self.plots_dir / 'stability_analysis.png'
        plt.savefig(stability_path, dpi=300, bbox_inches='tight')
        self.figures['stability_analysis'] = plt.gcf()
        plt.close()

    def _plot_overfitting_analysis(self, strategy_results):
        """ê³¼ì í•© ë¶„ì„ ì‹œê°í™”"""
        fig, ax = plt.subplots(figsize=(12, 8))

        models = ['RandomForest', 'LogisticRegression']
        colors = ['blue', 'red']

        for model_idx, (model_name, color) in enumerate(zip(models, colors)):
            strategies = []
            overfitting_gaps = []

            for strategy_name, strategy_result in strategy_results.items():
                if (strategy_result['models'].get(model_name) and
                    strategy_result['models'][model_name] is not None):

                    gap = strategy_result['models'][model_name]['overfitting_gap']
                    strategies.append(strategy_name.replace('StratifiedKFold_', 'SK'))
                    overfitting_gaps.append(gap)

            if strategies:
                x_pos = np.arange(len(strategies)) + model_idx * 0.4
                ax.bar(x_pos, overfitting_gaps, 0.4,
                      label=model_name, alpha=0.7, color=color)

        ax.set_xlabel('CV Strategy')
        ax.set_ylabel('Overfitting Gap (Train - Test F1)')
        ax.set_title('Overfitting Analysis by CV Strategy')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # xì¶• ë¼ë²¨ ì„¤ì •
        if strategies:  # strategiesê°€ ì •ì˜ë˜ì–´ ìˆë‹¤ë©´
            ax.set_xticks(np.arange(len(strategies)) + 0.2)
            ax.set_xticklabels(strategies, rotation=45)

        plt.tight_layout()

        # ì €ì¥
        overfitting_path = self.plots_dir / 'overfitting_analysis.png'
        plt.savefig(overfitting_path, dpi=300, bbox_inches='tight')
        self.figures['overfitting_analysis'] = plt.gcf()
        plt.close()

    def _recommend_optimal_strategy(self, strategy_results, stability_analysis,
                                   statistical_analysis, adversarial_auc):
        """ìµœì  CV ì „ëµ ì¶”ì²œ"""
        print("\nğŸ¯ 6ë‹¨ê³„: ìµœì  CV ì „ëµ ì¶”ì²œ")

        recommendations = {}

        # ê° ëª¨ë¸ë³„ ìµœì  ì „ëµ ë¶„ì„
        for model_name in ['RandomForest', 'LogisticRegression']:
            print(f"\n  ğŸ¤– {model_name} ìµœì  ì „ëµ ë¶„ì„:")

            # ì„±ëŠ¥ ê¸°ë°˜ ë­í‚¹
            performance_ranking = []
            for strategy_name, strategy_result in strategy_results.items():
                if (strategy_result['models'].get(model_name) and
                    strategy_result['models'][model_name] is not None):

                    model_result = strategy_result['models'][model_name]
                    performance_ranking.append({
                        'strategy': strategy_name,
                        'f1_mean': model_result['f1_macro_mean'],
                        'f1_std': model_result['f1_macro_std'],
                        'overfitting_gap': model_result['overfitting_gap']
                    })

            # ì„±ëŠ¥ìˆœ ì •ë ¬
            performance_ranking.sort(key=lambda x: x['f1_mean'], reverse=True)

            # ì•ˆì •ì„± ê³ ë ¤
            stability_bonus = {}
            if model_name in stability_analysis.get('StratifiedKFold_5', {}).get('models', {}):
                for strategy_name, strategy_stability in stability_analysis.items():
                    if strategy_stability['models'].get(model_name):
                        stability_coef = strategy_stability['models'][model_name]['stability_coefficient']
                        # ë‚®ì€ ì•ˆì •ì„± ê³„ìˆ˜(ë” ì•ˆì •)ì— ë³´ë„ˆìŠ¤
                        stability_bonus[strategy_name] = max(0, 0.02 - stability_coef)

            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            final_ranking = []
            for perf in performance_ranking:
                strategy = perf['strategy']
                base_score = perf['f1_mean']
                stability_boost = stability_bonus.get(strategy, 0)
                overfitting_penalty = max(0, perf['overfitting_gap'] - 0.1) * 0.1

                final_score = base_score + stability_boost - overfitting_penalty

                final_ranking.append({
                    'strategy': strategy,
                    'final_score': final_score,
                    'base_performance': base_score,
                    'stability_boost': stability_boost,
                    'overfitting_penalty': overfitting_penalty
                })

            final_ranking.sort(key=lambda x: x['final_score'], reverse=True)

            # ìµœì  ì „ëµ ì„ ì •
            if final_ranking:
                best_strategy = final_ranking[0]

                recommendations[model_name] = {
                    'recommended_strategy': best_strategy['strategy'],
                    'final_score': best_strategy['final_score'],
                    'ranking': final_ranking[:3],  # ìƒìœ„ 3ê°œ
                    'reason': self._generate_recommendation_reason(
                        best_strategy, adversarial_auc
                    )
                }

                print(f"    ğŸ† ì¶”ì²œ: {best_strategy['strategy']} (ì ìˆ˜: {best_strategy['final_score']:.4f})")
                print(f"    ğŸ’­ ì´ìœ : {recommendations[model_name]['reason']}")

        # ì „ì²´ ì¶”ì²œì‚¬í•­
        overall_recommendation = self._generate_overall_recommendation(
            recommendations, adversarial_auc
        )

        return {
            'model_specific': recommendations,
            'overall': overall_recommendation
        }

    def _generate_recommendation_reason(self, best_strategy, adversarial_auc):
        """ì¶”ì²œ ì´ìœ  ìƒì„±"""
        strategy_name = best_strategy['strategy']
        reasons = []

        if 'StratifiedKFold_10' in strategy_name:
            reasons.append("ë” ë§ì€ í´ë“œë¡œ ì•ˆì •ì„± í™•ë³´")
        if 'Repeated' in strategy_name:
            reasons.append("ë°˜ë³µ ê²€ì¦ìœ¼ë¡œ ì‹ ë¢°ì„± í–¥ìƒ")
        if 'TimeSeriesSplit' in strategy_name:
            reasons.append("ì‹œê°„ ìˆœì„œ ê³ ë ¤ë¡œ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€")
        if 'DomainAware' in strategy_name:
            reasons.append("Domain Shift ê³ ë ¤í•œ ë§ì¶¤í˜• ì „ëµ")

        if adversarial_auc and adversarial_auc > 0.7:
            reasons.append("ë†’ì€ Domain Shift ëŒ€ì‘")

        if best_strategy['overfitting_penalty'] < 0.01:
            reasons.append("ë‚®ì€ ê³¼ì í•© ìœ„í—˜")

        return "; ".join(reasons) if reasons else "ìµœê³  ì„±ëŠ¥"

    def _generate_overall_recommendation(self, model_recommendations, adversarial_auc):
        """ì „ì²´ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        # ê°€ì¥ ë§ì´ ì¶”ì²œëœ ì „ëµ ì°¾ê¸°
        strategy_votes = {}
        for model_rec in model_recommendations.values():
            strategy = model_rec['recommended_strategy']
            strategy_votes[strategy] = strategy_votes.get(strategy, 0) + 1

        if strategy_votes:
            best_overall_strategy = max(strategy_votes, key=strategy_votes.get)
        else:
            best_overall_strategy = "StratifiedKFold_5"

        # ì¶”ê°€ ì¡°ê±´ë¶€ ì¶”ì²œ
        additional_recommendations = []

        if adversarial_auc and adversarial_auc > 0.75:
            additional_recommendations.append("ì‹¬ê°í•œ Domain Shift â†’ TimeSeriesSplit ìš°ì„  ê³ ë ¤")
        elif adversarial_auc and adversarial_auc > 0.65:
            additional_recommendations.append("ë³´í†µ Domain Shift â†’ StratifiedKFold í´ë“œ ìˆ˜ ì¦ê°€")

        return {
            'best_strategy': best_overall_strategy,
            'consensus_level': strategy_votes.get(best_overall_strategy, 0),
            'additional_recommendations': additional_recommendations
        }

    def _log_cv_results(self, results):
        """ì‹¤í—˜ ì¶”ì ì— ê²°ê³¼ ë¡œê¹…"""
        # ì£¼ìš” ë©”íŠ¸ë¦­ ë¡œê¹…
        strategy_results = results['strategy_results']

        metrics = {}

        # ê° ì „ëµë³„ ì„±ëŠ¥ ë¡œê¹…
        for strategy_name, strategy_result in strategy_results.items():
            for model_name, model_result in strategy_result['models'].items():
                if model_result:
                    prefix = f"{strategy_name}_{model_name}"
                    metrics[f"{prefix}_f1_mean"] = model_result['f1_macro_mean']
                    metrics[f"{prefix}_f1_std"] = model_result['f1_macro_std']
                    metrics[f"{prefix}_overfitting_gap"] = model_result['overfitting_gap']

        # ì¶”ì²œ ì „ëµ ë¡œê¹…
        for model_name, recommendation in results['recommendations']['model_specific'].items():
            metrics[f"best_strategy_{model_name}"] = 1  # ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ëŠ” ìˆ«ìë¡œ

        self.tracker.log_metrics(metrics)

        # íŒŒë¼ë¯¸í„° ë¡œê¹…
        params = {
            'n_strategies': len(strategy_results),
            'overall_best_strategy': results['recommendations']['overall']['best_strategy'],
            'n_samples': results['data_info']['n_samples'],
            'n_classes': results['data_info']['n_classes']
        }
        self.tracker.log_params(params)

        # ì‹œê°í™” ë¡œê¹…
        for fig_name, fig in self.figures.items():
            self.tracker.log_figure(fig, fig_name)

        self.tracker.end_run()

    def _print_analysis_summary(self, results):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*70)
        print("ğŸ“Š êµì°¨ê²€ì¦ ì „ëµ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*70)

        # ì „ì²´ ì¶”ì²œ
        overall_rec = results['recommendations']['overall']
        print(f"\nğŸ† ì „ì²´ ì¶”ì²œ ì „ëµ: {overall_rec['best_strategy']}")
        print(f"   í•©ì˜ ìˆ˜ì¤€: {overall_rec['consensus_level']}/2 ëª¨ë¸")

        # ëª¨ë¸ë³„ ì¶”ì²œ
        print(f"\nğŸ¤– ëª¨ë¸ë³„ ì¶”ì²œ:")
        for model_name, rec in results['recommendations']['model_specific'].items():
            print(f"  â€¢ {model_name}: {rec['recommended_strategy']}")
            print(f"    â””â”€ ì´ìœ : {rec['reason']}")

        # ì„±ëŠ¥ ìš”ì•½
        print(f"\nğŸ“Š ì„±ëŠ¥ ìš”ì•½:")
        best_performances = {}
        for strategy_name, strategy_result in results['strategy_results'].items():
            for model_name, model_result in strategy_result['models'].items():
                if model_result:
                    key = f"{model_name}"
                    if key not in best_performances:
                        best_performances[key] = []
                    best_performances[key].append({
                        'strategy': strategy_name,
                        'score': model_result['f1_macro_mean']
                    })

        for model_name, performances in best_performances.items():
            best_perf = max(performances, key=lambda x: x['score'])
            print(f"  â€¢ {model_name} ìµœê³ : {best_perf['score']:.4f} ({best_perf['strategy']})")

        # ì¶”ê°€ ê¶Œì¥ì‚¬í•­
        if overall_rec['additional_recommendations']:
            print(f"\nğŸ’¡ ì¶”ê°€ ê¶Œì¥ì‚¬í•­:")
            for rec in overall_rec['additional_recommendations']:
                print(f"  â€¢ {rec}")

        print("="*70)


if __name__ == "__main__":
    """êµì°¨ê²€ì¦ ì „ëµ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª êµì°¨ê²€ì¦ ì „ëµ ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹œì‘...")

    # ë”ë¯¸ ë°ì´í„° ìƒì„±
    from sklearn.datasets import make_classification

    X, y = make_classification(
        n_samples=1000,
        n_features=20,
        n_classes=5,
        n_informative=15,
        n_clusters_per_class=1,
        random_state=42
    )

    X_df = pd.DataFrame(X, columns=[f'feature_{i:02d}' for i in range(20)])
    y_series = pd.Series(y)

    # ë¶„ì„ ì‹¤í–‰ (Adversarial AUC ì‹œë®¬ë ˆì´ì…˜)
    analyzer = CrossValidationAnalyzer()
    results = analyzer.compare_cv_strategies(
        X_df, y_series,
        adversarial_auc=0.72  # ë†’ì€ Domain Shift ì‹œë®¬ë ˆì´ì…˜
    )

    print("\nâœ… T005 ì™„ë£Œ: êµì°¨ê²€ì¦ ì „ëµ ë¹„êµ ë¶„ì„ ì„±ê³µ!")
    print("ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: T006 (ê³¼ì í•© ì§„ë‹¨ ì‹œìŠ¤í…œ êµ¬í˜„)")
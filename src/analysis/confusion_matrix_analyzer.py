"""
T010: Confusion Matrix ì‹¬í™” ë¶„ì„ì„ í†µí•œ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ í‰ê°€

ì´ ëª¨ë“ˆì€ Confusion Matrixë¥¼ í™œìš©í•˜ì—¬ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ì˜ ì„±ëŠ¥ì„
í´ë˜ìŠ¤ë³„ë¡œ ìƒì„¸íˆ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    confusion_matrix, classification_report,
    precision_recall_fscore_support,
    balanced_accuracy_score, cohen_kappa_score
)
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_predict
import warnings
from pathlib import Path
import mlflow
import wandb
from typing import Dict, List, Tuple, Optional, Union
import logging
from itertools import combinations
from scipy import stats

class ConfusionMatrixAnalyzer:
    """Confusion Matrixë¥¼ í†µí•œ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì‹¬í™” ë¶„ì„ í´ë˜ìŠ¤"""

    def __init__(self, config=None, experiment_tracker=None):
        """
        Confusion Matrix ë¶„ì„ê¸° ì´ˆê¸°í™”

        Args:
            config: ì„¤ì • ê°ì²´
            experiment_tracker: ì‹¤í—˜ ì¶”ì ê¸° ê°ì²´
        """
        self.config = config
        self.experiment_tracker = experiment_tracker
        self.logger = logging.getLogger(__name__)

        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
        if config:
            self.plots_dir = Path(config.get_paths().get('plots', 'plots'))
            self.experiments_dir = Path(config.get_paths().get('experiments', 'experiments'))
        else:
            self.plots_dir = Path('plots')
            self.experiments_dir = Path('experiments')

        self.confusion_dir = self.plots_dir / 'confusion_analysis'
        self.confusion_dir.mkdir(parents=True, exist_ok=True)

        # í•œê¸€ í°íŠ¸ ì„¤ì •
        plt.rcParams['font.family'] = 'DejaVu Sans'
        warnings.filterwarnings('ignore', category=UserWarning)

        self.results = {}

    def calculate_confusion_matrix(
        self,
        model,
        X_val: pd.DataFrame,
        y_val: pd.Series,
        class_names: List[str] = None
    ) -> Dict:
        """
        Confusion Matrix ê³„ì‚° ë° ê¸°ë³¸ ë©”íŠ¸ë¦­ ì¶”ì¶œ

        Args:
            model: í•™ìŠµëœ ëª¨ë¸
            X_val: ê²€ì¦ ë°ì´í„° íŠ¹ì„±
            y_val: ê²€ì¦ ë°ì´í„° ë ˆì´ë¸”
            class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸

        Returns:
            Confusion Matrix ë¶„ì„ ê²°ê³¼
        """
        try:
            self.logger.info("ğŸ”„ Confusion Matrix ê³„ì‚° ì‹œì‘...")

            # ì˜ˆì¸¡ ìˆ˜í–‰
            y_pred = model.predict(X_val)
            y_pred_proba = model.predict_proba(X_val)

            # Confusion Matrix ê³„ì‚°
            cm = confusion_matrix(y_val, y_pred)

            # í´ë˜ìŠ¤ ì´ë¦„ ì„¤ì •
            if class_names is None:
                unique_classes = sorted(set(list(y_val.unique()) + list(y_pred)))
                class_names = [f"Class_{i}" for i in unique_classes]

            # ì •ê·œí™”ëœ Confusion Matrix (í–‰ ë‹¨ìœ„)
            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            cm_normalized = np.nan_to_num(cm_normalized)

            # ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°
            precision, recall, f1, support = precision_recall_fscore_support(y_val, y_pred, average=None)

            # ì „ì²´ ë©”íŠ¸ë¦­
            overall_metrics = {
                'accuracy': (y_pred == y_val).mean(),
                'balanced_accuracy': balanced_accuracy_score(y_val, y_pred),
                'macro_f1': f1.mean(),
                'weighted_f1': np.average(f1, weights=support),
                'cohen_kappa': cohen_kappa_score(y_val, y_pred)
            }

            # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­
            class_metrics = pd.DataFrame({
                'class': class_names[:len(precision)],
                'class_id': sorted(set(list(y_val.unique()) + list(y_pred)))[:len(precision)],
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'support': support
            })

            results = {
                'confusion_matrix': cm,
                'confusion_matrix_normalized': cm_normalized,
                'class_names': class_names,
                'class_metrics': class_metrics,
                'overall_metrics': overall_metrics,
                'predictions': y_pred,
                'prediction_probabilities': y_pred_proba,
                'true_labels': y_val
            }

            self.logger.info(f"âœ… Confusion Matrix ê³„ì‚° ì™„ë£Œ")
            self.logger.info(f"   - ì „ì²´ ì •í™•ë„: {overall_metrics['accuracy']:.4f}")
            self.logger.info(f"   - ê· í˜• ì •í™•ë„: {overall_metrics['balanced_accuracy']:.4f}")
            self.logger.info(f"   - Macro F1: {overall_metrics['macro_f1']:.4f}")

            return results

        except Exception as e:
            self.logger.error(f"âŒ Confusion Matrix ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            raise

    def analyze_misclassification_patterns(self, cm_results: Dict) -> Dict:
        """
        ì˜¤ë¶„ë¥˜ íŒ¨í„´ ë¶„ì„

        Args:
            cm_results: Confusion Matrix ë¶„ì„ ê²°ê³¼

        Returns:
            ì˜¤ë¶„ë¥˜ íŒ¨í„´ ë¶„ì„ ê²°ê³¼
        """
        try:
            self.logger.info("ğŸ”„ ì˜¤ë¶„ë¥˜ íŒ¨í„´ ë¶„ì„ ì‹œì‘...")

            cm = cm_results['confusion_matrix']
            cm_norm = cm_results['confusion_matrix_normalized']
            class_names = cm_results['class_names']

            n_classes = len(class_names)

            # 1. ê°€ì¥ ë¹ˆë²ˆí•œ ì˜¤ë¶„ë¥˜ ì°¾ê¸°
            misclassifications = []
            for i in range(n_classes):
                for j in range(n_classes):
                    if i != j and cm[i, j] > 0:
                        misclassifications.append({
                            'true_class': class_names[i],
                            'predicted_class': class_names[j],
                            'count': cm[i, j],
                            'rate': cm_norm[i, j],
                            'true_class_id': i,
                            'predicted_class_id': j
                        })

            # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬
            misclassifications.sort(key=lambda x: x['count'], reverse=True)

            # 2. í´ë˜ìŠ¤ë³„ ì˜¤ë¶„ë¥˜ í†µê³„
            class_error_stats = []
            for i, class_name in enumerate(class_names):
                if i < cm.shape[0]:  # ì¸ë±ìŠ¤ ë²”ìœ„ í™•ì¸
                    correct = cm[i, i] if i < cm.shape[1] else 0
                    total = cm[i, :].sum() if cm[i, :].sum() > 0 else 1

                    # ê°€ì¥ í”í•œ ì˜¤ë¶„ë¥˜ í´ë˜ìŠ¤
                    misclass_counts = [(j, cm[i, j]) for j in range(n_classes) if j != i and j < cm.shape[1]]
                    most_confused_with = max(misclass_counts, key=lambda x: x[1]) if misclass_counts else (None, 0)

                    class_error_stats.append({
                        'class': class_name,
                        'class_id': i,
                        'correct_predictions': correct,
                        'total_instances': total,
                        'error_rate': 1 - (correct / total),
                        'most_confused_with': class_names[most_confused_with[0]] if most_confused_with[0] is not None else 'None',
                        'most_confused_count': most_confused_with[1]
                    })

            # 3. ëŒ€ì¹­ì  ì˜¤ë¶„ë¥˜ ì°¾ê¸° (Aâ†’Bì™€ Bâ†’Aê°€ ëª¨ë‘ ë†’ì€ ê²½ìš°)
            symmetric_errors = []
            for i in range(n_classes):
                for j in range(i+1, n_classes):
                    if i < cm.shape[0] and j < cm.shape[1] and i < cm.shape[1] and j < cm.shape[0]:
                        error_ij = cm[i, j]
                        error_ji = cm[j, i]
                        if error_ij > 0 and error_ji > 0:
                            symmetric_errors.append({
                                'class_pair': (class_names[i], class_names[j]),
                                'mutual_confusion_score': min(error_ij, error_ji),
                                'total_confusion': error_ij + error_ji,
                                'asymmetry': abs(error_ij - error_ji) / max(error_ij + error_ji, 1)
                            })

            symmetric_errors.sort(key=lambda x: x['mutual_confusion_score'], reverse=True)

            results = {
                'top_misclassifications': misclassifications[:20],  # ìƒìœ„ 20ê°œ
                'class_error_statistics': class_error_stats,
                'symmetric_errors': symmetric_errors[:10],  # ìƒìœ„ 10ê°œ
                'total_misclassifications': len([x for x in misclassifications if x['count'] > 0])
            }

            self.logger.info("âœ… ì˜¤ë¶„ë¥˜ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ")
            self.logger.info(f"   - ì´ ì˜¤ë¶„ë¥˜ íŒ¨í„´ ìˆ˜: {results['total_misclassifications']}")
            if misclassifications:
                top_error = misclassifications[0]
                self.logger.info(f"   - ê°€ì¥ ë¹ˆë²ˆí•œ ì˜¤ë¶„ë¥˜: {top_error['true_class']} â†’ {top_error['predicted_class']} ({top_error['count']}íšŒ)")

            return results

        except Exception as e:
            self.logger.error(f"âŒ ì˜¤ë¶„ë¥˜ íŒ¨í„´ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {}

    def create_confusion_matrix_plots(
        self,
        cm_results: Dict,
        model_name: str = "RandomForest"
    ) -> Dict[str, str]:
        """
        Confusion Matrix ì‹œê°í™”

        Args:
            cm_results: Confusion Matrix ë¶„ì„ ê²°ê³¼
            model_name: ëª¨ë¸ ì´ë¦„

        Returns:
            ìƒì„±ëœ í”Œë¡¯ íŒŒì¼ ê²½ë¡œë“¤
        """
        plot_paths = {}
        model_dir = self.confusion_dir / f"{model_name.lower()}-confusion"
        model_dir.mkdir(exist_ok=True)

        try:
            cm = cm_results['confusion_matrix']
            cm_norm = cm_results['confusion_matrix_normalized']
            class_names = cm_results['class_names']

            # í´ë˜ìŠ¤ ìˆ˜ê°€ ë§ì€ ê²½ìš° ë¼ë²¨ ë‹¨ìˆœí™”
            display_labels = class_names
            if len(class_names) > 15:
                display_labels = [f"C{i}" for i in range(len(class_names))]

            # 1. ì›ë³¸ Confusion Matrix
            plt.figure(figsize=(12, 10))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                       xticklabels=display_labels, yticklabels=display_labels)
            plt.title(f'{model_name}\nConfusion Matrix (Counts)')
            plt.xlabel('Predicted Class')
            plt.ylabel('True Class')
            plt.tight_layout()

            plot_path = model_dir / 'confusion_matrix_counts.png'
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            plt.close()
            plot_paths['counts'] = str(plot_path)

            # 2. ì •ê·œí™”ëœ Confusion Matrix
            plt.figure(figsize=(12, 10))
            sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',
                       xticklabels=display_labels, yticklabels=display_labels,
                       vmin=0, vmax=1)
            plt.title(f'{model_name}\nNormalized Confusion Matrix (Proportions)')
            plt.xlabel('Predicted Class')
            plt.ylabel('True Class')
            plt.tight_layout()

            plot_path = model_dir / 'confusion_matrix_normalized.png'
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            plt.close()
            plot_paths['normalized'] = str(plot_path)

            # 3. í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë©”íŠ¸ë¦­
            class_metrics = cm_results['class_metrics']

            fig, axes = plt.subplots(2, 2, figsize=(15, 12))

            # Precision
            axes[0,0].bar(range(len(class_metrics)), class_metrics['precision'], alpha=0.7, color='skyblue')
            axes[0,0].set_title('Precision by Class')
            axes[0,0].set_xlabel('Class')
            axes[0,0].set_ylabel('Precision')
            axes[0,0].set_xticks(range(len(class_metrics)))
            axes[0,0].set_xticklabels(display_labels[:len(class_metrics)], rotation=45)
            axes[0,0].grid(True, alpha=0.3)

            # Recall
            axes[0,1].bar(range(len(class_metrics)), class_metrics['recall'], alpha=0.7, color='lightcoral')
            axes[0,1].set_title('Recall by Class')
            axes[0,1].set_xlabel('Class')
            axes[0,1].set_ylabel('Recall')
            axes[0,1].set_xticks(range(len(class_metrics)))
            axes[0,1].set_xticklabels(display_labels[:len(class_metrics)], rotation=45)
            axes[0,1].grid(True, alpha=0.3)

            # F1-Score
            axes[1,0].bar(range(len(class_metrics)), class_metrics['f1_score'], alpha=0.7, color='lightgreen')
            axes[1,0].set_title('F1-Score by Class')
            axes[1,0].set_xlabel('Class')
            axes[1,0].set_ylabel('F1-Score')
            axes[1,0].set_xticks(range(len(class_metrics)))
            axes[1,0].set_xticklabels(display_labels[:len(class_metrics)], rotation=45)
            axes[1,0].grid(True, alpha=0.3)

            # Support
            axes[1,1].bar(range(len(class_metrics)), class_metrics['support'], alpha=0.7, color='gold')
            axes[1,1].set_title('Support (Sample Count) by Class')
            axes[1,1].set_xlabel('Class')
            axes[1,1].set_ylabel('Sample Count')
            axes[1,1].set_xticks(range(len(class_metrics)))
            axes[1,1].set_xticklabels(display_labels[:len(class_metrics)], rotation=45)
            axes[1,1].grid(True, alpha=0.3)

            plt.tight_layout()
            plot_path = model_dir / 'class_performance_metrics.png'
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            plt.close()
            plot_paths['metrics'] = str(plot_path)

            # 4. ì˜¤ë¶„ë¥˜ íˆíŠ¸ë§µ (ëŒ€ê°ì„  ì œì™¸)
            cm_errors = cm.copy()
            np.fill_diagonal(cm_errors, 0)  # ì •í™•í•œ ì˜ˆì¸¡ ì œê±°

            if cm_errors.sum() > 0:  # ì˜¤ë¶„ë¥˜ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                plt.figure(figsize=(12, 10))
                sns.heatmap(cm_errors, annot=True, fmt='d', cmap='Reds',
                           xticklabels=display_labels, yticklabels=display_labels)
                plt.title(f'{model_name}\nMisclassification Patterns')
                plt.xlabel('Predicted Class')
                plt.ylabel('True Class')
                plt.tight_layout()

                plot_path = model_dir / 'misclassification_heatmap.png'
                plt.savefig(plot_path, dpi=150, bbox_inches='tight')
                plt.close()
                plot_paths['misclassification'] = str(plot_path)

            self.logger.info(f"âœ… {len(plot_paths)}ê°œ Confusion Matrix ì‹œê°í™” ì™„ë£Œ: {model_dir}")

        except Exception as e:
            self.logger.error(f"âŒ Confusion Matrix ì‹œê°í™” ì˜¤ë¥˜: {str(e)}")

        return plot_paths

    def analyze_class_difficulty(self, cm_results: Dict, pattern_results: Dict) -> Dict:
        """
        í´ë˜ìŠ¤ë³„ ë¶„ë¥˜ ë‚œì´ë„ ë¶„ì„

        Args:
            cm_results: Confusion Matrix ë¶„ì„ ê²°ê³¼
            pattern_results: ì˜¤ë¶„ë¥˜ íŒ¨í„´ ë¶„ì„ ê²°ê³¼

        Returns:
            í´ë˜ìŠ¤ ë‚œì´ë„ ë¶„ì„ ê²°ê³¼
        """
        try:
            self.logger.info("ğŸ”„ í´ë˜ìŠ¤ ë‚œì´ë„ ë¶„ì„ ì‹œì‘...")

            class_metrics = cm_results['class_metrics']
            class_errors = pattern_results['class_error_statistics']

            # ë‚œì´ë„ ì ìˆ˜ ê³„ì‚° (ì—¬ëŸ¬ ì§€í‘œì˜ ì¡°í•©)
            difficulty_analysis = []

            for i, row in class_metrics.iterrows():
                error_info = next((x for x in class_errors if x['class_id'] == row['class_id']), {})

                # ë‚œì´ë„ ì ìˆ˜ ê³„ì‚° (0-1, ë†’ì„ìˆ˜ë¡ ì–´ë ¤ì›€)
                precision_penalty = 1 - row['precision']
                recall_penalty = 1 - row['recall']
                f1_penalty = 1 - row['f1_score']
                error_rate = error_info.get('error_rate', 0)

                # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì¢…í•© ë‚œì´ë„ ê³„ì‚°
                difficulty_score = (
                    0.3 * precision_penalty +
                    0.3 * recall_penalty +
                    0.2 * f1_penalty +
                    0.2 * error_rate
                )

                # ìƒ˜í”Œ ìˆ˜ ëŒ€ë¹„ ì„±ëŠ¥ (ë¶ˆê· í˜• ì˜í–¥)
                sample_ratio = row['support'] / class_metrics['support'].sum()
                imbalance_penalty = 1 / (sample_ratio + 0.01)  # ì‘ì€ í´ë˜ìŠ¤ì¼ìˆ˜ë¡ ë†’ì€ penalty

                difficulty_analysis.append({
                    'class': row['class'],
                    'class_id': row['class_id'],
                    'difficulty_score': difficulty_score,
                    'precision': row['precision'],
                    'recall': row['recall'],
                    'f1_score': row['f1_score'],
                    'support': row['support'],
                    'sample_ratio': sample_ratio,
                    'imbalance_penalty': imbalance_penalty,
                    'error_rate': error_rate,
                    'most_confused_with': error_info.get('most_confused_with', 'None'),
                    'difficulty_level': self._categorize_difficulty(difficulty_score)
                })

            # ë‚œì´ë„ìˆœ ì •ë ¬
            difficulty_analysis.sort(key=lambda x: x['difficulty_score'], reverse=True)

            # í†µê³„ ê³„ì‚°
            difficulty_stats = {
                'most_difficult_classes': difficulty_analysis[:5],
                'easiest_classes': difficulty_analysis[-5:],
                'avg_difficulty': np.mean([x['difficulty_score'] for x in difficulty_analysis]),
                'difficulty_distribution': {
                    'very_hard': len([x for x in difficulty_analysis if x['difficulty_level'] == 'Very Hard']),
                    'hard': len([x for x in difficulty_analysis if x['difficulty_level'] == 'Hard']),
                    'medium': len([x for x in difficulty_analysis if x['difficulty_level'] == 'Medium']),
                    'easy': len([x for x in difficulty_analysis if x['difficulty_level'] == 'Easy']),
                    'very_easy': len([x for x in difficulty_analysis if x['difficulty_level'] == 'Very Easy'])
                }
            }

            results = {
                'class_difficulty_analysis': difficulty_analysis,
                'difficulty_statistics': difficulty_stats
            }

            self.logger.info("âœ… í´ë˜ìŠ¤ ë‚œì´ë„ ë¶„ì„ ì™„ë£Œ")
            if difficulty_analysis:
                hardest = difficulty_analysis[0]
                easiest = difficulty_analysis[-1]
                self.logger.info(f"   - ê°€ì¥ ì–´ë ¤ìš´ í´ë˜ìŠ¤: {hardest['class']} (ë‚œì´ë„: {hardest['difficulty_score']:.3f})")
                self.logger.info(f"   - ê°€ì¥ ì‰¬ìš´ í´ë˜ìŠ¤: {easiest['class']} (ë‚œì´ë„: {easiest['difficulty_score']:.3f})")

            return results

        except Exception as e:
            self.logger.error(f"âŒ í´ë˜ìŠ¤ ë‚œì´ë„ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {}

    def _categorize_difficulty(self, score: float) -> str:
        """ë‚œì´ë„ ì ìˆ˜ë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
        if score >= 0.8:
            return 'Very Hard'
        elif score >= 0.6:
            return 'Hard'
        elif score >= 0.4:
            return 'Medium'
        elif score >= 0.2:
            return 'Easy'
        else:
            return 'Very Easy'

    def analyze_confusion_matrix_comprehensive(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_test: pd.DataFrame = None,
        y_test: pd.Series = None,
        model_params: Dict = None,
        class_names: List[str] = None
    ) -> Dict:
        """
        ì¢…í•©ì ì¸ Confusion Matrix ë¶„ì„

        Args:
            X_train: í›ˆë ¨ ë°ì´í„° íŠ¹ì„±
            y_train: í›ˆë ¨ ë°ì´í„° ë ˆì´ë¸”
            X_test: í…ŒìŠ¤íŠ¸ ë°ì´í„° íŠ¹ì„±
            y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë ˆì´ë¸”
            model_params: ëª¨ë¸ íŒŒë¼ë¯¸í„°
            class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì¢…í•© ë¶„ì„ ê²°ê³¼
        """
        try:
            self.logger.info("ğŸš€ T010: Confusion Matrix ì¢…í•© ë¶„ì„ ì‹œì‘")

            # ê¸°ë³¸ ëª¨ë¸ íŒŒë¼ë¯¸í„°
            if model_params is None:
                model_params = {
                    'n_estimators': 100,
                    'max_depth': 10,
                    'random_state': 42,
                    'class_weight': 'balanced'
                }

            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ í›ˆë ¨ ë°ì´í„°ì—ì„œ ë¶„í• 
            if X_test is None or y_test is None:
                from sklearn.model_selection import train_test_split
                X_train_split, X_test, y_train_split, y_test = train_test_split(
                    X_train, y_train, test_size=0.3, random_state=42, stratify=y_train
                )
            else:
                X_train_split = X_train
                y_train_split = y_train

            # í´ë˜ìŠ¤ ì´ë¦„ ì„¤ì •
            if class_names is None:
                unique_classes = sorted(y_train.unique())
                class_names = [f"Class_{i}" for i in unique_classes]

            # ëª¨ë¸ í›ˆë ¨
            self.logger.info("ğŸ”„ RandomForest ëª¨ë¸ í›ˆë ¨...")
            model = RandomForestClassifier(**model_params)
            model.fit(X_train_split, y_train_split)

            # 1. Confusion Matrix ê³„ì‚°
            cm_results = self.calculate_confusion_matrix(
                model, X_test, y_test, class_names
            )

            # 2. ì˜¤ë¶„ë¥˜ íŒ¨í„´ ë¶„ì„
            pattern_results = self.analyze_misclassification_patterns(cm_results)

            # 3. í´ë˜ìŠ¤ ë‚œì´ë„ ë¶„ì„
            difficulty_results = self.analyze_class_difficulty(cm_results, pattern_results)

            # 4. ì‹œê°í™” ìƒì„±
            plot_paths = self.create_confusion_matrix_plots(
                cm_results, model_name="RandomForest-Balanced"
            )

            # 5. ê²°ê³¼ í†µí•©
            results = {
                'confusion_matrix_results': cm_results,
                'misclassification_patterns': pattern_results,
                'class_difficulty_analysis': difficulty_results,
                'plot_paths': plot_paths,
                'model_performance': {
                    'overall_metrics': cm_results['overall_metrics'],
                    'model_params': model_params
                }
            }

            # ì‹¤í—˜ ì¶”ì 
            if self.experiment_tracker:
                self._log_to_experiment_tracker(results)

            self.results = results

            self.logger.info("ğŸ‰ T010: Confusion Matrix ë¶„ì„ ì™„ë£Œ!")
            return results

        except Exception as e:
            self.logger.error(f"âŒ T010 ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            raise

    def _log_to_experiment_tracker(self, results: Dict):
        """ì‹¤í—˜ ì¶”ì  ì‹œìŠ¤í…œì— ê²°ê³¼ ë¡œê·¸"""
        try:
            if not self.experiment_tracker:
                return

            overall_metrics = results['model_performance']['overall_metrics']

            # MLflow ë¡œê¹…
            if hasattr(self.experiment_tracker, 'mlflow_client'):
                with mlflow.start_run(run_name="T010_Confusion_Matrix_Analysis"):
                    # ì „ì²´ ë©”íŠ¸ë¦­ ë¡œê¹…
                    for metric_name, metric_value in overall_metrics.items():
                        mlflow.log_metric(f"overall_{metric_name}", metric_value)

                    # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­ ë¡œê¹…
                    class_metrics = results['confusion_matrix_results']['class_metrics']
                    for idx, row in class_metrics.head(10).iterrows():  # ìƒìœ„ 10ê°œ í´ë˜ìŠ¤ë§Œ
                        mlflow.log_metric(f"class_{row['class_id']}_f1", row['f1_score'])
                        mlflow.log_metric(f"class_{row['class_id']}_precision", row['precision'])
                        mlflow.log_metric(f"class_{row['class_id']}_recall", row['recall'])

                    # ì‹œê°í™” ë¡œê¹…
                    for plot_name, plot_path in results['plot_paths'].items():
                        mlflow.log_artifact(plot_path, f"confusion_plots/{plot_name}")

            # WandB ë¡œê¹…
            if hasattr(self.experiment_tracker, 'wandb_run'):
                # ì „ì²´ ë©”íŠ¸ë¦­
                wandb_metrics = {f"confusion/{k}": v for k, v in overall_metrics.items()}
                wandb.log(wandb_metrics)

                # ì‹œê°í™” ë¡œê¹…
                for plot_name, plot_path in results['plot_paths'].items():
                    wandb.log({f"confusion/{plot_name}": wandb.Image(plot_path)})

        except Exception as e:
            self.logger.warning(f"ì‹¤í—˜ ì¶”ì  ë¡œê¹… ì‹¤íŒ¨: {str(e)}")

    def print_summary(self, results: Dict):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        try:
            print("\n" + "="*80)
            print("ğŸ¯ T010: CONFUSION MATRIX ë¶„ì„ ê²°ê³¼ ìš”ì•½")
            print("="*80)

            overall_metrics = results['model_performance']['overall_metrics']
            class_metrics = results['confusion_matrix_results']['class_metrics']
            pattern_results = results['misclassification_patterns']
            difficulty_results = results['class_difficulty_analysis']

            print(f"ğŸ“Š ì „ì²´ ì„±ëŠ¥:")
            print(f"   â€¢ ì •í™•ë„ (Accuracy): {overall_metrics['accuracy']:.4f}")
            print(f"   â€¢ ê· í˜• ì •í™•ë„ (Balanced Accuracy): {overall_metrics['balanced_accuracy']:.4f}")
            print(f"   â€¢ Macro F1-Score: {overall_metrics['macro_f1']:.4f}")
            print(f"   â€¢ Weighted F1-Score: {overall_metrics['weighted_f1']:.4f}")
            print(f"   â€¢ Cohen's Kappa: {overall_metrics['cohen_kappa']:.4f}")

            print(f"\nğŸ† í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ (ìƒìœ„ 5ê°œ):")
            top_classes = class_metrics.nlargest(5, 'f1_score')
            for idx, row in top_classes.iterrows():
                print(f"   {idx+1}. {row['class']:12s}: F1={row['f1_score']:.3f}, P={row['precision']:.3f}, R={row['recall']:.3f} (n={int(row['support'])})")

            print(f"\nğŸ“‰ ì„±ëŠ¥ì´ ë‚®ì€ í´ë˜ìŠ¤ (í•˜ìœ„ 5ê°œ):")
            bottom_classes = class_metrics.nsmallest(5, 'f1_score')
            for idx, row in bottom_classes.iterrows():
                print(f"   â€¢ {row['class']:12s}: F1={row['f1_score']:.3f}, P={row['precision']:.3f}, R={row['recall']:.3f} (n={int(row['support'])})")

            print(f"\nğŸ”„ ì£¼ìš” ì˜¤ë¶„ë¥˜ íŒ¨í„´:")
            top_misclass = pattern_results['top_misclassifications'][:5]
            for i, error in enumerate(top_misclass):
                print(f"   {i+1}. {error['true_class']} â†’ {error['predicted_class']}: {error['count']}íšŒ ({error['rate']:.1%})")

            print(f"\nğŸ˜° ê°€ì¥ ì–´ë ¤ìš´ í´ë˜ìŠ¤ (ìƒìœ„ 5ê°œ):")
            difficult_stats = difficulty_results['difficulty_statistics']
            for i, cls in enumerate(difficult_stats['most_difficult_classes']):
                print(f"   {i+1}. {cls['class']:12s}: ë‚œì´ë„ {cls['difficulty_score']:.3f} ({cls['difficulty_level']})")
                print(f"      â””â”€ ì£¼ìš” í˜¼ë™ ëŒ€ìƒ: {cls['most_confused_with']}")

            print(f"\nğŸ˜Š ê°€ì¥ ì‰¬ìš´ í´ë˜ìŠ¤ (ìƒìœ„ 5ê°œ):")
            for i, cls in enumerate(difficult_stats['easiest_classes']):
                print(f"   {i+1}. {cls['class']:12s}: ë‚œì´ë„ {cls['difficulty_score']:.3f} ({cls['difficulty_level']})")

            print(f"\nğŸ“Š ë‚œì´ë„ ë¶„í¬:")
            diff_dist = difficult_stats['difficulty_distribution']
            total_classes = sum(diff_dist.values())
            for level, count in diff_dist.items():
                percentage = (count / total_classes) * 100 if total_classes > 0 else 0
                print(f"   â€¢ {level}: {count}ê°œ ({percentage:.1f}%)")

            print(f"\nğŸ” ì¶”ê°€ ì¸ì‚¬ì´íŠ¸:")

            # í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„
            support_std = class_metrics['support'].std()
            support_mean = class_metrics['support'].mean()
            imbalance_ratio = support_std / support_mean if support_mean > 0 else 0

            if imbalance_ratio > 0.5:
                print(f"   â€¢ ì‹¬ê°í•œ í´ë˜ìŠ¤ ë¶ˆê· í˜• ê°ì§€ (ë³€ë™ê³„ìˆ˜: {imbalance_ratio:.2f})")
                min_class = class_metrics.loc[class_metrics['support'].idxmin()]
                max_class = class_metrics.loc[class_metrics['support'].idxmax()]
                ratio = max_class['support'] / max(min_class['support'], 1)
                print(f"     â””â”€ ìµœëŒ€/ìµœì†Œ ë¹„ìœ¨: {ratio:.1f}:1 ({max_class['class']} vs {min_class['class']})")

            # ëŒ€ì¹­ ì˜¤ë¶„ë¥˜ íŒ¨í„´
            symmetric_errors = pattern_results['symmetric_errors']
            if symmetric_errors:
                print(f"   â€¢ ìƒí˜¸ í˜¼ë™ë˜ëŠ” í´ë˜ìŠ¤ ìŒ: {len(symmetric_errors)}ìŒ")
                top_symmetric = symmetric_errors[0]
                print(f"     â””â”€ ê°€ì¥ ë¬¸ì œë˜ëŠ” ìŒ: {top_symmetric['class_pair'][0]} â†” {top_symmetric['class_pair'][1]}")

            print(f"\nğŸ“ ìƒì„±ëœ ì‹œê°í™”:")
            for plot_name, plot_path in results['plot_paths'].items():
                print(f"   â€¢ {plot_name}: {plot_path}")

            print("\n" + "="*80)

        except Exception as e:
            print(f"âŒ ìš”ì•½ ì¶œë ¥ ì˜¤ë¥˜: {str(e)}")
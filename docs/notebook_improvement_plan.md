# ë…¸íŠ¸ë¶ ê°œì„  ê³„íš

**ë¶„ì„ ëŒ€ìƒ**: `notebooks/01_main_analysis.ipynb`
**í˜„ì¬ ìˆ˜ì¤€**: ì¤‘ê¸‰ (6.5/10)
**ëª©í‘œ ìˆ˜ì¤€**: ê³ ê¸‰ (8.5+/10)

---

## ğŸ”´ ê¸´ê¸‰ ê°œì„  í•­ëª© (High Priority)

### 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¶€ì¬ âš ï¸
**ë¬¸ì œ**: ëª¨ë“  ëª¨ë¸ì´ ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë§Œ ì‚¬ìš©, ì„±ëŠ¥ ìµœì í™” ë¯¸ì‹¤ì‹œ
**ì˜í–¥**: 2-5% F1 score ê°œì„  ê¸°íšŒ ì†ì‹¤

**í•„ìš” ì‘ì—…**:
- [ ] RandomizedSearchCV ë˜ëŠ” GridSearchCV êµ¬í˜„
- [ ] LightGBM íŒŒë¼ë¯¸í„° íƒìƒ‰ (n_estimators, max_depth, learning_rate, num_leaves)
- [ ] XGBoost íŒŒë¼ë¯¸í„° íƒìƒ‰ (n_estimators, max_depth, learning_rate, subsample)
- [ ] ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ì¬í•™ìŠµ
- [ ] Cross-validationìœ¼ë¡œ íŠœë‹ íš¨ê³¼ ê²€ì¦

**ì˜ˆìƒ ì½”ë“œ**:
```python
from sklearn.model_selection import RandomizedSearchCV

param_distributions = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [3, 5, 7, 9, -1],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'num_leaves': [31, 50, 100, 150],
    'min_child_samples': [20, 30, 50],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

random_search = RandomizedSearchCV(
    lgbm, param_distributions,
    n_iter=50, cv=5, scoring='f1_macro',
    random_state=42, n_jobs=-1
)
```

---

### 2. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë¶€ì¡± âš ï¸
**ë¬¸ì œ**: ì›ë³¸ 52ê°œ í”¼ì²˜ë§Œ ì‚¬ìš©, íŒŒìƒ ë³€ìˆ˜ ìƒì„± ì—†ìŒ
**ì˜í–¥**: 1-3% F1 score ê°œì„  ê¸°íšŒ ì†ì‹¤

**í•„ìš” ì‘ì—…**:
- [ ] í”¼ì²˜ ê°„ ìƒí˜¸ì‘ìš© í•­ ìƒì„± (multiplication, division)
- [ ] ë‹¤í•­ì‹ í”¼ì²˜ ìƒì„± (PolynomialFeatures)
- [ ] í†µê³„ì  í”¼ì²˜ ìƒì„± (mean, std, min, max of feature groups)
- [ ] ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ í”¼ì²˜ ìƒì„±
- [ ] í”¼ì²˜ ì¤‘ìš”ë„ ê¸°ë°˜ ì„ íƒ (SelectFromModel)

**ì˜ˆìƒ ì½”ë“œ**:
```python
from sklearn.preprocessing import PolynomialFeatures

# ìƒìœ„ ì¤‘ìš”ë„ í”¼ì²˜ë“¤ ê°„ ìƒí˜¸ì‘ìš©
top_features = ['X_01', 'X_06', 'X_45', ...]
poly = PolynomialFeatures(degree=2, interaction_only=True)
interaction_features = poly.fit_transform(X_scaled[top_features])

# í†µê³„ì  í”¼ì²˜
X_scaled['feature_mean'] = X_scaled[feature_cols].mean(axis=1)
X_scaled['feature_std'] = X_scaled[feature_cols].std(axis=1)
X_scaled['feature_max'] = X_scaled[feature_cols].max(axis=1)
X_scaled['feature_min'] = X_scaled[feature_cols].min(axis=1)
```

---

### 3. sklearn Pipeline ë¯¸ì‚¬ìš© âš ï¸
**ë¬¸ì œ**: ìˆ˜ë™ ì „ì²˜ë¦¬ë¡œ ì¸í•œ ë°ì´í„° ìœ ì¶œ ìœ„í—˜, ì¬í˜„ì„± ì €í•˜
**ì˜í–¥**: í”„ë¡œë•ì…˜ ë°°í¬ ë¶ˆê°€, ê²€ì¦ ì‹ ë¢°ë„ ë¬¸ì œ

**í•„ìš” ì‘ì—…**:
- [ ] ì „ì²˜ë¦¬ Pipeline êµ¬ì¶• (Scaler â†’ Feature Engineering)
- [ ] ëª¨ë¸ Pipeline í†µí•©
- [ ] Pipelineìœ¼ë¡œ Cross-validation ì¬ì‹¤í–‰
- [ ] Pipeline ì €ì¥ (joblib/pickle)

**ì˜ˆìƒ ì½”ë“œ**:
```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler

pipeline = Pipeline([
    ('scaler', MinMaxScaler()),
    ('feature_engineer', CustomFeatureEngineering()),
    ('model', lgb.LGBMClassifier())
])

pipeline.fit(X_train, y_train)
predictions = pipeline.predict(X_test)
```

---

## ğŸŸ¡ ì¤‘ìš” ê°œì„  í•­ëª© (Medium Priority)

### 4. ì•™ìƒë¸” ê¸°ë²• ë¶€ì¬
**ë¬¸ì œ**: ë‹¨ì¼ ëª¨ë¸ë§Œ ì‚¬ìš©, ì•™ìƒë¸” ì„±ëŠ¥ í–¥ìƒ ê¸°íšŒ ë¯¸í™œìš©
**ì˜í–¥**: 1-2% F1 score ê°œì„  ê¸°íšŒ ì†ì‹¤

**í•„ìš” ì‘ì—…**:
- [ ] VotingClassifier (Soft voting)
- [ ] StackingClassifier (Meta-learner ì¶”ê°€)
- [ ] Blending (holdout ê¸°ë°˜ ì•™ìƒë¸”)
- [ ] ì•™ìƒë¸” ì„±ëŠ¥ ë¹„êµ ë° ê²€ì¦

**ì˜ˆìƒ ì½”ë“œ**:
```python
from sklearn.ensemble import VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression

# Voting
voting_clf = VotingClassifier(
    estimators=[
        ('lgbm', lgbm_tuned),
        ('xgb', xgb_tuned),
        ('rf', RandomForestClassifier())
    ],
    voting='soft'
)

# Stacking
stacking_clf = StackingClassifier(
    estimators=[
        ('lgbm', lgbm_tuned),
        ('xgb', xgb_tuned)
    ],
    final_estimator=LogisticRegression(multi_class='multinomial'),
    cv=5
)
```

---

### 5. ëª¨ë¸ í•´ì„ ë° ì„¤ëª…ê°€ëŠ¥ì„± ë¶€ì¡±
**ë¬¸ì œ**: ëª¨ë¸ ì˜ˆì¸¡ ê·¼ê±° ë¶ˆëª…í™•, ë””ë²„ê¹… ì–´ë ¤ì›€
**ì˜í–¥**: ëª¨ë¸ ì‹ ë¢°ë„ ì €í•˜, ê°œì„  ë°©í–¥ ë¶ˆëª…í™•

**í•„ìš” ì‘ì—…**:
- [ ] SHAP values ê³„ì‚° ë° ì‹œê°í™”
- [ ] Permutation importance
- [ ] Confusion matrix ìƒì„¸ ë¶„ì„
- [ ] í´ë˜ìŠ¤ë³„ ì˜¤ë¶„ë¥˜ íŒ¨í„´ ë¶„ì„
- [ ] Feature importance ì‹¬ì¸µ ë¶„ì„

**ì˜ˆìƒ ì½”ë“œ**:
```python
import shap

# SHAP values
explainer = shap.TreeExplainer(lgbm)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test)

# Confusion matrix
from sklearn.metrics import confusion_matrix, classification_report
cm = confusion_matrix(y_test, predictions)
print(classification_report(y_test, predictions))
```

---

### 6. í”¼ì²˜ ì„ íƒ ë¯¸ì‹¤ì‹œ
**ë¬¸ì œ**: ëª¨ë“  52ê°œ í”¼ì²˜ ì‚¬ìš©, ë…¸ì´ì¦ˆ í”¼ì²˜ í¬í•¨ ê°€ëŠ¥ì„±
**ì˜í–¥**: ê³¼ì í•© ìœ„í—˜, í•™ìŠµ ì†ë„ ì €í•˜

**í•„ìš” ì‘ì—…**:
- [ ] Recursive Feature Elimination (RFE)
- [ ] SelectFromModel (feature importance ê¸°ë°˜)
- [ ] Correlation-based feature selection
- [ ] í”¼ì²˜ ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ

**ì˜ˆìƒ ì½”ë“œ**:
```python
from sklearn.feature_selection import RFE, SelectFromModel

# RFE
rfe = RFE(estimator=lgbm, n_features_to_select=30, step=1)
rfe.fit(X_train, y_train)
selected_features = X_train.columns[rfe.support_]

# Feature importance ê¸°ë°˜
selector = SelectFromModel(lgbm, threshold='median')
selector.fit(X_train, y_train)
```

---

## ğŸŸ¢ ê¶Œì¥ ê°œì„  í•­ëª© (Low Priority)

### 7. ì½”ë“œ ì¡°ì§í™” ë° ëª¨ë“ˆí™” ë¶€ì¡±
**ë¬¸ì œ**: ë°˜ë³µ ì½”ë“œ, ê¸´ ì…€, ì¬ì‚¬ìš©ì„± ì €í•˜
**ì˜í–¥**: ìœ ì§€ë³´ìˆ˜ ì–´ë ¤ì›€, ê°€ë…ì„± ì €í•˜

**í•„ìš” ì‘ì—…**:
- [ ] ë°˜ë³µ ì½”ë“œ í•¨ìˆ˜í™” (plotting, evaluation)
- [ ] ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ë¶„ë¦¬ (utils.py)
- [ ] ì„¤ì •ê°’ ìƒìˆ˜í™” (config.py)
- [ ] Docstring ì¶”ê°€

**ì˜ˆìƒ êµ¬ì¡°**:
```python
# utils.py
def plot_confusion_matrix(y_true, y_pred, labels):
    """Confusion matrix ì‹œê°í™”"""
    pass

def evaluate_model(model, X_test, y_test):
    """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ì¶œë ¥"""
    pass

def cross_validate_model(model, X, y, cv=5):
    """Cross-validation ìˆ˜í–‰"""
    pass
```

---

### 8. ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¯¸íƒìƒ‰
**ë¬¸ì œ**: ì „í†µì  MLë§Œ ì‚¬ìš©, ì‹ ê²½ë§ ì„±ëŠ¥ ë¹„êµ ì—†ìŒ
**ì˜í–¥**: ìµœì  ì•Œê³ ë¦¬ì¦˜ ì„ íƒ ë¶ˆí™•ì‹¤

**í•„ìš” ì‘ì—…**:
- [ ] Simple MLP (Multi-Layer Perceptron) êµ¬í˜„
- [ ] TabNet ë˜ëŠ” FT-Transformer íƒìƒ‰
- [ ] ë”¥ëŸ¬ë‹ vs ì „í†µ ML ì„±ëŠ¥ ë¹„êµ
- [ ] ê³„ì‚° ë¹„ìš© ëŒ€ë¹„ ì„±ëŠ¥ ë¶„ì„

**ì˜ˆìƒ ì½”ë“œ**:
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

mlp = Sequential([
    Dense(128, activation='relu', input_shape=(52,)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(n_classes, activation='softmax')
])

mlp.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
            metrics=['accuracy'])
```

---

### 9. ê³ ê¸‰ ê²€ì¦ ì „ëµ ë¶€ì¬
**ë¬¸ì œ**: ë‹¨ì¼ CV ì „ëµë§Œ ì‚¬ìš©
**ì˜í–¥**: ê²€ì¦ ì‹ ë¢°ë„ ì œí•œì 

**í•„ìš” ì‘ì—…**:
- [ ] Repeated K-Fold CV
- [ ] Nested Cross-Validation (íŠœë‹ í¸í–¥ ì œê±°)
- [ ] Leave-One-Out CV (ë°ì´í„° ì ì„ ê²½ìš°)
- [ ] ì‹œê³„ì—´ ë°ì´í„°ë©´ TimeSeriesSplit

**ì˜ˆìƒ ì½”ë“œ**:
```python
from sklearn.model_selection import RepeatedStratifiedKFold

repeated_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)
scores = cross_val_score(lgbm, X_scaled, y, cv=repeated_cv, scoring='f1_macro')
```

---

### 10. ë¬¸ì„œí™” ë° ì£¼ì„ ê°œì„ 
**ë¬¸ì œ**: í•œì˜ í˜¼ìš©, ì„¤ëª… ë¶€ì¡±
**ì˜í–¥**: í˜‘ì—… ì–´ë ¤ì›€, ì¬í˜„ì„± ì €í•˜

**í•„ìš” ì‘ì—…**:
- [ ] ë§ˆí¬ë‹¤ìš´ ì„¹ì…˜ êµ¬ì¡°í™”
- [ ] ì½”ë“œ ì£¼ì„ í‘œì¤€í™” (ì˜ì–´ ë˜ëŠ” í•œê¸€ í†µì¼)
- [ ] ë¶„ì„ ë°°ê²½ ë° ëª©ì  ëª…ì‹œ
- [ ] ê²°ê³¼ í•´ì„ ë° ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ë¯¸ ì¶”ê°€

---

## ğŸ“Š ê°œì„  ìš°ì„ ìˆœìœ„ ìš”ì•½

| ìˆœìœ„ | í•­ëª© | ì˜ˆìƒ íš¨ê³¼ | ë‚œì´ë„ | ì†Œìš” ì‹œê°„ |
|-----|------|----------|--------|----------|
| 1 | í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ | ì¤‘ | 2-3ì‹œê°„ |
| 2 | í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ | ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ | ì¤‘ìƒ | 3-4ì‹œê°„ |
| 3 | sklearn Pipeline | ğŸ”¥ğŸ”¥ğŸ”¥ | í•˜ | 1-2ì‹œê°„ |
| 4 | ì•™ìƒë¸” ê¸°ë²• | ğŸ”¥ğŸ”¥ğŸ”¥ | ì¤‘ | 2-3ì‹œê°„ |
| 5 | ëª¨ë¸ í•´ì„ (SHAP) | ğŸ”¥ğŸ”¥ | ì¤‘ | 1-2ì‹œê°„ |
| 6 | í”¼ì²˜ ì„ íƒ | ğŸ”¥ğŸ”¥ | í•˜ | 1ì‹œê°„ |
| 7 | ì½”ë“œ ë¦¬íŒ©í† ë§ | ğŸ”¥ | ì¤‘ | 2-3ì‹œê°„ |
| 8 | ë”¥ëŸ¬ë‹ ëª¨ë¸ | ğŸ”¥ | ìƒ | 4-6ì‹œê°„ |
| 9 | ê³ ê¸‰ ê²€ì¦ ì „ëµ | ğŸ”¥ | í•˜ | 1ì‹œê°„ |
| 10 | ë¬¸ì„œí™” ê°œì„  | ğŸ”¥ | í•˜ | 1-2ì‹œê°„ |

---

## ğŸ¯ ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íš

### Phase 1: Quick Wins (1-2ì¼)
1. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
2. sklearn Pipeline êµ¬ì¶•
3. í”¼ì²˜ ì„ íƒ

**ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ**: +3-6% F1 score

### Phase 2: Advanced Techniques (2-3ì¼)
4. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
5. ì•™ìƒë¸” ê¸°ë²•
6. ëª¨ë¸ í•´ì„ (SHAP)

**ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ**: +2-4% F1 score

### Phase 3: Polish & Production (1-2ì¼)
7. ì½”ë“œ ë¦¬íŒ©í† ë§
8. ë¬¸ì„œí™” ê°œì„ 
9. ê³ ê¸‰ ê²€ì¦ ì „ëµ

**ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ**: +0-1% F1 score (ì•ˆì •ì„± í–¥ìƒ)

### Phase 4: Experimental (ì„ íƒ)
10. ë”¥ëŸ¬ë‹ ëª¨ë¸ íƒìƒ‰

---

## ğŸ“ˆ ì˜ˆìƒ ìµœì¢… ì„±ëŠ¥

**í˜„ì¬ ì„±ëŠ¥**:
- LightGBM CV: ~0.798 F1-macro
- XGBoost CV: ~0.800 F1-macro

**ê°œì„  í›„ ì˜ˆìƒ ì„±ëŠ¥**:
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹: 0.820-0.840
- í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§: 0.830-0.850
- ì•™ìƒë¸”: 0.840-0.860

**ëª©í‘œ**: **0.85+ F1-macro score**
# ğŸ“Š Chapter 0: í˜„ì¬ ì½”ë“œë² ì´ìŠ¤ ë¶„ì„ ë° ì´í•´

**ì‘ì„±ì¼**: 2025-09-25
**ëª©ì **: ê¸°ì¡´ ì™„ë£Œëœ ì‘ì—…ë“¤ì— ëŒ€í•œ ì²´ê³„ì  ë¶„ì„ ë° ì´í•´ë¥¼ í†µí•œ í•™ìŠµ
**ë²”ìœ„**: EDA, ì „ì²˜ë¦¬, ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ë§ ë‹¨ê³„

---

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš” ì¬í™•ì¸

### ë¬¸ì œ ì •ì˜
- **ê³¼ì œëª…**: Dacon ìŠ¤ë§ˆíŠ¸ ì œì¡° ì¥ë¹„ ì´ìƒ ê°ì§€ AI ê²½ì§„ëŒ€íšŒ
- **ë¬¸ì œ ìœ í˜•**: 21ê°œ í´ë˜ìŠ¤ ë‹¤ì¤‘ ë¶„ë¥˜ (Multi-class Classification)
- **í‰ê°€ ì§€í‘œ**: Accuracy (í•˜ì§€ë§Œ ëª©í‘œëŠ” Macro F1-score 0.90+)
- **ë°ì´í„°**: 52ê°œ ì„¼ì„œ íŠ¹ì„± (X_01 ~ X_52), ë¸”ë™ë°•ìŠ¤ í™˜ê²½

### ë°ì´í„°ì…‹ íŠ¹ì„±
- **Training Set**: 21,693ê°œ ìƒ˜í”Œ (ê° í´ë˜ìŠ¤ 1,033ê°œì”© ì™„ë²½í•œ ê· í˜•)
- **Test Set**: 15,004ê°œ ìƒ˜í”Œ
- **íŠ¹ì§•**: ê²°ì¸¡ê°’ ì—†ìŒ, ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (~17MB)

---

## ğŸ“ˆ 1ë‹¨ê³„: íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA) ë¶„ì„

### 1.1 ê¸°ë³¸ EDA (`eda.py`) ë¶„ì„

**êµ¬í˜„ëœ ê¸°ëŠ¥ë“¤**:

#### `load_data()` í•¨ìˆ˜
```python
def load_data():
    train = pd.read_csv('data/open/train.csv')
    test = pd.read_csv('data/open/test.csv')
    sample_sub = pd.read_csv('data/open/sample_submission.csv')
    return train, test, sample_sub
```
- **ëª©ì **: í‘œì¤€í™”ëœ ë°ì´í„° ë¡œë”©
- **ì¥ì **: ì¼ê´€ëœ ë°ì´í„° ì ‘ê·¼ ë°©ì‹
- **í•™ìŠµ**: í•¨ìˆ˜í˜• ì ‘ê·¼ìœ¼ë¡œ ì¬ì‚¬ìš©ì„± í™•ë³´

#### `basic_info()` í•¨ìˆ˜
```python
def basic_info(df, name):
    print(f"Shape: {df.shape}")
    print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    print("Data Types:")
    print(df.dtypes.value_counts())

    if 'target' in df.columns:
        print("Target distribution:")
        print(df['target'].value_counts().sort_index())
        print(f"Class balance ratio: {df['target'].value_counts().min() / df['target'].value_counts().max():.3f}")
```

**ë°œê²¬ëœ í•µì‹¬ í†µì°°**:
- âœ… **ì™„ë²½í•œ í´ë˜ìŠ¤ ê· í˜•**: 21ê°œ í´ë˜ìŠ¤ ê°ê° ì •í™•íˆ 1,033ê°œ ìƒ˜í”Œ
- âœ… **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ì „ì²´ ë°ì´í„°ì…‹ ì•½ 17MB
- âœ… **ê¹¨ë—í•œ ë°ì´í„°**: ê²°ì¸¡ê°’ ì—†ìŒ

#### `analyze_features()` í•¨ìˆ˜
íŠ¹ì„± í†µê³„ ìš”ì•½ì„ í†µí•´ ë°œê²¬í•œ íŒ¨í„´:
```python
Min values range: [-0.235, 0.000]
Max values range: [0.037, 100.241]
Mean values range: [0.018, 50.956]
```

**í•™ìŠµ í¬ì¸íŠ¸**:
- íŠ¹ì„±ë“¤ì˜ ìŠ¤ì¼€ì¼ì´ í¬ê²Œ ë‹¤ë¦„ â†’ ìŠ¤ì¼€ì¼ë§ í•„ìˆ˜
- ì¼ë¶€ íŠ¹ì„±ì€ 0-1 ë²”ìœ„, ì¼ë¶€ëŠ” ë” í° ë²”ìœ„ â†’ ë„ë©”ì¸ íŠ¹ì„± ì°¨ì´ ì‹œì‚¬

#### `analyze_feature_patterns()` í•¨ìˆ˜
íŠ¹ì„±ì„ ë²”ìœ„ë³„ë¡œ ë¶„ë¥˜:
- **ì •ê·œí™”ëœ íŠ¹ì„±** (47ê°œ): 0-1 ì‚¬ì´ ê°’ë“¤ â†’ ëŒ€ë¶€ë¶„ì˜ ì„¼ì„œ ë°ì´í„°
- **ë‹¤ë¥¸ ë²”ìœ„ íŠ¹ì„±** (5ê°œ): X_11, X_19, X_37, X_40 ë“± â†’ íŠ¹ë³„í•œ ì˜ë¯¸ ê°€ëŠ¥ì„±

**ì¤‘ìš”í•œ ì¸ì‚¬ì´íŠ¸**:
- X_40, X_11 ë“±ì€ ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ â†’ ì¤‘ìš” ì„¼ì„œì¼ ê°€ëŠ¥ì„±
- ë²”ìœ„ íŒ¨í„´ìœ¼ë¡œ ì„¼ì„œ ê·¸ë£¹í™” ê°€ëŠ¥

### 1.2 ê³ ê¸‰ ë¶„ì„ (`advanced_analysis.py`) ë¶„ì„

#### íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
```python
def analyze_feature_importance(df):
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(df[feature_cols], df['target'])

    importance_df = pd.DataFrame({
        'feature': feature_cols,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)
```

**ë°œê²¬ëœ ì¤‘ìš” íŠ¹ì„±ë“¤**:
1. **X_40**: 8.6% - ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„±
2. **X_11**: 5.6%
3. **X_46**: 5.5%
4. **X_36**: 5.3%
5. **X_34**: 3.9%

**í•™ìŠµ í¬ì¸íŠ¸**:
- Random Forestì˜ feature importanceëŠ” Gini ë¶ˆìˆœë„ ê¸°ë°˜
- ìƒìœ„ 5ê°œ íŠ¹ì„±ì´ ì „ì²´ ì¤‘ìš”ë„ì˜ ì•½ 31% ì°¨ì§€
- íŠ¹ì„± ê°„ ì¤‘ìš”ë„ ì°¨ì´ê°€ ëª…í™• â†’ íŠ¹ì„± ì„ íƒ íš¨ê³¼ ê¸°ëŒ€

#### ìƒê´€ê´€ê³„ ë¶„ì„
```python
def correlation_analysis(df):
    corr_matrix = df[feature_cols].corr()

    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_val = abs(corr_matrix.iloc[i, j])
            if corr_val > 0.8:  # ë†’ì€ ìƒê´€ê´€ê³„ ê°ì§€
                high_corr_pairs.append(...)
```

**ë°œê²¬ëœ ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ**:
- **47ê°œ ê³ ìƒê´€ ìŒ** ë°œê²¬ (ìƒê´€ê³„ìˆ˜ > 0.8)
- ì£¼ìš” ì˜ˆì‹œ:
  - X_06 â†” X_45: 1.000 (ì™„ì „ ìƒê´€)
  - X_04 â†” X_39: 0.991
  - X_05 â†” X_25: 0.998

**ì¤‘ìš”í•œ ì¸ì‚¬ì´íŠ¸**:
- ë‹¤ì¤‘ê³µì„ ì„±ì´ ì‹¬ê° â†’ ì°¨ì› ì¶•ì†Œ í•„ìš”
- ì™„ì „ ìƒê´€(1.000) íŠ¹ì„±ë“¤ì€ ì¤‘ë³µ ì œê±° ê°€ëŠ¥
- ì •ë³´ ì†ì‹¤ ì—†ì´ íŠ¹ì„± ìˆ˜ë¥¼ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŒ

#### ì°¨ì› ì¶•ì†Œ ë¶„ì„
```python
def dimensionality_analysis(df):
    pca = PCA()
    pca_result = pca.fit_transform(X_scaled)

    cumsum_ratio = np.cumsum(pca.explained_variance_ratio_)
    n_components_95 = np.argmax(cumsum_ratio >= 0.95) + 1
```

**PCA ë¶„ì„ ê²°ê³¼**:
- **95% ë¶„ì‚°ì„ 18ê°œ ì£¼ì„±ë¶„ìœ¼ë¡œ ì„¤ëª…** ê°€ëŠ¥
- 52ê°œ â†’ 18ê°œë¡œ 65% ì°¨ì› ì¶•ì†Œ ê°€ëŠ¥
- t-SNEë¡œ í´ë˜ìŠ¤ ê°„ ë¶„ë¦¬ë„ ì–‘í˜¸ í™•ì¸

**í•™ìŠµ í¬ì¸íŠ¸**:
- PCAëŠ” ì„ í˜• ë³€í™˜ìœ¼ë¡œ í•´ì„ì„± ì¼ë¶€ ì†ì‹¤
- í•˜ì§€ë§Œ ì°¨ì› ì¶•ì†Œ íš¨ê³¼ëŠ” ë§¤ìš° í¼
- ì›ë³¸ íŠ¹ì„± + PCA íŠ¹ì„± ì¡°í•© ì „ëµ ê³ ë ¤ ê°€ëŠ¥

---

## ğŸ”§ 2ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë¶„ì„

### 2.1 `DataPreprocessor` í´ë˜ìŠ¤ êµ¬ì¡° ë¶„ì„

#### í´ë˜ìŠ¤ ì„¤ê³„ ì² í•™
```python
class DataPreprocessor:
    def __init__(self, remove_corr_threshold=0.95, variance_threshold=0.01, n_features=None):
        self.scaler = RobustScaler()  # ì•„ì›ƒë¼ì´ì–´ ëŒ€ì‘
        self.variance_selector = VarianceThreshold(threshold=variance_threshold)
        self.corr_features_to_remove = []
        self.feature_selector = None
```

**ì„¤ê³„ ìš°ìˆ˜ì„±**:
- âœ… **íŒŒë¼ë¯¸í„°í™”**: threshold ê°’ë“¤ì„ ì¡°ì • ê°€ëŠ¥
- âœ… **RobustScaler ì„ íƒ**: ì•„ì›ƒë¼ì´ì–´ì— ê°•ê±´í•œ ìŠ¤ì¼€ì¼ë§
- âœ… **ëª¨ë“ˆí™”**: ê° ì „ì²˜ë¦¬ ë‹¨ê³„ê°€ ë¶„ë¦¬ë˜ì–´ ìˆìŒ
- âœ… **ìƒíƒœ ì €ì¥**: fit í›„ transform ê°€ëŠ¥í•œ scikit-learn ìŠ¤íƒ€ì¼

#### ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìˆœì„œ
```python
def fit(self, X, y=None):
    # 1. ìŠ¤ì¼€ì¼ë§ (RobustScaler)
    X_scaled = pd.DataFrame(self.scaler.fit_transform(X), columns=X.columns)

    # 2. ë‚®ì€ ë¶„ì‚° íŠ¹ì„± ì œê±° (VarianceThreshold)
    X_variance = pd.DataFrame(
        self.variance_selector.fit_transform(X_scaled),
        columns=X_scaled.columns[self.variance_selector.get_support()]
    )

    # 3. ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì„± ì œê±° (Custom Implementation)
    self.corr_features_to_remove = self.remove_high_correlation(X_variance, self.remove_corr_threshold)
    X_corr = X_variance.drop(columns=self.corr_features_to_remove)

    # 4. íŠ¹ì„± ì„ íƒ (SelectKBest with f_classif)
    if self.n_features and y is not None:
        self.feature_selector = SelectKBest(f_classif, k=self.n_features)
        X_selected = self.feature_selector.fit_transform(X_corr, y)
```

**ì²˜ë¦¬ ìˆœì„œì˜ í•©ë¦¬ì„± ë¶„ì„**:
1. **ìŠ¤ì¼€ì¼ë§ ìš°ì„ **: ìƒê´€ê³„ìˆ˜ ê³„ì‚° ì „ì— ìŠ¤ì¼€ì¼ ì •ê·œí™”
2. **ë¶„ì‚° í•„í„°ë§**: ì •ë³´ê°€ ì—†ëŠ” íŠ¹ì„± ì¡°ê¸° ì œê±°
3. **ìƒê´€ê´€ê³„ ì œê±°**: ì¤‘ë³µ ì •ë³´ ì œê±°ë¡œ ì°¨ì› ì¶•ì†Œ
4. **íŠ¹ì„± ì„ íƒ**: ëª©í‘œ ë³€ìˆ˜ì™€ì˜ ê´€ë ¨ì„± ê¸°ë°˜ ìµœì¢… ì„ íƒ

**í•™ìŠµëœ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤**:
- ì „ì²˜ë¦¬ ìˆœì„œê°€ ê²°ê³¼ì— í° ì˜í–¥ì„ ë¯¸ì¹¨
- DataFrame í˜•íƒœ ìœ ì§€ë¡œ íŠ¹ì„± ì´ë¦„ ì¶”ì 
- fit/transform íŒ¨í„´ìœ¼ë¡œ data leakage ë°©ì§€

### 2.2 3ê°€ì§€ ì „ì²˜ë¦¬ ì „ëµ ë¶„ì„

#### ì „ëµë³„ íŠ¹ì§•
```python
def create_preprocessors():
    preprocessors = {
        'basic': DataPreprocessor(
            remove_corr_threshold=0.95,    # ë³´ìˆ˜ì  ìƒê´€ê´€ê³„ ì œê±°
            variance_threshold=0.01
        ),
        'aggressive': DataPreprocessor(
            remove_corr_threshold=0.90,    # ì ê·¹ì  ìƒê´€ê´€ê³„ ì œê±°
            variance_threshold=0.02
        ),
        'feature_selected': DataPreprocessor(
            remove_corr_threshold=0.95,
            variance_threshold=0.01,
            n_features=30                   # ìµœì¢… 30ê°œ íŠ¹ì„±ìœ¼ë¡œ ì œí•œ
        )
    }
```

**ì „ëµë³„ ê¸°ëŒ€ íš¨ê³¼**:

| ì „ëµ | íŠ¹ì„± ìˆ˜ ë³€í™” | ì¥ì  | ë‹¨ì  |
|------|-------------|------|------|
| **Basic** | 52â†’40 | ì •ë³´ ë³´ì¡´, ì•ˆì •ì„± | ë‹¤ì¤‘ê³µì„ ì„± ì”ì¡´ |
| **Aggressive** | 52â†’38 | ë‹¤ì¤‘ê³µì„ ì„± ì ê·¹ ì œê±° | ì •ë³´ ì†ì‹¤ ìœ„í—˜ |
| **Feature Selected** | 52â†’30 | ìµœì  íŠ¹ì„±ë§Œ ì„ íƒ | ê³¼ì í•© ìœ„í—˜ |

**í•™ìŠµ í¬ì¸íŠ¸**:
- ë‹¤ì–‘í•œ ì „ì²˜ë¦¬ ì „ëµì„ ì‹¤í—˜ì ìœ¼ë¡œ ë¹„êµ
- í•˜ì´í¼íŒŒë¼ë¯¸í„°ì²˜ëŸ¼ ì „ì²˜ë¦¬ë„ íŠœë‹ ëŒ€ìƒ
- ë„ë©”ì¸ íŠ¹ì„±ì— ë”°ë¼ ìµœì  ì „ëµì´ ë‹¬ë¼ì§

---

## ğŸ¤– 3ë‹¨ê³„: ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ë§ ë¶„ì„

### 3.1 ëª¨ë¸ ì„ íƒ ì „ëµ ë¶„ì„

#### í¬í•¨ëœ ëª¨ë¸ë“¤ê³¼ ëª©ì 
```python
def create_baseline_models():
    models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
        'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, multi_class='ovr'),
        'SVM': SVC(random_state=42, probability=True),
        'KNN': KNeighborsClassifier(n_neighbors=5),
        'Naive Bayes': GaussianNB(),
        'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42, max_iter=500)
    }
```

**ëª¨ë¸ ì„ íƒ ì² í•™ ë¶„ì„**:

1. **Tree-based ëª¨ë¸ë“¤** (RF, ET, GB):
   - íŠ¹ì„± ì„ íƒ ìë™í™”
   - ë¹„ì„ í˜• ê´€ê³„ í¬ì°©
   - ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì— ê°•í•¨

2. **Linear ëª¨ë¸ë“¤** (LR, SVM):
   - í•´ì„ ê°€ëŠ¥ì„± ë†’ìŒ
   - ê³ ì°¨ì›ì—ì„œë„ ì•ˆì •ì 
   - ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ í™•ì¸ìš©

3. **Instance-based** (KNN):
   - ì§€ì—­ì  íŒ¨í„´ í¬ì°©
   - ë‹¨ìˆœí•˜ì§€ë§Œ íš¨ê³¼ì 

4. **Probabilistic** (Naive Bayes):
   - ë¹ ë¥¸ í•™ìŠµ/ì˜ˆì¸¡
   - íŠ¹ì„± ë…ë¦½ì„± ê°€ì •

5. **Neural Network**:
   - ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ
   - ë”¥ëŸ¬ë‹ ì „ ë‹¨ê³„

### 3.2 í‰ê°€ ì‹œìŠ¤í…œ ë¶„ì„

#### í‰ê°€ ë©”íŠ¸ë¦­ê³¼ ê³¼ì í•© ê°ì§€
```python
def evaluate_model(model, X_train, X_val, y_train, y_val, model_name):
    # ì˜ˆì¸¡
    y_train_pred = model.predict(X_train)
    y_val_pred = model.predict(X_val)

    # ì •í™•ë„ ê³„ì‚°
    train_acc = accuracy_score(y_train, y_train_pred)
    val_acc = accuracy_score(y_val, y_val_pred)

    print(f"Train Accuracy: {train_acc:.4f}")
    print(f"Validation Accuracy: {val_acc:.4f}")
    print(f"Overfitting: {train_acc - val_acc:.4f}")
```

**í‰ê°€ ì‹œìŠ¤í…œì˜ ìš°ìˆ˜ì„±**:
- âœ… **ê³¼ì í•© ì§€í‘œ**: train_acc - val_acc ì°¨ì´ë¡œ ì •ëŸ‰í™”
- âœ… **ì¼ê´€ëœ í‰ê°€**: ëª¨ë“  ëª¨ë¸ì„ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í‰ê°€
- âœ… **ê²°ê³¼ ì €ì¥**: ì¬í˜„ì„±ì„ ìœ„í•œ ì˜ˆì¸¡ê°’ ë³´ê´€

**ê°œì„  ì—¬ì§€**:
- âŒ **ë‹¨ì¼ ì§€í‘œ**: Accuracyë§Œ ì‚¬ìš© (Macro F1-score í•„ìš”)
- âŒ **ë‹¨ì¼ ë¶„í• **: í•˜ë‚˜ì˜ train/val splitë§Œ ì‚¬ìš©
- âŒ **í†µê³„ì  ê²€ì • ë¶€ì¬**: ì„±ëŠ¥ ì°¨ì´ì˜ ìœ ì˜ì„± ê²€ì • ì—†ìŒ

### 3.3 ì„±ëŠ¥ ê²°ê³¼ ë¶„ì„

#### PROJECT_SUMMARY.mdì—ì„œ í™•ì¸ëœ ì„±ëŠ¥
- **í˜„ì¬ ìµœê³  ì„±ëŠ¥**: 76.96% (Random Forest 5-Fold CV)
- **ì‹¤ì œ ì œì¶œ ì ìˆ˜**: 67.596% (ì•½ 9.4% ì„±ëŠ¥ ê²©ì°¨)

**ì„±ëŠ¥ ê²©ì°¨ ì›ì¸ ì¶”ì •**:
1. **Cross-Validation vs Single Split**: CVê°€ ë” ë‚™ê´€ì 
2. **ê³¼ì í•©**: ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ì ì‘
3. **ë¦¬ë”ë³´ë“œ ê³¼ì í•©**: Public test setì— íŠ¹í™”
4. **ì „ì²˜ë¦¬ ë¶ˆì¼ì¹˜**: Train/Test ë¶„í¬ ì°¨ì´

### 3.4 ì œì¶œ ì‹œìŠ¤í…œ ë¶„ì„

#### `create_submission()` í•¨ìˆ˜
```python
def create_submission(best_model, preprocessor_name='basic'):
    # ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¡œ ì¬í›ˆë ¨
    X_full_processed = preprocessor.fit_transform(X_full, y_full)
    X_test_processed = preprocessor.transform(X_test)

    # ëª¨ë¸ ì¬í›ˆë ¨
    best_model.fit(X_full_processed, y_full)

    # ì˜ˆì¸¡ ë° ì €ì¥
    test_predictions = best_model.predict(X_test_processed)
    submission = pd.DataFrame({
        'ID': test['ID'],
        'target': test_predictions
    })
    submission.to_csv('submission.csv', index=False)
```

**êµ¬í˜„ì˜ í•©ë¦¬ì„±**:
- âœ… **ì „ì²´ ë°ì´í„° í™œìš©**: ìµœì¢… ëª¨ë¸ì€ ëª¨ë“  í›ˆë ¨ ë°ì´í„° ì‚¬ìš©
- âœ… **ì¼ê´€ëœ ì „ì²˜ë¦¬**: ë™ì¼í•œ preprocessor ê°ì²´ ì‚¬ìš©
- âœ… **í‘œì¤€ í˜•ì‹**: ëŒ€íšŒ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ì œì¶œ íŒŒì¼

---

## ğŸ“Š 4ë‹¨ê³„: í˜„ì¬ ì„±ëŠ¥ ë¶„ì„ ë° ë¬¸ì œ ì§„ë‹¨

### 4.1 ì„±ëŠ¥ ê²©ì°¨ ë¶„ì„

#### í™•ì¸ëœ ì„±ëŠ¥ ì§€í‘œë“¤
- **Cross-Validation**: 76.96% (Random Forest)
- **ì‹¤ì œ ë¦¬ë”ë³´ë“œ**: 67.596%
- **ì„±ëŠ¥ ê²©ì°¨**: -9.36% (ìƒë‹¹í•œ ì°¨ì´)

**ê°€ëŠ¥í•œ ì›ì¸ë“¤**:

1. **ê³¼ì í•© (Overfitting)**:
   ```python
   print(f"Overfitting: {train_acc - val_acc:.4f}")
   ```
   - í›ˆë ¨ ì •í™•ë„ 1.0000 vs ê²€ì¦ ì •í™•ë„ 0.7624
   - ê³¼ì í•©ë„: 0.2376 (23.76%)

2. **êµì°¨ê²€ì¦ í¸í–¥**:
   - Single hold-out validation vs 5-fold CV
   - CVê°€ ë” ë‚™ê´€ì ì¸ ì¶”ì •ì„ ì œê³µ

3. **ë°ì´í„° ë¶„í¬ ì°¨ì´**:
   - Train vs Test setì˜ ë¶„í¬ê°€ ë‹¤ë¥¼ ê°€ëŠ¥ì„±
   - Domain shift ë˜ëŠ” temporal shift

4. **ë¦¬ë”ë³´ë“œ íŠ¹ì„±**:
   - Public LBì˜ ì¼ë¶€ë¶„ë§Œ ê³µê°œ
   - Private LBì™€ ë‹¤ë¥¸ ë¶„í¬ì¼ ê°€ëŠ¥ì„±

### 4.2 ëª¨ë¸ ë³µì¡ë„ ë¶„ì„

#### í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ë“¤ì˜ ë³µì¡ë„
- **Random Forest**: n_estimators=100 (ì¤‘ê°„ ë³µì¡ë„)
- **Extra Trees**: n_estimators=100 (ì¤‘ê°„ ë³µì¡ë„)
- **Gradient Boosting**: n_estimators=100 (ë†’ì€ ë³µì¡ë„)

**ë³µì¡ë„ vs ì„±ëŠ¥ trade-off**:
- ë†’ì€ ë³µì¡ë„ â†’ í›ˆë ¨ ì„±ëŠ¥ ìƒìŠ¹, ì¼ë°˜í™” ì„±ëŠ¥ í•˜ë½
- ì ì ˆí•œ regularization ë¶€ì¬
- Early stopping, cross-validation ê¸°ë°˜ tuning í•„ìš”

### 4.3 íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ì˜ í•œê³„

#### í˜„ì¬ íŠ¹ì„± ì²˜ë¦¬ ìˆ˜ì¤€
1. **ê¸°ë³¸ì  ì „ì²˜ë¦¬**ë§Œ ìˆ˜í–‰:
   - ìŠ¤ì¼€ì¼ë§ (RobustScaler)
   - ìƒê´€ê´€ê³„ ì œê±°
   - ë¶„ì‚° í•„í„°ë§

2. **ë¶€ì¬í•œ ê³ ê¸‰ ê¸°ë²•ë“¤**:
   - ë„ë©”ì¸ ê¸°ë°˜ íŠ¹ì„± ìƒì„±
   - ì„¼ì„œ ê°„ ìƒí˜¸ì‘ìš© íŠ¹ì„±
   - ì‹œê°„ ìœˆë„ìš° ê¸°ë°˜ íŠ¹ì„± (if applicable)
   - ë¹„ì„ í˜• ë³€í™˜ íŠ¹ì„±

#### ê°œì„  ê¸°íšŒ
- **ì„¼ì„œ ê·¸ë£¹í™”**: X_40, X_11, X_46 ë“± ì¤‘ìš” ì„¼ì„œë“¤ì˜ ì¡°í•©
- **í†µê³„ì  íŠ¹ì„±**: ì„¼ì„œ ê·¸ë£¹ë³„ mean, std, skew, kurtosis
- **ë¹„ìœ¨ íŠ¹ì„±**: ì¤‘ìš” ì„¼ì„œë“¤ ê°„ì˜ ë¹„ìœ¨ ê´€ê³„

---

## ğŸ“ í•™ìŠµëœ í•µì‹¬ í†µì°°ë“¤

### 5.1 ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì›Œí¬í”Œë¡œìš° ì´í•´

#### ì²´ê³„ì  ì ‘ê·¼ë²•ì˜ ì¤‘ìš”ì„±
1. **EDA ë‹¨ê³„**: ë°ì´í„° ì´í•´ê°€ ëª¨ë“  í›„ì† ì‘ì—…ì˜ ê¸°ë°˜
2. **ì „ì²˜ë¦¬ ì „ëµ**: ë‹¨ìˆœí•œ ì •ë¦¬ê°€ ì•„ë‹Œ ì „ëµì  ì„ íƒ
3. **ëª¨ë¸ ë¹„êµ**: ë‹¤ì–‘í•œ ì ‘ê·¼ë²•ì˜ ì²´ê³„ì  ì‹¤í—˜
4. **ì„±ëŠ¥ ê²€ì¦**: ë‹¨ìˆœí•œ ì •í™•ë„ ì´ìƒì˜ ì¢…í•©ì  í‰ê°€

#### ì¬í˜„ê°€ëŠ¥í•œ ì—°êµ¬ì˜ ì‹¤ì²œ
- âœ… **random_state ì¼ê´€ì„±**: ëª¨ë“  ëª¨ë¸ì—ì„œ 42 ì‚¬ìš©
- âœ… **í•¨ìˆ˜ ëª¨ë“ˆí™”**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì½”ë“œ êµ¬ì¡°
- âœ… **ê²°ê³¼ ì €ì¥**: ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸° pickle ì €ì¥
- âœ… **íŒŒë¼ë¯¸í„° ê¸°ë¡**: ì‹¤í—˜ ì„¤ì •ì˜ ëª…ì‹œì  ê¸°ë¡

### 5.2 ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ í†µì°°

#### Tree-based ëª¨ë¸ì˜ íš¨ê³¼ì„±
- **Random Forest**: ê°€ì¥ ì•ˆì •ì ì¸ ì„±ëŠ¥
- **Extra Trees**: ë” ë§ì€ randomnessë¡œ ì¼ë°˜í™” í–¥ìƒ
- **Gradient Boosting**: ë†’ì€ ì„±ëŠ¥ì´ì§€ë§Œ ê³¼ì í•© ìœ„í—˜

#### ì•™ìƒë¸”ì˜ í•„ìš”ì„±
- ë‹¨ì¼ ëª¨ë¸ë¡œëŠ” 0.90+ ëª©í‘œ ë‹¬ì„± ì–´ë ¤ì›€
- ì„œë¡œ ë‹¤ë¥¸ íŠ¹ì„±ì˜ ëª¨ë¸ ì¡°í•© í•„ìš”
- ë‹¤ì–‘ì„±(diversity)ê³¼ ì„±ëŠ¥ì˜ ê· í˜•

### 5.3 ê²½ì§„ëŒ€íšŒ íŠ¹í™” ì¸ì‚¬ì´íŠ¸

#### CV vs LB ì ìˆ˜ ê²©ì°¨
- **ì¼ë°˜ì  í˜„ìƒ**: ëŒ€ë¶€ë¶„ì˜ ê²½ì§„ëŒ€íšŒì—ì„œ ê´€ì°°
- **ê´€ë¦¬ ì „ëµ**: ë³´ìˆ˜ì ì¸ ëª¨ë¸ ì„ íƒ, robust validation
- **ë¦¬ë”ë³´ë“œ ê³¼ì í•© ë°©ì§€**: ì œì¶œ íšŸìˆ˜ ì œí•œ, ì•ˆì •ì„± ìš°ì„ 

#### í´ë˜ìŠ¤ ê· í˜•ì˜ ì´ì 
- **ì™„ë²½í•œ ê· í˜•**: ê° í´ë˜ìŠ¤ 1,033ê°œì”©
- **ë‹¨ìˆœí•œ í‰ê°€**: ë³µì¡í•œ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ë¶ˆí•„ìš”
- **Macro F1-score ìœ ë¦¬**: ëª¨ë“  í´ë˜ìŠ¤ ë™ë“±í•œ ì¤‘ìš”ë„

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•œ ê°œì„  ë°©í–¥

### 6.1 ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„ ì‚¬í•­

1. **í‰ê°€ ì§€í‘œ ê°œì„ **:
   ```python
   from sklearn.metrics import f1_score
   macro_f1 = f1_score(y_val, y_val_pred, average='macro')
   ```

2. **êµì°¨ê²€ì¦ ê°•í™”**:
   ```python
   from sklearn.model_selection import StratifiedKFold
   skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
   ```

3. **ê³¼ì í•© ë°©ì§€**:
   ```python
   # Random Forest ì •ê·œí™”
   rf = RandomForestClassifier(
       max_depth=20,           # ê¹Šì´ ì œí•œ
       min_samples_split=10,   # ë¶„í•  ìµœì†Œ ìƒ˜í”Œ
       min_samples_leaf=5      # ì ë…¸ë“œ ìµœì†Œ ìƒ˜í”Œ
   )
   ```

### 6.2 ì¤‘ê¸° ê°œì„  ì „ëµ

1. **ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§**:
   - ì„¼ì„œ ê·¸ë£¹ë³„ í†µê³„ì  íŠ¹ì„±
   - ì£¼ìš” ì„¼ì„œ ê°„ ìƒí˜¸ì‘ìš© íŠ¹ì„±
   - PCA + ì›ë³¸ íŠ¹ì„± ì¡°í•©

2. **ìµœì‹  ëª¨ë¸ ë„ì…**:
   - XGBoost/LightGBM with hyperparameter tuning
   - TabNet for tabular data
   - Stacking ensemble

3. **ê²€ì¦ ì „ëµ ê³ ë„í™”**:
   - Time-based split (if temporal order exists)
   - Multiple random splits with different seeds
   - Adversarial validation for train/test distribution comparison

---

## âœ… í˜„ì¬ ì½”ë“œë² ì´ìŠ¤ ê°•ì ê³¼ ì•½ì 

### ê°•ì  (Strengths)
- âœ… **ì²´ê³„ì  êµ¬ì¡°**: EDA â†’ ì „ì²˜ë¦¬ â†’ ëª¨ë¸ë§ ìˆœì„œ
- âœ… **ëª¨ë“ˆí™”**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜ë“¤
- âœ… **ì¬í˜„ì„±**: random_stateì™€ ì €ì¥/ë¡œë”© ì‹œìŠ¤í…œ
- âœ… **ë‹¤ì–‘ì„±**: 8ê°œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ
- âœ… **ì‹¤ìš©ì„±**: ì‹¤ì œ ì œì¶œê¹Œì§€ ì™„ë£Œëœ íŒŒì´í”„ë¼ì¸

### ì•½ì  (Weaknesses)
- âŒ **ë‹¨ìˆœí•œ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§**: ê¸°ë³¸ì ì¸ ì „ì²˜ë¦¬ë§Œ
- âŒ **í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë¶€ì¬**: ê¸°ë³¸ê°’ë§Œ ì‚¬ìš©
- âŒ **ì•™ìƒë¸” ì „ëµ ë¯¸í¡**: ë‹¨ìˆœ íˆ¬í‘œ ë°©ì‹
- âŒ **í‰ê°€ ì§€í‘œ ë‹¨ì¼í™”**: Accuracyë§Œ ì‚¬ìš©
- âŒ **êµì°¨ê²€ì¦ ë¶€ì¡±**: ë‹¨ì¼ ë¶„í• ë§Œ ì‚¬ìš©

---

**ê²°ë¡ **: í˜„ì¬ ì½”ë“œë² ì´ìŠ¤ëŠ” ê²¬ê³ í•œ ê¸°ë°˜ì„ ì œê³µí•˜ì§€ë§Œ, 0.90+ ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•´ì„œëŠ” íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§, ëª¨ë¸ ìµœì í™”, ì•™ìƒë¸” ì „ëµì˜ ëŒ€í­ ê°œì„ ì´ í•„ìš”í•˜ë‹¤.

**ë‹¤ìŒ ì¥ì—ì„œ**: Phase 1 ì„±ëŠ¥ ì§„ë‹¨ì„ í†µí•´ êµ¬ì²´ì ì¸ ê°œì„ ì ë“¤ì„ ì‹¤í—˜ì ìœ¼ë¡œ ê²€ì¦í•˜ê³ , ë‹¨ê³„ë³„ ì„±ëŠ¥ í–¥ìƒ ì „ëµì„ ì‹¤í–‰í•  ì˜ˆì •ì´ë‹¤.